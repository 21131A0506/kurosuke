{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"kotaemon","text":"<p>Quick and easy AI components to build Kotaemon - applicable in client project.</p>"},{"location":"#install","title":"Install","text":"<pre><code>pip install kotaemon@git+ssh://git@github.com/Cinnamon/kotaemon.git\n</code></pre>"},{"location":"#contribute","title":"Contribute","text":""},{"location":"#setup","title":"Setup","text":"<ul> <li>Create conda environment (suggest 3.10)</li> </ul> <pre><code>conda create -n kotaemon python=3.10\nconda activate kotaemon\n</code></pre> <ul> <li>Clone the repo</li> </ul> <pre><code>git clone git@github.com:Cinnamon/kotaemon.git\ncd kotaemon\n</code></pre> <ul> <li>Install all</li> </ul> <pre><code>pip install -e \".[dev]\"\n</code></pre> <ul> <li>Pre-commit</li> </ul> <pre><code>pre-commit install\n</code></pre> <ul> <li>Test</li> </ul> <pre><code>pytest tests\n</code></pre>"},{"location":"#credential-sharing","title":"Credential sharing","text":"<p>This repo uses git-secret to share credentials, which internally uses <code>gpg</code> to encrypt and decrypt secret files.</p> <p>This repo uses <code>python-dotenv</code> to manage credentials stored as environment variable. Please note that the use of <code>python-dotenv</code> and credentials are for development purposes only. Thus, it should not be used in the main source code (i.e. <code>kotaemon/</code> and <code>tests/</code>), but can be used in <code>examples/</code>.</p>"},{"location":"#install-git-secret","title":"Install git-secret","text":"<p>Please follow the official guide to install git-secret.</p> <p>For Windows users, see For Windows users.</p> <p>For users who don't have sudo privilege to install packages, follow the <code>Manual Installation</code> in the official guide and set <code>PREFIX</code> to a path that you have access to. And please don't forget to add <code>PREFIX</code> to your <code>PATH</code>.</p>"},{"location":"#gaining-access","title":"Gaining access","text":"<p>In order to gain access to the secret files, you must provide your gpg public file to anyone who has access and ask them to ask your key to the keyring. For a quick tutorial on generating your gpg key pair, you can refer to the <code>Using gpg</code> section from the git-secret main page.</p>"},{"location":"#decrypt-the-secret-file","title":"Decrypt the secret file","text":"<p>The credentials are encrypted in the <code>.env.secret</code> file. To print the decrypted content to stdout, run</p> <pre><code>git-secret cat [filename]\n</code></pre> <p>Or to get the decrypted <code>.env</code> file, run</p> <pre><code>git-secret reveal [filename]\n</code></pre>"},{"location":"#for-windows-users","title":"For Windows users","text":"<p>git-secret is currently not available for Windows, thus the easiest way is to use it in WSL (please use the latest version of WSL2). From there you have 2 options:</p> <ol> <li>Using the gpg of WSL.</li> </ol> <p>This is the most straight-forward option since you would use WSL just like any other unix environment. However, the downside is that you have to make WSL your main environment, which means WSL must have write permission on your repo. To achieve this, you must either:</p> <ul> <li>Clone and store your repo inside WSL's file system.</li> <li> <p>Provide WSL with necessary permission on your Windows file system. This can be achieve by setting <code>automount</code> options for WSL. To do that, add these content to <code>/etc/wsl.conf</code> and then restart your sub-system.</p> <pre><code>[automount]\noptions = \"metadata,umask=022,fmask=011\"\n</code></pre> <p>This enables all permissions for user owner.</p> </li> <li> <p>Using the gpg of Windows but with git-secret from WSL.</p> </li> </ul> <p>For those who use Windows as the main environment, having to switch back and forth between Windows and WSL will be inconvenient. You can instead stay within your Windows environment and apply some tricks to use <code>git-secret</code> from WSL.</p> <ul> <li>Install and setup <code>gpg</code> on Windows.</li> <li>Install <code>git-secret</code> on WSL. Now in Windows, you can invoke <code>git-secret</code> using <code>wsl git-secret</code>.</li> <li> <p>Alternatively you can setup alias in CMD to shorten the syntax. Please refer to this SO answer for the instruction. Some recommended aliases are:</p> <pre><code>@echo off\n\n:: Commands\nDOSKEY ls=dir /B $*\nDOSKEY ll=dir /a $*\nDOSKEY git-secret=wsl git-secret $*\nDOSKEY gs=wsl git-secret $*\n</code></pre> <p>Now you can invoke <code>git-secret</code> in CMD using <code>git-secret</code> or <code>gs</code>.</p> <ul> <li>For Powershell users, similar behaviours can be achieved using <code>Set-Alias</code> and <code>profile.ps1</code>. Please refer this SO thread as an example.</li> </ul> </li> </ul>"},{"location":"#code-base-structure","title":"Code base structure","text":"<ul> <li>documents: define document</li> <li>loaders</li> </ul>"},{"location":"contributing/","title":"Getting started","text":""},{"location":"contributing/#setup","title":"Setup","text":"<ul> <li>Create conda environment (suggest 3.10)</li> </ul> <pre><code>conda create -n kotaemon python=3.10\nconda activate kotaemon\n</code></pre> <ul> <li>Clone the repo</li> </ul> <pre><code>git clone git@github.com:Cinnamon/kotaemon.git\ncd kotaemon\n</code></pre> <ul> <li>Install all</li> </ul> <pre><code>pip install -e \".[dev]\"\n</code></pre> <ul> <li>Pre-commit</li> </ul> <pre><code>pre-commit install\n</code></pre> <ul> <li>Test</li> </ul> <pre><code>pytest tests\n</code></pre>"},{"location":"contributing/#credential-sharing","title":"Credential sharing","text":"<p>This repo uses git-secret to share credentials, which internally uses <code>gpg</code> to encrypt and decrypt secret files.</p> <p>This repo uses <code>python-dotenv</code> to manage credentials stored as environment variable. Please note that the use of <code>python-dotenv</code> and credentials are for development purposes only. Thus, it should not be used in the main source code (i.e. <code>kotaemon/</code> and <code>tests/</code>), but can be used in <code>examples/</code>.</p>"},{"location":"contributing/#install-git-secret","title":"Install git-secret","text":"<p>Please follow the official guide to install git-secret.</p> <p>For Windows users, see For Windows users.</p> <p>For users who don't have sudo privilege to install packages, follow the <code>Manual Installation</code> in the official guide and set <code>PREFIX</code> to a path that you have access to. And please don't forget to add <code>PREFIX</code> to your <code>PATH</code>.</p>"},{"location":"contributing/#gaining-access-to-credientials","title":"Gaining access to credientials","text":"<p>In order to gain access to the secret files, you must provide your gpg public file to anyone who has access and ask them to ask your key to the keyring. For a quick tutorial on generating your gpg key pair, you can refer to the <code>Using gpg</code> section from the git-secret main page.</p>"},{"location":"contributing/#decrypt-the-secret-file","title":"Decrypt the secret file","text":"<p>The credentials are encrypted in the <code>.env.secret</code> file. To print the decrypted content to stdout, run</p> <pre><code>git-secret cat [filename]\n</code></pre> <p>Or to get the decrypted <code>.env</code> file, run</p> <pre><code>git-secret reveal [filename]\n</code></pre>"},{"location":"contributing/#for-windows-users","title":"For Windows users","text":"<p>git-secret is currently not available for Windows, thus the easiest way is to use it in WSL (please use the latest version of WSL2). From there you have 2 options:</p> <ol> <li>Using the gpg of WSL.</li> </ol> <p>This is the most straight-forward option since you would use WSL just like any other unix environment. However, the downside is that you have to make WSL your main environment, which means WSL must have write permission on your repo. To achieve this, you must either:</p> <ul> <li>Clone and store your repo inside WSL's file system.</li> <li> <p>Provide WSL with necessary permission on your Windows file system. This can be achieve by setting <code>automount</code> options for WSL. To do that, add these content to <code>/etc/wsl.conf</code> and then restart your sub-system.</p> <pre><code>[automount]\noptions = \"metadata,umask=022,fmask=011\"\n</code></pre> <p>This enables all permissions for user owner.</p> </li> <li> <p>Using the gpg of Windows but with git-secret from WSL.</p> </li> </ul> <p>For those who use Windows as the main environment, having to switch back and forth between Windows and WSL will be inconvenient. You can instead stay within your Windows environment and apply some tricks to use <code>git-secret</code> from WSL.</p> <ul> <li>Install and setup <code>gpg</code> on Windows.</li> <li>Install <code>git-secret</code> on WSL. Now in Windows, you can invoke <code>git-secret</code> using <code>wsl git-secret</code>.</li> <li> <p>Alternatively you can setup alias in CMD to shorten the syntax. Please refer to this SO answer for the instruction. Some recommended aliases are:</p> <pre><code>@echo off\n\n:: Commands\nDOSKEY ls=dir /B $*\nDOSKEY ll=dir /a $*\nDOSKEY git-secret=wsl git-secret $*\nDOSKEY gs=wsl git-secret $*\n</code></pre> <p>Now you can invoke <code>git-secret</code> in CMD using <code>git-secret</code> or <code>gs</code>.</p> <ul> <li>For Powershell users, similar behaviours can be achieved using <code>Set-Alias</code> and <code>profile.ps1</code>. Please refer this SO thread as an example.</li> </ul> </li> </ul>"},{"location":"contributing/#pr-guideline","title":"PR guideline","text":""},{"location":"contributing/#common-conventions","title":"Common conventions","text":"<ul> <li>Review should be done as soon as possible (within 2 business days).</li> <li>PR title: [ticket] One-line description (example: [AUR-385, AUR-388] Declare BaseComponent and decide LLM call interface).</li> <li>[Encouraged] Provide a quick description in the PR, so that:</li> <li>Reviewers can quickly understand the direction of the PR.</li> <li>It will be included in the commit message when the PR is merged.</li> </ul>"},{"location":"contributing/#environment-caching-on-pr","title":"Environment caching on PR","text":"<ul> <li>To speed up CI, environments are cached based on the version specified in <code>__init__.py</code>. </li> <li>Since dependencies versions in <code>setup.py</code> are not pinned, you need to pump the version in order to use a new environment. That environment will then be cached and used by your subsequence commits within the PR, until you pump the version again</li> <li>The new environment created during your PR is cached and will be available to others once the PR is merged.</li> <li>If you are experimenting with new dependencies and want a fresh environment every time, add <code>[ignore cache]</code> in your commit message. The CI will create a fresh environment to run your commit and then discard it.</li> <li>If your PR include updated dependencies, the recommended workflow would be:</li> <li>Doing development as usual.</li> <li>When you want to run the CI, push a commit with the message containing <code>[ignore cache]</code>.</li> <li>Once the PR is final, pump the version in <code>__init__.py</code> and push a final commit not containing <code>[ignore cache]</code>.</li> </ul> <p>Examples: https://github.com/Cinnamon/kotaemon/pull/2</p>"},{"location":"contributing/#merge-pr-guideline","title":"Merge PR guideline","text":"<ul> <li>Use squash and merge option</li> <li>1st line message is the PR title.</li> <li>The text area is the PR description.</li> </ul>"},{"location":"contributing/#develop-pipelines","title":"Develop pipelines","text":"<ul> <li>Nodes</li> <li>Params</li> <li>Run function</li> </ul> <pre><code>from kotaemon.base import BaseComponent\n\nclass Pipeline(BaseComponent):\n   llm: AzureOpenAIEmbedding\n   doc_store: BaseDocumentStore\n\n   def run(self, input1, input2) -&gt; str:\n      input = input1 + input2\n      output = self.llm(input)\n      self.doc_store.add(output)\n      return output\n\npipeline = Pipeline(llm=AzureOpenAILLM(), doc_store=InMemoryDocstore())\noutput = pipeline(\"this is text1\", \"this is text2\")\n</code></pre>"},{"location":"create-a-component/","title":"Creating a component","text":"<p>A fundamental concept in kotaemon is \"component\".</p> <p>Anything that isn't data or data structure is a \"component\". A component can be thought of as a step within a pipeline. It takes in some input, processes it, and returns an output, just the same as a Python function! The output will then become an input for the next component in a pipeline. In fact, a pipeline is just a component. More appropriately, a nested component: a component that makes use of one or more other components in the processing step. So in reality, there isn't a difference between a pipeline and a component! Because of that, in kotaemon, we will consider them the same as \"component\".</p> <p>To define a component, you will:</p> <ol> <li>Create a class that subclasses from <code>kotaemon.base.BaseComponent</code></li> <li>Declare init params with type annotation</li> <li>Declare nodes (nodes are just other components!) with type annotation</li> <li>Implement the processing logic in <code>run</code>.</li> </ol> <p>The syntax of a component is as follow:</p> <pre><code>from kotaemon.base import BaseComponent\nfrom kotaemon.llms import AzureChatOpenAI\nfrom kotaemon.parsers import RegexExtractor\n\n\nclass FancyPipeline(BaseComponent):\n    param1: str = \"This is param1\"\n    param2: int = 10\n    param3: float\n\n    node1: BaseComponent    # this is a node because of BaseComponent type annotation\n    node2: AzureChatOpenAI  # this is also a node because AzureChatOpenAI subclasses BaseComponent\n    node3: RegexExtractor   # this is also a node bceause RegexExtractor subclasses BaseComponent\n\n    def run(self, some_text: str):\n        prompt = (self.param1 + some_text) * int(self.param2 + self.param3)\n        llm_pred = self.node2(prompt).text\n        matches = self.node3(llm_pred)\n        return matches\n</code></pre> <p>Then this component can be used as follow:</p> <pre><code>llm = AzureChatOpenAI(endpoint=\"some-endpont\")\nextractor = RegexExtractor(pattern=[\"yes\", \"Yes\"])\n\ncomponent = FancyPipeline(\n    param1=\"Hello\"\n    param3=1.5\n    node1=llm,\n    node2=llm,\n    node3=extractor\n)\ncomponent(\"goodbye\")\n</code></pre> <p>This way, we can define each operation as a reusable component, and use them to compose larger reusable components!</p>"},{"location":"create-a-component/#benefits-of-component","title":"Benefits of component","text":"<p>By defining a component as above, we formally encapsulate all the necessary information inside a single class. This introduces several benefits:</p> <ol> <li>Allow tools like promptui to inspect the inner working of a component in    order to automatically generate the promptui.</li> <li>Allow visualizing a pipeline for debugging purpose.</li> </ol>"},{"location":"data-components/","title":"Data & Data Structure Components","text":"<p>The data &amp; data structure components include:</p> <ul> <li>The <code>Document</code> class.</li> <li>The document store.</li> <li>The vector store.</li> </ul>"},{"location":"data-components/#data-loader","title":"Data Loader","text":"<ul> <li>PdfLoader</li> <li>Layout-aware with table parsing PdfLoader<ul> <li>MathPixLoader: To use this loader, you need MathPix API key, refer to mathpix docs for more information</li> <li>OCRLoader: This loader uses lib-table and Flax pipeline to perform OCR and read table structure from PDF file (TODO: add more info about deployment of this module).</li> <li>Output:<ul> <li> <p>Document: text + metadata to identify whether it is table or not</p> <pre><code>- \"source\": source file name\n- \"type\": \"table\" or \"text\"\n- \"table_origin\": original table in markdown format (to be feed to LLM or visualize using external tools)\n- \"page_label\": page number in the original PDF document\n</code></pre> </li> </ul> </li> </ul> </li> </ul>"},{"location":"data-components/#document-store","title":"Document Store","text":"<ul> <li>InMemoryDocumentStore</li> </ul>"},{"location":"data-components/#vector-store","title":"Vector Store","text":"<ul> <li>ChromaVectorStore</li> <li>InMemoryVectorStore</li> </ul>"},{"location":"overview/","title":"Overview","text":""},{"location":"overview/#introduction","title":"Introduction","text":"<p><code>kotaemon</code> library focuses on the AI building blocks to implement the Kotaemon. It can be used in both client project and in product development. It consists of base interfaces, core components and a list of utilities:</p> <ul> <li>Base interfaces: <code>kotaemon</code> defines the base interface of a component in a pipeline. A pipeline is also a component. By clearly define this interface, a pipeline of steps can be easily constructed and orchestrated.</li> <li>Core components: <code>kotaemon</code> implements (or wraps 3rd-party libraries like Langchain, llama-index,... when possible) commonly used components in kotaemon use cases. Some of these components are: LLM, vector store, document store, retriever... For a detailed list and description of these components, please refer to the Pipeline Components and Data &amp; Data Structure Components sections.</li> <li>List of utilities: <code>lib-knowledge</code> provides utilities and tools that are usually needed in client project. For example, it provides a prompt engineering UI for AI developers in a project to quickly create a prompt engineering tool for DMs and QALs. It also provides a command to quickly spin up a project code base. For a full list and description of these utilities, please refer to the Utilities section.</li> </ul> <pre><code>mindmap\n  root((kotaemon))\n    Base Interfaces\n      Document\n      LLMInterface\n      RetrievedDocument\n      BaseEmbeddings\n      BaseChat\n      BaseCompletion\n      ...\n    Core Components\n      LLMs\n        AzureOpenAI\n        OpenAI\n      Embeddings\n        AzureOpenAI\n        OpenAI\n        HuggingFaceEmbedding\n      VectorStore\n        InMemoryVectorstore\n        ChromaVectorstore\n      Agent\n      Tool\n      DocumentStore\n      ...\n    Utilities\n      Scaffold project\n      PromptUI\n      Documentation Support</code></pre>"},{"location":"overview/#expected-benefit","title":"Expected benefit","text":"<p>Before <code>kotaemon</code>:</p> <ul> <li>Starting everything from scratch.</li> <li>Knowledge and expertise is fragmented.</li> <li>Nothing to reuse.</li> <li>No way to collaborate between tech and non-tech experts.</li> </ul> <p><code>kotaemon</code> expects to completely revolutionize the way we are building LLM-related projects. It helps the company side-steps those issues by:</p> <ul> <li>Standardize the interface to (1) make building LLM pipeline clearer (2) more reasonable to integrate pipelines between different projects.</li> <li>Centralize LLM-related technical components into 1 place. Avoid fragmented technology development. Easy to find the LLM-related technology inside the company.</li> <li>Centralize bug fixes and improvements in 1 place.</li> <li>Reduce boilerplate code during project development.</li> <li>Lightning fast prototyping.</li> </ul>"},{"location":"overview/#install","title":"Install","text":"<p>The kotaemon can be installed from source with: <pre><code>pip install kotaemon@git+ssh://git@github.com/Cinnamon/kotaemon.git\n</code></pre> or from Cinnamon's internal python package index: <pre><code>pip install kotaemon --extra-index-url https://ian_devpi.promptui.dm.cinnamon.is/root/packages\n</code></pre></p>"},{"location":"overview/#example-use-cases","title":"Example use cases","text":"<ul> <li>Start a project from scratch: <code>kh start-project</code></li> <li>Run prompt engineering UI tool: <code>kh promptui export</code>, then <code>kh promptui run</code>.</li> </ul>"},{"location":"upload-package/","title":"Upload python package to private index","text":"<p>Devpi server endpoint (subjected to change): https://ian_devpi.promptui.dm.cinnamon.is/root/packages</p> <p>Install devpi-client <pre><code>pip install devpi-client\n</code></pre></p> <p>Login to the server <pre><code>devpi use &lt;server endpoint&gt; # set server endpoint provided above\ndevpi login &lt;user name&gt; --password=&lt;your password&gt; # login\n</code></pre> If you don't yet have an account, please contact Ian or John.</p> <p>Upload your package <pre><code>devpi use &lt;package name&gt;\\dev # choose the index to upload your package\ncd &lt;your package directory which must contain a pyproject.toml/setup.py&gt;\ndevpi upload\n</code></pre></p>"},{"location":"utilities/","title":"Utilities","text":"<p>Utilities detail can be referred in the sub-pages of this section.</p>"},{"location":"utilities/#prompt-engineering-ui","title":"Prompt engineering UI","text":"<p>Important: despite the name prompt engineering UI, this tool allows DMs to test any kind of parameters that are exposed by AIRs. Prompt is one kind of param. There can be other type of params that DMs can tweak (e.g. top_k, temperature...).</p> <p>Note: For hands-on examination of how to use prompt engineering UI, refer <code>./examples/promptui</code> and <code>./examples/example2/</code></p> <p>In client projects, AI developers typically build the pipeline. However, for LLM projects requiring Japanese and domain expertise in prompt creation, non-technical team members (DM, BizDev, and QALs) can be more effective. To facilitate this, \"xxx\" offers a user-friendly prompt engineering UI that AI developers integrate into their pipelines. This enables non-technical members to adjust prompts and parameters, run experiments, and export results for optimization.</p> <p>As of Sept 2023, there are 2 kinds of prompt engineering UI:</p> <ul> <li>Simple pipeline: run one-way from start to finish.</li> <li>Chat pipeline: interactive back-and-forth.</li> </ul>"},{"location":"utilities/#simple-pipeline","title":"Simple pipeline","text":"<p>For simple pipeline, the supported client project workflow looks as follow:</p> <ol> <li>[AIR] Build pipeline</li> <li>[AIR] Export pipeline to config: <code>$ kh promptui export &lt;module.path.piplineclass&gt; --output &lt;path/to/config/file.yml&gt;</code></li> <li>[AIR] Customize the config</li> <li>[AIR] Spin up prompt engineering UI: <code>$ kh promptui run &lt;path/to/config/file.yml&gt;</code></li> <li>[DM] Change params, run inference</li> <li>[DM] Export to Excel</li> <li>[DM] Select the set of params that achieve the best output</li> </ol> <p>The prompt engineering UI prominently involves from step 2 to step 7 (step 1 is normal AI tasks in project, while step 7 happens exclusively in Excel file).</p>"},{"location":"utilities/#step-2-export-pipeline-to-config","title":"Step 2 - Export pipeline to config","text":"<p>Command:</p> <pre><code>$ kh promptui export &lt;module.path.piplineclass&gt; --output &lt;path/to/config/file.yml&gt;\n</code></pre> <p>where: - <code>&lt;module.path.pipelineclass&gt;</code> is a dot-separated path to the pipeline. For example, if your pipeline can be accessed with <code>from projectA.pipelines import AnsweringPipeline</code>, then this value is <code>projectA.pipelines.AnswerPipeline</code>. - <code>&lt;path/to/config/file.yml&gt;</code> is the target file path that the config will be exported to. If the config file already exists, and contains information of other pipelines, the config of current pipeline will additionally be added. If it contains information of the current pipeline (in the past), the old information will be replaced.</p> <p>By default, all params in a pipeline (including nested params) will be export to the configuration file. For params that you do not wish to expose to the UI, you can directly remove them from the config YAML file. You can also annotate those param with <code>ignore_ui=True</code>, and they will be ignored in the config generation process. Example:</p> <pre><code>class Pipeline(BaseComponent):\n    param1: str = Param(default=\"hello\")\n    param2: str = Param(default=\"goodbye\", ignore_ui=True)\n</code></pre> <p>Declared as above, and <code>param1</code> will show up in the config YAML file, while <code>param2</code> will not.</p>"},{"location":"utilities/#step-3-customize-the-config","title":"Step 3 - Customize the config","text":"<p>AIR can further edit the config file in this step to get the most suitable UI (step 4) with their tasks. The exported config will have this overall schema:</p> <pre><code>&lt;module.path.pipelineclass1&gt;:\n  params:\n    ... (Detail param information to initiate a pipeline. This corresponds to the pipeline init parameters.)\n  inputs:\n    ... (Detail the input of the pipeline e.g. a text prompt, an FNOL... This corresponds to the params of `run(...)` method.)\n  outputs:\n    ... (Detail the output of the pipeline e.g. prediction, accuracy... This is the output information we wish to see in the UI.)\n  logs:\n    ... (Detail what information should show up in the log.)\n</code></pre>"},{"location":"utilities/#input-and-params","title":"Input and params","text":"<p>The inputs section have the overall schema as follow:</p> <pre><code>inputs:\n  &lt;input-variable-name-1&gt;:\n    component: &lt;supported-UI-component&gt;\n    params: # this section is optional)\n      value: &lt;default-value&gt;\n  &lt;input-variable-name-2&gt;:\n    ... # similar to above\nparams:\n  &lt;param-variable-name-1&gt;:\n    ... # similar to those in the inputs\n</code></pre> <p>The list of supported prompt UI and their corresponding gradio UI components:</p> <pre><code>COMPONENTS_CLASS = {\n    \"text\": gr.components.Textbox,\n    \"checkbox\": gr.components.CheckboxGroup,\n    \"dropdown\": gr.components.Dropdown,\n    \"file\": gr.components.File,\n    \"image\": gr.components.Image,\n    \"number\": gr.components.Number,\n    \"radio\": gr.components.Radio,\n    \"slider\": gr.components.Slider,\n}\n</code></pre>"},{"location":"utilities/#outputs","title":"Outputs","text":"<p>The outputs are a list of variables that we wish to show in the UI. Since in Python, the function output doesn't have variable name, so output declaration is a little bit different than input and param declaration:</p> <pre><code>outputs:\n  - component: &lt;supported-UI-component&gt;\n    step: &lt;name-of-pipeline-step&gt;\n    item: &lt;jsonpath way to retrieve the info&gt;\n  - ... # similar to above\n</code></pre> <p>where: - component: the same text string and corresponding Gradio UI as in inputs &amp; params - step: the pipeline step that we wish to look fetch and show output on the UI - item: the jsonpath mechanism to get the targeted variable from the step above</p>"},{"location":"utilities/#logs","title":"Logs","text":"<p>The logs show a list of sheetname and how to retrieve the desired information.</p> <pre><code>logs:\n  &lt;logname&gt;:\n    inputs:\n      - name: &lt;column name&gt;\n        step: &lt;the pipeline step that we would wish to see the input&gt;\n        variable: &lt;the variable in the step&gt;\n      - ...\n    outputs:\n      - name: &lt;column name&gt;\n        step: &lt;the pipeline step that we would wish to see the output&gt;\n        item: &lt;how to retrieve the output of that step&gt;\n</code></pre>"},{"location":"utilities/#step-4-5-spin-up-prompt-engineering-ui-perform-prompt-engineering","title":"Step 4 + 5 - Spin up prompt engineering UI + Perform prompt engineering","text":"<p>Command:</p> <pre><code>$ kh promptui run &lt;path/to/config/file.yml&gt;\n</code></pre> <p>This will generate an UI as follow:</p> <p></p> <p>where: - The tabs at the top of the UI corresponds to the pipeline to do prompt engineering. - The inputs and params tabs allow users to edit (these corresponds to the inputs and params in the config file). - The outputs panel holds the UI elements to show the outputs defined in config file. - The Run button: will execute pipeline with the supplied inputs and params, and render result in the outputs panel. - The Export button: will export the logs of all the run to an Excel files users to inspect for best set of params.</p>"},{"location":"utilities/#step-6-export-to-excel","title":"Step 6 - Export to Excel","text":"<p>Upon clicking export, the users can download Excel file.</p>"},{"location":"utilities/#chat-pipeline","title":"Chat pipeline","text":"<p>Chat pipeline workflow is different from simple pipeline workflow. In simple pipeline, each Run creates a set of output, input and params for users to compare. In chat pipeline, each Run is not a one-off run, but a long interactive session. Hence, the workflow is as follow:</p> <ol> <li>Set the desired parameters.</li> <li>Click \"New chat\" to start a chat session with the supplied parameters. This set of parameters will persist until the end of the chat session. During an ongoing chat session, changing the parameters will not take any effect.</li> <li>Chat and interact with the chat bot on the right panel. You can add any additional input (if any), and they will be supplied to the chatbot.</li> <li>During chat, the log of the chat will show up in the \"Output\" tabs. This is empty by default, so if you want to show the log here, tell the AI developers to configure the UI settings.</li> <li>When finishing chat, select your preference in the radio box. Click \"End chat\". This will save the chat log and the preference to disk.</li> <li>To compare the result of different run, click \"Export\" to get an Excel spreadsheet summary of different run.</li> </ol>"},{"location":"examples/","title":"Index","text":"<ul> <li>:fontawesome-brands-html5: HTML for content and structure</li> <li>:fontawesome-brands-js: JavaScript for interactivity</li> <li>:fontawesome-brands-css3: CSS for text running out of boxes</li> <li>:fontawesome-brands-internet-explorer: Internet Explorer ... huh?</li> </ul>"},{"location":"examples/NAV/","title":"NAV","text":"<ul> <li>Insurance Mvp</li> <li>Promptui</li> <li>Example2</li> <li>Tmnf</li> </ul>"},{"location":"examples/insurance_mvp/","title":"Example of MVP pipeline for insurance","text":""},{"location":"examples/promptui/","title":"Example project with prompt engineering tool","text":"Indexing pipeline Generation pipeline"},{"location":"examples/promptui/#prerequisite","title":"Prerequisite","text":"<ul> <li>Have <code>kotaemon</code> installed.</li> <li>Have OPENAI_API_KEY for the https://aurora-nlp-2.openai.azure.com/ endpoint   (can look at the <code>ai_aurora</code> Slack channel).</li> </ul>"},{"location":"examples/promptui/#run","title":"Run","text":"<ol> <li>Create UI config file: <code>python run_ui.py --export</code>. This will create a    boilerplate config.yml file that describe the inputs, outputs and params for    each of the pipeline.</li> <li>Edit the config file. You can:</li> <li>Remove params that you don't want the users to modify.</li> <li>Switch the params default value, or the params UI component.</li> <li>Remove or add more outputs that you want to see in the UI. You can copy the      <code>final.yml</code> to <code>config.yml</code> to see the UI in the screenshots. (*)</li> <li>Add the <code>logs</code> section in order for the export functionality to work.      Further information regarding logs can be referred      here. File <code>final.yml</code> also contains sample logs section.</li> <li>Run the UI: <code>python run_ui.py</code>. This will run the UI according to the config.</li> <li>Do some Run. Then you can try the Export function.</li> </ol>"},{"location":"examples/promptui/#add-outputs-to-see-in-the-ui","title":"Add outputs to see in the UI","text":"<p>By default, only the pipeline final output will show up on the UI. You can further specify the steps in the pipeline that you want to see the outputs. For example, by default, this is the outputs section in the indexing pipeline:</p> <pre><code>pipeline.IndexingPipeline:\n  ...\n  outputs:\n  - component: text\n    step: .\n  ...\n</code></pre> <p>If you want to see the outputs of the embedding step, you can add the <code>.embedding</code> step in the outputs, so the outputs section of the config will look like this:</p> <pre><code>pipeline.IndexingPipeline:\n  ...\n  outputs:\n  - component: text\n    step: .\n  - component: text\n    step: .embedding\n  ...\n</code></pre> <p>This way, the output section will have 2 output textboxes rather than originally</p> <p>Similarly, this is the default outputs section in the question answering pipeline:</p> <pre><code>pipeline.QuestionAnsweringPipeline:\n  ...\n  outputs:\n  - component: text\n    step: .\n  ...\n</code></pre> <p>You can add the <code>.prompt</code> step to also views the prompt.</p> <pre><code>pipeline.QuestionAnsweringPipeline:\n  ...\n  outputs:\n  - component: text\n    step: .\n  - component: text\n    step: .prompt\n  ...\n</code></pre>"},{"location":"examples/example2/","title":"Example2 adoption","text":"<p>This example project showcases the ability of <code>kotaemon</code> on project Example2.</p> <p>This example project corresponds to Example2's code at this commit here. The most up-to-date Example2 information should reside in the latest commit of that code base, so if you want to understand Example2, you should refer there.</p>"},{"location":"examples/example2/#prerequisite","title":"Prerequisite","text":"<ul> <li>Install <code>kotaemon</code></li> <li>Example2 data: <code>scenarios.json</code>. Please ask on <code>#data_request_for_research</code>   channel as follow:</li> </ul> <pre><code>@channel, I would like to try out the Example2 demo and would like access to the\nExample2's artifacts.\n</code></pre>"},{"location":"examples/example2/#contributing","title":"Contributing","text":"<p>Please feel free to update this example projects to newer version of Example2. In case you want the kotaemon members to update, please inform the channel <code>#llm-productization</code> on Slack.</p>"},{"location":"examples/example1/","title":"Sample Example1 project","text":"<p>This sample project demonstrates the ability of <code>kotaemon</code> on the FNOL decision problem presented in Example1 project.</p> <p>The Example1 pipeline contains 3 sub-pipelines:</p> <ul> <li>Sub-Pipeline 1: 3-factor judgement</li> <li>Sub-Pipeline 2: Injury</li> <li>Sub-Pipeline 3: Disclaimer</li> </ul> <pre><code>flowchart TD\n    classDef VectorStore fill:#dcecfe,stroke:#dcecfe,color:black\n    classDef LLM fill:#e6ffbe,stroke:#e6ffbe,color:black\n\n    subgraph legend\n        legend-llm[LLM call]:::LLM\n        legend-vector-store[Index/Retrieval]:::VectorStore\n    end\n    fnol ~~~~~~~~~ legend\n\n    fnol[Test FNOL] --&gt; is_factor[is factor or not]\n    fnol --&gt; sudden[Sudden or not]\n    fnol --&gt; coincidence[Coincidence or not]\n    fnol --&gt; external[External or not]\n    fnol --&gt; vectorize[Vectorize]\n\n    subgraph 3factor-flow [3-factor]\n        sudden:::LLM --&gt; concat[Concat]\n        coincidence:::LLM --&gt; concat\n        external:::LLM --&gt; concat\n        concat --&gt; is_factor\n\n        vectorize:::VectorStore --&gt; retrieve-top5[\"Retrieve top-5\n                                                    relevant that has\n                                                    label as Not liable\"]:::VectorStore\n            --&gt; rank-top5[\"Rank the 5\n                            retrieved FNOLs\"]:::LLM\n            --&gt;is_factor\n\n        is_factor:::LLM --&gt; parse-3factor[Parse]\n            --&gt; 3factor-decision[\"3-factor decision\n                                    Yes/No\"]\n    end\n\n\n    fnol --&gt; injury[Injury or not]\n\n    subgraph injury-flow [Injury]\n        injury:::LLM -----&gt; parse-injury[Parse]\n            --&gt; injury-decision[\"Injury decision\n                                Yes/No\"]\n    end\n\n    fnol --&gt; disclaimer[\"Does the FNOL match\n                        any type of disclaimer\"]\n    fnol --&gt; disaster\n    fnol --&gt; driving\n    fnol --&gt; surgery\n    fnol --&gt; ball-games\n    fnol --&gt; altercation-or-brain\n\n    subgraph disclaimer-flow [Disclaimer]\n        disclaimer:::LLM --&gt; type2{\"match\n                                    type 2\"}\n            --&gt; disaster[\"Relate to\n                            earthquake, volcanic\n                            eruption,tsunami,\n                            landslide, flood\"]:::LLM\n            ---&gt; disaster-parse[Parse]\n\n        disclaimer --&gt; type6{\"match\n                                type 6\"}\n            --&gt; driving[\"Relate to\n                        driving special cases\"]:::LLM\n            ---&gt; driving-parse[Parse]\n\n        disclaimer --&gt; type9{\"match\n                              type 9\"}\n            --&gt; surgery[\"relate to\n                        pain/injury as\n                        result of surgery\"]:::LLM\n            ---&gt; surgery-parse[Parse]\n\n        disclaimer --&gt; type11{\"match\n                                type 11\"}\n            --&gt; ball-games[\"relate to\n                            common ball games\n                            like soccer, football,\n                            basketball, baseball\"]:::LLM\n            ---&gt; ball-games-parse[Parse]\n\n        disclaimer --&gt; no-match{\"doesn't\n                                  match\"}\n            --&gt; altercation-or-brain[\"relate to\n                                        physical altercation\n                                        or brain disease\"]:::LLM\n            ---&gt; altercation-or-brain-parse[Parse]\n\n        disaster-parse --&gt; disclaimer-decision[\"Disclaimer decision\n                                                Yes/No\"]\n        driving-parse --&gt; disclaimer-decision\n        surgery-parse --&gt; disclaimer-decision\n        ball-games-parse --&gt; disclaimer-decision\n        altercation-or-brain-parse --&gt; disclaimer-decision\n    end\n\n    3factor-decision --&gt; final[\"Final judgement: Liable when\n                                Yes 3 factor,\n                                Yes injury,\n                                No disclaimer.\n                                Not liable otherwise\"]\n    injury-decision --&gt; final\n    disclaimer-decision --&gt; final</code></pre> <p>The overall visualization in Example1 is documented here.</p> <p>To setup:</p> <ul> <li>Make sure <code>kotaemon</code> and the modules in <code>requirements.txt</code> are installed.</li> <li>The <code>assets</code> folder is downloaded and unzip in this working directory.</li> <li>The Azure credentials are obtained.</li> </ul> <p>Then:</p> <ul> <li>Run a holistic pipeline: <code>python pipeline.py</code>.</li> <li>Run the test: <code>pytest test_pipelines.py</code>.</li> </ul>"},{"location":"reference/NAV/","title":"NAV","text":"<ul> <li>Agents<ul> <li>Base</li> <li>Io<ul> <li>Base</li> </ul> </li> <li>Langchain Based</li> <li>React<ul> <li>Agent</li> <li>Prompt</li> </ul> </li> <li>Rewoo<ul> <li>Agent</li> <li>Planner</li> <li>Prompt</li> <li>Solver</li> </ul> </li> <li>Tools<ul> <li>Base</li> <li>Google</li> <li>Llm</li> <li>Wikipedia</li> </ul> </li> <li>Utils</li> </ul> </li> <li>Base<ul> <li>Component</li> <li>Schema</li> </ul> </li> <li>Chatbot<ul> <li>Base</li> <li>Simple Respondent</li> </ul> </li> <li>Cli</li> <li>Embeddings<ul> <li>Base</li> <li>Langchain Based</li> </ul> </li> <li>Indices<ul> <li>Base</li> <li>Extractors<ul> <li>Doc Parsers</li> </ul> </li> <li>Ingests<ul> <li>Files</li> </ul> </li> <li>Qa<ul> <li>Citation</li> <li>Text Based</li> </ul> </li> <li>Rankings<ul> <li>Base</li> <li>Cohere</li> <li>Llm</li> </ul> </li> <li>Splitters</li> <li>Vectorindex</li> </ul> </li> <li>Llms<ul> <li>Branching</li> <li>Chats<ul> <li>Base</li> <li>Langchain Based</li> </ul> </li> <li>Completions<ul> <li>Base</li> <li>Langchain Based</li> </ul> </li> <li>Cot</li> <li>Linear</li> <li>Prompts<ul> <li>Base</li> <li>Template</li> </ul> </li> </ul> </li> <li>Loaders<ul> <li>Base</li> <li>Excel Loader</li> <li>Mathpix Loader</li> <li>Ocr Loader</li> <li>Unstructured Loader</li> <li>Utils<ul> <li>Box</li> <li>Pdf Ocr</li> <li>Table</li> </ul> </li> </ul> </li> <li>Parsers<ul> <li>Regex Extractor</li> </ul> </li> <li>Storages<ul> <li>Docstores<ul> <li>Base</li> <li>Elasticsearch</li> <li>In Memory</li> <li>Simple File</li> </ul> </li> <li>Vectorstores<ul> <li>Base</li> <li>Chroma</li> <li>In Memory</li> <li>Simple File</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/cli/","title":"Cli","text":""},{"location":"reference/cli/#cli.export","title":"export","text":"<pre><code>export(export_path, output)\n</code></pre> <p>Export a pipeline to a config file</p> Source code in <code>kotaemon\\cli.py</code> <pre><code>@promptui.command()\n@click.argument(\"export_path\", nargs=1)\n@click.option(\"--output\", default=\"promptui.yml\", show_default=True, required=False)\ndef export(export_path, output):\n    \"\"\"Export a pipeline to a config file\"\"\"\n    import sys\n\n    from theflow.utils.modules import import_dotted_string\n\n    from kotaemon.contribs.promptui.config import export_pipeline_to_config\n\n    sys.path.append(os.getcwd())\n    cls = import_dotted_string(export_path, safe=False)\n    export_pipeline_to_config(cls, output)\n    check_config_format(output)\n</code></pre>"},{"location":"reference/cli/#cli.run","title":"run","text":"<pre><code>run(run_path, share, username, password, appname, port)\n</code></pre> <p>Run the UI from a config file</p> <p>Examples:</p> <pre><code>\b\n# Run with default config file\n$ kh promptui run\n\n\b\n# Run with username and password supplied\n$ kh promptui run --username admin --password password\n\n\b\n# Run with username and prompted password\n$ kh promptui run --username admin\n\n# Run and share to promptui\n# kh promptui run --username admin --password password --share --appname hey                 --port 7861\n</code></pre> Source code in <code>kotaemon\\cli.py</code> <pre><code>@promptui.command()\n@click.argument(\"run_path\", required=False, default=\"promptui.yml\")\n@click.option(\n    \"--share\",\n    is_flag=True,\n    show_default=True,\n    default=False,\n    help=\"Share the app through Gradio. Requires --username to enable authentication.\",\n)\n@click.option(\n    \"--username\",\n    required=False,\n    help=(\n        \"Username for the user. If not provided, the promptui will not have \"\n        \"authentication.\"\n    ),\n)\n@click.option(\n    \"--password\",\n    required=False,\n    help=\"Password for the user. If not provided, will be prompted.\",\n)\n@click.option(\n    \"--appname\",\n    required=False,\n    help=\"The share app subdomain. Requires --share and --username\",\n)\n@click.option(\n    \"--port\",\n    required=False,\n    help=\"Port to run the app. If not provided, will $GRADIO_SERVER_PORT (7860)\",\n)\ndef run(run_path, share, username, password, appname, port):\n    \"\"\"Run the UI from a config file\n\n    Examples:\n\n        \\b\n        # Run with default config file\n        $ kh promptui run\n\n        \\b\n        # Run with username and password supplied\n        $ kh promptui run --username admin --password password\n\n        \\b\n        # Run with username and prompted password\n        $ kh promptui run --username admin\n\n        # Run and share to promptui\n        # kh promptui run --username admin --password password --share --appname hey \\\n                --port 7861\n    \"\"\"\n    import sys\n\n    from kotaemon.contribs.promptui.ui import build_from_dict\n\n    sys.path.append(os.getcwd())\n\n    check_config_format(run_path)\n    demo = build_from_dict(run_path)\n\n    params: dict = {}\n    if username is not None:\n        if password is not None:\n            auth = (username, password)\n        else:\n            auth = (username, click.prompt(\"Password\", hide_input=True))\n        params[\"auth\"] = auth\n\n    port = int(port) if port else int(os.getenv(\"GRADIO_SERVER_PORT\", \"7860\"))\n    params[\"server_port\"] = port\n\n    if share:\n        if username is None:\n            raise ValueError(\n                \"Username must be provided to enable authentication for sharing\"\n            )\n        if appname:\n            from kotaemon.contribs.promptui.tunnel import Tunnel\n\n            tunnel = Tunnel(\n                appname=str(appname), username=str(username), local_port=port\n            )\n            url = tunnel.run()\n            print(f\"App is shared at {url}\")\n        else:\n            params[\"share\"] = True\n            print(\"App is shared at Gradio\")\n\n    demo.launch(**params)\n</code></pre>"},{"location":"reference/cli/#cli.makedoc","title":"makedoc","text":"<pre><code>makedoc(module, output, separation_level)\n</code></pre> <p>Make documentation for module <code>module</code></p> <p>Example:</p> <pre><code>\b\n# Make component documentation for kotaemon library\n$ kh makedoc kotaemon\n</code></pre> Source code in <code>kotaemon\\cli.py</code> <pre><code>@main.command()\n@click.argument(\"module\", required=True)\n@click.option(\n    \"--output\", default=\"docs.md\", required=False, help=\"The output markdown file\"\n)\n@click.option(\n    \"--separation-level\", required=False, default=1, help=\"Organize markdown layout\"\n)\ndef makedoc(module, output, separation_level):\n    \"\"\"Make documentation for module `module`\n\n    Example:\n\n        \\b\n        # Make component documentation for kotaemon library\n        $ kh makedoc kotaemon\n    \"\"\"\n    from kotaemon.contribs.docs import make_doc\n\n    make_doc(module, output, separation_level)\n    print(f\"Documentation exported to {output}\")\n</code></pre>"},{"location":"reference/cli/#cli.start_project","title":"start_project","text":"<pre><code>start_project(template)\n</code></pre> <p>Start a project from a template.</p> <p>Important: the value for --template corresponds to the name of the template folder, which is located at https://github.com/Cinnamon/kotaemon/tree/main/templates The default value is \"project-default\", which should work when you are starting a client project.</p> Source code in <code>kotaemon\\cli.py</code> <pre><code>@main.command()\n@click.option(\n    \"--template\",\n    default=\"project-default\",\n    required=False,\n    help=\"Template name\",\n    show_default=True,\n)\ndef start_project(template):\n    \"\"\"Start a project from a template.\n\n    Important: the value for --template corresponds to the name of the template folder,\n    which is located at https://github.com/Cinnamon/kotaemon/tree/main/templates\n    The default value is \"project-default\", which should work when you are starting a\n    client project.\n    \"\"\"\n\n    print(\"Retrieving template...\")\n    os.system(\n        \"cookiecutter git@github.com:Cinnamon/kotaemon.git \"\n        f\"--directory='templates/{template}'\"\n    )\n</code></pre>"},{"location":"reference/agents/","title":"Agents","text":""},{"location":"reference/agents/#agents.BaseAgent","title":"BaseAgent","text":"<p>             Bases: <code>BaseComponent</code></p> <p>Define base agent interface</p> Source code in <code>kotaemon\\agents\\base.py</code> <pre><code>class BaseAgent(BaseComponent):\n    \"\"\"Define base agent interface\"\"\"\n\n    name: str = Param(help=\"Name of the agent.\")\n    agent_type: AgentType = Param(help=\"Agent type, must be one of AgentType\")\n    description: str = Param(\n        help=(\n            \"Description used to tell the model how/when/why to use the agent. You can\"\n            \" provide few-shot examples as a part of the description. This will be\"\n            \" input to the prompt of LLM.\"\n        )\n    )\n    llm: Optional[BaseLLM] = Node(\n        help=(\n            \"LLM to be used for the agent (optional). LLM must implement BaseLLM\"\n            \" interface.\"\n        )\n    )\n    prompt_template: Optional[Union[PromptTemplate, dict[str, PromptTemplate]]] = Param(\n        help=\"A prompt template or a dict to supply different prompt to the agent\"\n    )\n    plugins: list[BaseTool] = Param(\n        default_callback=lambda _: [],\n        help=\"List of plugins / tools to be used in the agent\",\n    )\n\n    @staticmethod\n    def safeguard_run(run_func, *args, **kwargs):\n        def wrapper(self, *args, **kwargs):\n            try:\n                return run_func(self, *args, **kwargs)\n            except Exception as e:\n                return AgentOutput(\n                    text=\"\",\n                    agent_type=self.agent_type,\n                    status=\"failed\",\n                    error=str(e),\n                )\n\n        return wrapper\n\n    def add_tools(self, tools: list[BaseTool]) -&gt; None:\n        \"\"\"Helper method to add tools and update agent state if needed\"\"\"\n        self.plugins.extend(tools)\n\n    def run(self, *args, **kwargs) -&gt; AgentOutput | list[AgentOutput]:\n        \"\"\"Run the component.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/agents/#agents.BaseAgent.add_tools","title":"add_tools","text":"<pre><code>add_tools(tools)\n</code></pre> <p>Helper method to add tools and update agent state if needed</p> Source code in <code>kotaemon\\agents\\base.py</code> <pre><code>def add_tools(self, tools: list[BaseTool]) -&gt; None:\n    \"\"\"Helper method to add tools and update agent state if needed\"\"\"\n    self.plugins.extend(tools)\n</code></pre>"},{"location":"reference/agents/#agents.BaseAgent.run","title":"run","text":"<pre><code>run(*args, **kwargs)\n</code></pre> <p>Run the component.</p> Source code in <code>kotaemon\\agents\\base.py</code> <pre><code>def run(self, *args, **kwargs) -&gt; AgentOutput | list[AgentOutput]:\n    \"\"\"Run the component.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/agents/#agents.AgentFinish","title":"AgentFinish","text":"<p>             Bases: <code>NamedTuple</code></p> <p>Agent's return value when finishing execution.</p> <p>Parameters:</p> Name Type Description Default <code>return_values</code> <p>The return values of the agent.</p> required <code>log</code> <p>The log message.</p> required Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>class AgentFinish(NamedTuple):\n    \"\"\"Agent's return value when finishing execution.\n\n    Args:\n        return_values: The return values of the agent.\n        log: The log message.\n    \"\"\"\n\n    return_values: dict\n    log: str\n</code></pre>"},{"location":"reference/agents/#agents.AgentOutput","title":"AgentOutput","text":"<p>             Bases: <code>LLMInterface</code></p> <p>Output from an agent.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>The text output from the agent.</p> required <code>agent_type</code> <p>The type of agent.</p> required <code>status</code> <p>The status after executing the agent.</p> required <code>error</code> <p>The error message if any.</p> required Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>class AgentOutput(LLMInterface, extra=Extra.allow):  # type: ignore [call-arg]\n    \"\"\"Output from an agent.\n\n    Args:\n        text: The text output from the agent.\n        agent_type: The type of agent.\n        status: The status after executing the agent.\n        error: The error message if any.\n    \"\"\"\n\n    text: str\n    type: str = \"agent\"\n    agent_type: AgentType\n    status: Literal[\"finished\", \"stopped\", \"failed\"]\n    error: Optional[str] = None\n</code></pre>"},{"location":"reference/agents/#agents.AgentType","title":"AgentType","text":"<p>             Bases: <code>Enum</code></p> <p>Enumerated type for agent types.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>class AgentType(Enum):\n    \"\"\"\n    Enumerated type for agent types.\n    \"\"\"\n\n    openai = \"openai\"\n    openai_multi = \"openai_multi\"\n    openai_tool = \"openai_tool\"\n    self_ask = \"self_ask\"\n    react = \"react\"\n    rewoo = \"rewoo\"\n    vanilla = \"vanilla\"\n</code></pre>"},{"location":"reference/agents/#agents.BaseScratchPad","title":"BaseScratchPad","text":"<p>Base class for output handlers.</p>"},{"location":"reference/agents/#agents.BaseScratchPad--attributes","title":"Attributes:","text":"<p>logger : logging.Logger     The logger object to log messages.</p>"},{"location":"reference/agents/#agents.BaseScratchPad--methods","title":"Methods:","text":"<p>stop():     Stop the output.</p> <p>update_status(output: str, **kwargs):     Update the status of the output.</p> <p>thinking(name: str):     Log that a process is thinking.</p> <p>done(_all=False):     Log that the process is done.</p> <p>stream_print(item: str):     Not implemented.</p> <p>json_print(item: Dict[str, Any]):     Log a JSON object.</p> <p>panel_print(item: Any, title: str = \"Output\", stream: bool = False):     Log a panel output.</p> <p>clear():     Not implemented.</p> <p>print(content: str, **kwargs):     Log arbitrary content.</p> <p>format_json(json_obj: str):     Format a JSON object.</p> <p>debug(content: str, **kwargs):     Log a debug message.</p> <p>info(content: str, **kwargs):     Log an informational message.</p> <p>warning(content: str, **kwargs):     Log a warning message.</p> <p>error(content: str, **kwargs):     Log an error message.</p> <p>critical(content: str, **kwargs):     Log a critical message.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>class BaseScratchPad:\n    \"\"\"\n    Base class for output handlers.\n\n    Attributes:\n    -----------\n    logger : logging.Logger\n        The logger object to log messages.\n\n    Methods:\n    --------\n    stop():\n        Stop the output.\n\n    update_status(output: str, **kwargs):\n        Update the status of the output.\n\n    thinking(name: str):\n        Log that a process is thinking.\n\n    done(_all=False):\n        Log that the process is done.\n\n    stream_print(item: str):\n        Not implemented.\n\n    json_print(item: Dict[str, Any]):\n        Log a JSON object.\n\n    panel_print(item: Any, title: str = \"Output\", stream: bool = False):\n        Log a panel output.\n\n    clear():\n        Not implemented.\n\n    print(content: str, **kwargs):\n        Log arbitrary content.\n\n    format_json(json_obj: str):\n        Format a JSON object.\n\n    debug(content: str, **kwargs):\n        Log a debug message.\n\n    info(content: str, **kwargs):\n        Log an informational message.\n\n    warning(content: str, **kwargs):\n        Log a warning message.\n\n    error(content: str, **kwargs):\n        Log an error message.\n\n    critical(content: str, **kwargs):\n        Log a critical message.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the BaseOutput object.\n\n        \"\"\"\n        self.logger = logging\n        self.log = []\n\n    def stop(self):\n        \"\"\"\n        Stop the output.\n        \"\"\"\n\n    def update_status(self, output: str, **kwargs):\n        \"\"\"\n        Update the status of the output.\n        \"\"\"\n        if check_log():\n            self.logger.info(output)\n\n    def thinking(self, name: str):\n        \"\"\"\n        Log that a process is thinking.\n        \"\"\"\n        if check_log():\n            self.logger.info(f\"{name} is thinking...\")\n\n    def done(self, _all=False):\n        \"\"\"\n        Log that the process is done.\n        \"\"\"\n\n        if check_log():\n            self.logger.info(\"Done\")\n\n    def stream_print(self, item: str):\n        \"\"\"\n        Stream print.\n        \"\"\"\n\n    def json_print(self, item: Dict[str, Any]):\n        \"\"\"\n        Log a JSON object.\n        \"\"\"\n        if check_log():\n            self.logger.info(json.dumps(item, indent=2))\n\n    def panel_print(self, item: Any, title: str = \"Output\", stream: bool = False):\n        \"\"\"\n        Log a panel output.\n\n        Args:\n            item : Any\n                The item to log.\n            title : str, optional\n                The title of the panel, defaults to \"Output\".\n            stream : bool, optional\n        \"\"\"\n        if not stream:\n            self.log.append(item)\n        if check_log():\n            self.logger.info(\"-\" * 20)\n            self.logger.info(item)\n            self.logger.info(\"-\" * 20)\n\n    def clear(self):\n        \"\"\"\n        Not implemented.\n        \"\"\"\n\n    def print(self, content: str, **kwargs):\n        \"\"\"\n        Log arbitrary content.\n        \"\"\"\n        self.log.append(content)\n        if check_log():\n            self.logger.info(content)\n\n    def format_json(self, json_obj: str):\n        \"\"\"\n        Format a JSON object.\n        \"\"\"\n        formatted_json = json.dumps(json_obj, indent=2)\n        return formatted_json\n\n    def debug(self, content: str, **kwargs):\n        \"\"\"\n        Log a debug message.\n        \"\"\"\n        if check_log():\n            self.logger.debug(content, **kwargs)\n\n    def info(self, content: str, **kwargs):\n        \"\"\"\n        Log an informational message.\n        \"\"\"\n        if check_log():\n            self.logger.info(content, **kwargs)\n\n    def warning(self, content: str, **kwargs):\n        \"\"\"\n        Log a warning message.\n        \"\"\"\n        if check_log():\n            self.logger.warning(content, **kwargs)\n\n    def error(self, content: str, **kwargs):\n        \"\"\"\n        Log an error message.\n        \"\"\"\n        if check_log():\n            self.logger.error(content, **kwargs)\n\n    def critical(self, content: str, **kwargs):\n        \"\"\"\n        Log a critical message.\n        \"\"\"\n        if check_log():\n            self.logger.critical(content, **kwargs)\n</code></pre>"},{"location":"reference/agents/#agents.BaseScratchPad.stop","title":"stop","text":"<pre><code>stop()\n</code></pre> <p>Stop the output.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def stop(self):\n    \"\"\"\n    Stop the output.\n    \"\"\"\n</code></pre>"},{"location":"reference/agents/#agents.BaseScratchPad.update_status","title":"update_status","text":"<pre><code>update_status(output, **kwargs)\n</code></pre> <p>Update the status of the output.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def update_status(self, output: str, **kwargs):\n    \"\"\"\n    Update the status of the output.\n    \"\"\"\n    if check_log():\n        self.logger.info(output)\n</code></pre>"},{"location":"reference/agents/#agents.BaseScratchPad.thinking","title":"thinking","text":"<pre><code>thinking(name)\n</code></pre> <p>Log that a process is thinking.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def thinking(self, name: str):\n    \"\"\"\n    Log that a process is thinking.\n    \"\"\"\n    if check_log():\n        self.logger.info(f\"{name} is thinking...\")\n</code></pre>"},{"location":"reference/agents/#agents.BaseScratchPad.done","title":"done","text":"<pre><code>done(_all=False)\n</code></pre> <p>Log that the process is done.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def done(self, _all=False):\n    \"\"\"\n    Log that the process is done.\n    \"\"\"\n\n    if check_log():\n        self.logger.info(\"Done\")\n</code></pre>"},{"location":"reference/agents/#agents.BaseScratchPad.stream_print","title":"stream_print","text":"<pre><code>stream_print(item)\n</code></pre> <p>Stream print.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def stream_print(self, item: str):\n    \"\"\"\n    Stream print.\n    \"\"\"\n</code></pre>"},{"location":"reference/agents/#agents.BaseScratchPad.json_print","title":"json_print","text":"<pre><code>json_print(item)\n</code></pre> <p>Log a JSON object.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def json_print(self, item: Dict[str, Any]):\n    \"\"\"\n    Log a JSON object.\n    \"\"\"\n    if check_log():\n        self.logger.info(json.dumps(item, indent=2))\n</code></pre>"},{"location":"reference/agents/#agents.BaseScratchPad.panel_print","title":"panel_print","text":"<pre><code>panel_print(item, title='Output', stream=False)\n</code></pre> <p>Log a panel output.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <p>Any The item to log.</p> required <code>title</code> <p>str, optional The title of the panel, defaults to \"Output\".</p> <code>'Output'</code> <code>stream</code> <p>bool, optional</p> <code>False</code> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def panel_print(self, item: Any, title: str = \"Output\", stream: bool = False):\n    \"\"\"\n    Log a panel output.\n\n    Args:\n        item : Any\n            The item to log.\n        title : str, optional\n            The title of the panel, defaults to \"Output\".\n        stream : bool, optional\n    \"\"\"\n    if not stream:\n        self.log.append(item)\n    if check_log():\n        self.logger.info(\"-\" * 20)\n        self.logger.info(item)\n        self.logger.info(\"-\" * 20)\n</code></pre>"},{"location":"reference/agents/#agents.BaseScratchPad.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Not implemented.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def clear(self):\n    \"\"\"\n    Not implemented.\n    \"\"\"\n</code></pre>"},{"location":"reference/agents/#agents.BaseScratchPad.print","title":"print","text":"<pre><code>print(content, **kwargs)\n</code></pre> <p>Log arbitrary content.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def print(self, content: str, **kwargs):\n    \"\"\"\n    Log arbitrary content.\n    \"\"\"\n    self.log.append(content)\n    if check_log():\n        self.logger.info(content)\n</code></pre>"},{"location":"reference/agents/#agents.BaseScratchPad.format_json","title":"format_json","text":"<pre><code>format_json(json_obj)\n</code></pre> <p>Format a JSON object.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def format_json(self, json_obj: str):\n    \"\"\"\n    Format a JSON object.\n    \"\"\"\n    formatted_json = json.dumps(json_obj, indent=2)\n    return formatted_json\n</code></pre>"},{"location":"reference/agents/#agents.BaseScratchPad.debug","title":"debug","text":"<pre><code>debug(content, **kwargs)\n</code></pre> <p>Log a debug message.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def debug(self, content: str, **kwargs):\n    \"\"\"\n    Log a debug message.\n    \"\"\"\n    if check_log():\n        self.logger.debug(content, **kwargs)\n</code></pre>"},{"location":"reference/agents/#agents.BaseScratchPad.info","title":"info","text":"<pre><code>info(content, **kwargs)\n</code></pre> <p>Log an informational message.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def info(self, content: str, **kwargs):\n    \"\"\"\n    Log an informational message.\n    \"\"\"\n    if check_log():\n        self.logger.info(content, **kwargs)\n</code></pre>"},{"location":"reference/agents/#agents.BaseScratchPad.warning","title":"warning","text":"<pre><code>warning(content, **kwargs)\n</code></pre> <p>Log a warning message.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def warning(self, content: str, **kwargs):\n    \"\"\"\n    Log a warning message.\n    \"\"\"\n    if check_log():\n        self.logger.warning(content, **kwargs)\n</code></pre>"},{"location":"reference/agents/#agents.BaseScratchPad.error","title":"error","text":"<pre><code>error(content, **kwargs)\n</code></pre> <p>Log an error message.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def error(self, content: str, **kwargs):\n    \"\"\"\n    Log an error message.\n    \"\"\"\n    if check_log():\n        self.logger.error(content, **kwargs)\n</code></pre>"},{"location":"reference/agents/#agents.BaseScratchPad.critical","title":"critical","text":"<pre><code>critical(content, **kwargs)\n</code></pre> <p>Log a critical message.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def critical(self, content: str, **kwargs):\n    \"\"\"\n    Log a critical message.\n    \"\"\"\n    if check_log():\n        self.logger.critical(content, **kwargs)\n</code></pre>"},{"location":"reference/agents/#agents.LangchainAgent","title":"LangchainAgent","text":"<p>             Bases: <code>BaseAgent</code></p> <p>Wrapper for Langchain Agent</p> Source code in <code>kotaemon\\agents\\langchain_based.py</code> <pre><code>class LangchainAgent(BaseAgent):\n    \"\"\"Wrapper for Langchain Agent\"\"\"\n\n    name: str = \"LangchainAgent\"\n    agent_type: AgentType\n    description: str = \"LangchainAgent for answering multi-step reasoning questions\"\n    AGENT_TYPE_MAP = {\n        AgentType.openai: LCAgentType.OPENAI_FUNCTIONS,\n        AgentType.openai_multi: LCAgentType.OPENAI_MULTI_FUNCTIONS,\n        AgentType.react: LCAgentType.ZERO_SHOT_REACT_DESCRIPTION,\n        AgentType.self_ask: LCAgentType.SELF_ASK_WITH_SEARCH,\n    }\n    agent: Optional[LCAgentExecutor] = None\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        if self.agent_type not in self.AGENT_TYPE_MAP:\n            raise NotImplementedError(\n                f\"AgentType {self.agent_type } not supported by Langchain wrapper\"\n            )\n        self.update_agent_tools()\n\n    def update_agent_tools(self):\n        assert isinstance(self.llm, (ChatLLM, LLM))\n        langchain_plugins = [tool.to_langchain_format() for tool in self.plugins]\n\n        # a fix for search_doc tool name:\n        # use \"Intermediate Answer\" for self-ask agent\n        found_search_tool = False\n        if self.agent_type == AgentType.self_ask:\n            for plugin in langchain_plugins:\n                if plugin.name == \"search_doc\":\n                    plugin.name = \"Intermediate Answer\"\n                    langchain_plugins = [plugin]\n                    found_search_tool = True\n                    break\n\n        if self.agent_type != AgentType.self_ask or found_search_tool:\n            # reinit Langchain AgentExecutor\n            self.agent = initialize_agent(\n                langchain_plugins,\n                # TODO: could cause bugs for non-langchain llms\n                # related to https://github.com/Cinnamon/kotaemon/issues/73\n                self.llm._obj,  # type: ignore\n                agent=self.AGENT_TYPE_MAP[self.agent_type],\n                handle_parsing_errors=True,\n                verbose=True,\n            )\n\n    def add_tools(self, tools: List[BaseTool]) -&gt; None:\n        super().add_tools(tools)\n        self.update_agent_tools()\n        return\n\n    def run(self, instruction: str) -&gt; AgentOutput:\n        assert (\n            self.agent is not None\n        ), \"Lanchain AgentExecutor is not correclty initialized\"\n\n        # Langchain AgentExecutor call\n        output = self.agent(instruction)[\"output\"]\n\n        return AgentOutput(\n            text=output,\n            agent_type=self.agent_type,\n            status=\"finished\",\n        )\n</code></pre>"},{"location":"reference/agents/#agents.ReactAgent","title":"ReactAgent","text":"<p>             Bases: <code>BaseAgent</code></p> <p>Sequential ReactAgent class inherited from BaseAgent. Implementing ReAct agent paradigm https://arxiv.org/pdf/2210.03629.pdf</p> Source code in <code>kotaemon\\agents\\react\\agent.py</code> <pre><code>class ReactAgent(BaseAgent):\n    \"\"\"\n    Sequential ReactAgent class inherited from BaseAgent.\n    Implementing ReAct agent paradigm https://arxiv.org/pdf/2210.03629.pdf\n    \"\"\"\n\n    name: str = \"ReactAgent\"\n    agent_type: AgentType = AgentType.react\n    description: str = \"ReactAgent for answering multi-step reasoning questions\"\n    llm: BaseLLM\n    prompt_template: Optional[PromptTemplate] = None\n    plugins: list[BaseTool] = Param(\n        default_callback=lambda _: [], help=\"List of tools to be used in the agent. \"\n    )\n    examples: dict[str, str | list[str]] = Param(\n        default_callback=lambda _: {}, help=\"Examples to be used in the agent. \"\n    )\n    intermediate_steps: list[tuple[AgentAction | AgentFinish, str]] = Param(\n        default_callback=lambda _: [],\n        help=\"List of AgentAction and observation (tool) output\",\n    )\n    max_iterations: int = 10\n    strict_decode: bool = False\n\n    def _compose_plugin_description(self) -&gt; str:\n        \"\"\"\n        Compose the worker prompt from the workers.\n\n        Example:\n        toolname1[input]: tool1 description\n        toolname2[input]: tool2 description\n        \"\"\"\n        prompt = \"\"\n        try:\n            for plugin in self.plugins:\n                prompt += f\"{plugin.name}[input]: {plugin.description}\\n\"\n        except Exception:\n            raise ValueError(\"Worker must have a name and description.\")\n        return prompt\n\n    def _construct_scratchpad(\n        self, intermediate_steps: list[tuple[AgentAction | AgentFinish, str]] = []\n    ) -&gt; str:\n        \"\"\"Construct the scratchpad that lets the agent continue its thought process.\"\"\"\n        thoughts = \"\"\n        for action, observation in intermediate_steps:\n            thoughts += action.log\n            thoughts += f\"\\nObservation: {observation}\\nThought:\"\n        return thoughts\n\n    def _parse_output(self, text: str) -&gt; Optional[AgentAction | AgentFinish]:\n        \"\"\"\n        Parse text output from LLM for the next Action or Final Answer\n        Using Regex to parse \"Action:\\n Action Input:\\n\" for the next Action\n        Using FINAL_ANSWER_ACTION to parse Final Answer\n\n        Args:\n            text[str]: input text to parse\n        \"\"\"\n        includes_answer = FINAL_ANSWER_ACTION in text\n        regex = (\n            r\"Action\\s*\\d*\\s*:[\\s]*(.*?)[\\s]*Action\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n        )\n        action_match = re.search(regex, text, re.DOTALL)\n        action_output: Optional[AgentAction | AgentFinish] = None\n        if action_match:\n            if includes_answer:\n                raise Exception(\n                    \"Parsing LLM output produced both a final answer \"\n                    f\"and a parse-able action: {text}\"\n                )\n            action = action_match.group(1).strip()\n            action_input = action_match.group(2)\n            tool_input = action_input.strip(\" \")\n            # ensure if its a well formed SQL query we don't remove any trailing \" chars\n            if tool_input.startswith(\"SELECT \") is False:\n                tool_input = tool_input.strip('\"')\n\n            action_output = AgentAction(action, tool_input, text)\n\n        elif includes_answer:\n            action_output = AgentFinish(\n                {\"output\": text.split(FINAL_ANSWER_ACTION)[-1].strip()}, text\n            )\n        else:\n            if self.strict_decode:\n                raise Exception(f\"Could not parse LLM output: `{text}`\")\n            else:\n                action_output = AgentFinish({\"output\": text}, text)\n\n        return action_output\n\n    def _compose_prompt(self, instruction) -&gt; str:\n        \"\"\"\n        Compose the prompt from template, worker description, examples and instruction.\n        \"\"\"\n        agent_scratchpad = self._construct_scratchpad(self.intermediate_steps)\n        tool_description = self._compose_plugin_description()\n        tool_names = \", \".join([plugin.name for plugin in self.plugins])\n        if self.prompt_template is None:\n            from .prompt import zero_shot_react_prompt\n\n            self.prompt_template = zero_shot_react_prompt\n        return self.prompt_template.populate(\n            instruction=instruction,\n            agent_scratchpad=agent_scratchpad,\n            tool_description=tool_description,\n            tool_names=tool_names,\n        )\n\n    def _format_function_map(self) -&gt; dict[str, BaseTool]:\n        \"\"\"Format the function map for the open AI function API.\n\n        Return:\n            Dict[str, Callable]: The function map.\n        \"\"\"\n        # Map the function name to the real function object.\n        function_map = {}\n        for plugin in self.plugins:\n            function_map[plugin.name] = plugin\n        return function_map\n\n    def clear(self):\n        \"\"\"\n        Clear and reset the agent.\n        \"\"\"\n        self.intermediate_steps = []\n\n    def run(self, instruction, max_iterations=None) -&gt; AgentOutput:\n        \"\"\"\n        Run the agent with the given instruction.\n\n        Args:\n            instruction: Instruction to run the agent with.\n            max_iterations: Maximum number of iterations\n                of reasoning steps, defaults to 10.\n\n        Return:\n            AgentOutput object.\n        \"\"\"\n        if not max_iterations:\n            max_iterations = self.max_iterations\n        assert max_iterations &gt; 0\n\n        self.clear()\n        logging.info(f\"Running {self.name} with instruction: {instruction}\")\n        total_cost = 0.0\n        total_token = 0\n        status = \"failed\"\n        response_text = None\n\n        for step_count in range(1, max_iterations + 1):\n            prompt = self._compose_prompt(instruction)\n            logging.info(f\"Prompt: {prompt}\")\n            response = self.llm(\n                prompt, stop=[\"Observation:\"]\n            )  # could cause bugs if llm doesn't have `stop` as a parameter\n            response_text = response.text\n            logging.info(f\"Response: {response_text}\")\n            action_step = self._parse_output(response_text)\n            if action_step is None:\n                raise ValueError(\"Invalid action\")\n            is_finished_chain = isinstance(action_step, AgentFinish)\n            if is_finished_chain:\n                result = \"\"\n            else:\n                assert isinstance(action_step, AgentAction)\n                action_name = action_step.tool\n                tool_input = action_step.tool_input\n                logging.info(f\"Action: {action_name}\")\n                logging.info(f\"Tool Input: {tool_input}\")\n                result = self._format_function_map()[action_name](tool_input)\n                logging.info(f\"Result: {result}\")\n\n            self.intermediate_steps.append((action_step, result))\n            if is_finished_chain:\n                logging.info(f\"Finished after {step_count} steps.\")\n                status = \"finished\"\n                break\n        else:\n            status = \"stopped\"\n\n        return AgentOutput(\n            text=response_text,\n            agent_type=self.agent_type,\n            status=status,\n            total_tokens=total_token,\n            total_cost=total_cost,\n            intermediate_steps=self.intermediate_steps,\n            max_iterations=max_iterations,\n        )\n</code></pre>"},{"location":"reference/agents/#agents.ReactAgent.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Clear and reset the agent.</p> Source code in <code>kotaemon\\agents\\react\\agent.py</code> <pre><code>def clear(self):\n    \"\"\"\n    Clear and reset the agent.\n    \"\"\"\n    self.intermediate_steps = []\n</code></pre>"},{"location":"reference/agents/#agents.ReactAgent.run","title":"run","text":"<pre><code>run(instruction, max_iterations=None)\n</code></pre> <p>Run the agent with the given instruction.</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <p>Instruction to run the agent with.</p> required <code>max_iterations</code> <p>Maximum number of iterations of reasoning steps, defaults to 10.</p> <code>None</code> Return <p>AgentOutput object.</p> Source code in <code>kotaemon\\agents\\react\\agent.py</code> <pre><code>def run(self, instruction, max_iterations=None) -&gt; AgentOutput:\n    \"\"\"\n    Run the agent with the given instruction.\n\n    Args:\n        instruction: Instruction to run the agent with.\n        max_iterations: Maximum number of iterations\n            of reasoning steps, defaults to 10.\n\n    Return:\n        AgentOutput object.\n    \"\"\"\n    if not max_iterations:\n        max_iterations = self.max_iterations\n    assert max_iterations &gt; 0\n\n    self.clear()\n    logging.info(f\"Running {self.name} with instruction: {instruction}\")\n    total_cost = 0.0\n    total_token = 0\n    status = \"failed\"\n    response_text = None\n\n    for step_count in range(1, max_iterations + 1):\n        prompt = self._compose_prompt(instruction)\n        logging.info(f\"Prompt: {prompt}\")\n        response = self.llm(\n            prompt, stop=[\"Observation:\"]\n        )  # could cause bugs if llm doesn't have `stop` as a parameter\n        response_text = response.text\n        logging.info(f\"Response: {response_text}\")\n        action_step = self._parse_output(response_text)\n        if action_step is None:\n            raise ValueError(\"Invalid action\")\n        is_finished_chain = isinstance(action_step, AgentFinish)\n        if is_finished_chain:\n            result = \"\"\n        else:\n            assert isinstance(action_step, AgentAction)\n            action_name = action_step.tool\n            tool_input = action_step.tool_input\n            logging.info(f\"Action: {action_name}\")\n            logging.info(f\"Tool Input: {tool_input}\")\n            result = self._format_function_map()[action_name](tool_input)\n            logging.info(f\"Result: {result}\")\n\n        self.intermediate_steps.append((action_step, result))\n        if is_finished_chain:\n            logging.info(f\"Finished after {step_count} steps.\")\n            status = \"finished\"\n            break\n    else:\n        status = \"stopped\"\n\n    return AgentOutput(\n        text=response_text,\n        agent_type=self.agent_type,\n        status=status,\n        total_tokens=total_token,\n        total_cost=total_cost,\n        intermediate_steps=self.intermediate_steps,\n        max_iterations=max_iterations,\n    )\n</code></pre>"},{"location":"reference/agents/#agents.RewooAgent","title":"RewooAgent","text":"<p>             Bases: <code>BaseAgent</code></p> <p>Distributive RewooAgent class inherited from BaseAgent. Implementing ReWOO paradigm https://arxiv.org/pdf/2305.18323.pdf</p> Source code in <code>kotaemon\\agents\\rewoo\\agent.py</code> <pre><code>class RewooAgent(BaseAgent):\n    \"\"\"Distributive RewooAgent class inherited from BaseAgent.\n    Implementing ReWOO paradigm https://arxiv.org/pdf/2305.18323.pdf\"\"\"\n\n    name: str = \"RewooAgent\"\n    agent_type: AgentType = AgentType.rewoo\n    description: str = \"RewooAgent for answering multi-step reasoning questions\"\n    planner_llm: BaseLLM\n    solver_llm: BaseLLM\n    prompt_template: dict[str, PromptTemplate] = Param(\n        default_callback=lambda _: {},\n        help=\"A dict to supply different prompt to the agent.\",\n    )\n    plugins: list[BaseTool] = Param(\n        default_callback=lambda _: [], help=\"A list of plugins to be used in the model.\"\n    )\n    examples: dict[str, str | list[str]] = Param(\n        default_callback=lambda _: {}, help=\"Examples to be used in the agent.\"\n    )\n\n    @Node.auto(depends_on=[\"planner_llm\", \"plugins\", \"prompt_template\", \"examples\"])\n    def planner(self):\n        return Planner(\n            model=self.planner_llm,\n            plugins=self.plugins,\n            prompt_template=self.prompt_template.get(\"Planner\", None),\n            examples=self.examples.get(\"Planner\", None),\n        )\n\n    @Node.auto(depends_on=[\"solver_llm\", \"prompt_template\", \"examples\"])\n    def solver(self):\n        return Solver(\n            model=self.solver_llm,\n            prompt_template=self.prompt_template.get(\"Solver\", None),\n            examples=self.examples.get(\"Solver\", None),\n        )\n\n    def _parse_plan_map(\n        self, planner_response: str\n    ) -&gt; tuple[dict[str, list[str]], dict[str, str]]:\n        \"\"\"\n        Parse planner output. It should be an n-to-n mapping from Plans to #Es.\n        This is because sometimes LLM cannot follow the strict output format.\n        Example:\n            #Plan1\n            #E1\n            #E2\n        should result in: {\"#Plan1\": [\"#E1\", \"#E2\"]}\n        Or:\n            #Plan1\n            #Plan2\n            #E1\n        should result in: {\"#Plan1\": [], \"#Plan2\": [\"#E1\"]}\n        This function should also return a plan map.\n\n        Returns:\n            tuple[Dict[str, List[str]], Dict[str, str]]: A list of plan map\n        \"\"\"\n        valid_chunk = [\n            line\n            for line in planner_response.splitlines()\n            if line.startswith(\"#Plan\") or line.startswith(\"#E\")\n        ]\n\n        plan_to_es: dict[str, list[str]] = dict()\n        plans: dict[str, str] = dict()\n        prev_key = \"\"\n        for line in valid_chunk:\n            key, description = line.split(\":\", 1)\n            key = key.strip()\n            if key.startswith(\"#Plan\"):\n                plans[key] = description.strip()\n                plan_to_es[key] = []\n                prev_key = key\n            elif key.startswith(\"#E\"):\n                plan_to_es[prev_key].append(key)\n\n        return plan_to_es, plans\n\n    def _parse_planner_evidences(\n        self, planner_response: str\n    ) -&gt; tuple[dict[str, str], list[list[str]]]:\n        \"\"\"\n        Parse planner output. This should return a mapping from #E to tool call.\n        It should also identify the level of each #E in dependency map.\n        Example:\n            {\n            \"#E1\": \"Tool1\", \"#E2\": \"Tool2\",\n            \"#E3\": \"Tool3\", \"#E4\": \"Tool4\"\n            }, [[#E1, #E2], [#E3, #E4]]\n\n        Returns:\n            tuple[dict[str, str], List[List[str]]]:\n            A mapping from #E to tool call and a list of levels.\n        \"\"\"\n        evidences: dict[str, str] = dict()\n        dependence: dict[str, list[str]] = dict()\n        for line in planner_response.splitlines():\n            if line.startswith(\"#E\") and line[2].isdigit():\n                e, tool_call = line.split(\":\", 1)\n                e, tool_call = e.strip(), tool_call.strip()\n                if len(e) == 3:\n                    dependence[e] = []\n                    evidences[e] = tool_call\n                    for var in re.findall(r\"#E\\d+\", tool_call):\n                        if var in evidences:\n                            dependence[e].append(var)\n                else:\n                    evidences[e] = \"No evidence found\"\n        level = []\n        while dependence:\n            select = [i for i in dependence if not dependence[i]]\n            if len(select) == 0:\n                raise ValueError(\"Circular dependency detected.\")\n            level.append(select)\n            for item in select:\n                dependence.pop(item)\n            for item in dependence:\n                for i in select:\n                    if i in dependence[item]:\n                        dependence[item].remove(i)\n\n        return evidences, level\n\n    def _run_plugin(\n        self,\n        e: str,\n        planner_evidences: dict[str, str],\n        worker_evidences: dict[str, str],\n        output=BaseScratchPad(),\n    ):\n        \"\"\"\n        Run a plugin for a given evidence.\n        This function should also cumulate the cost and tokens.\n        \"\"\"\n        result = dict(e=e, plugin_cost=0, plugin_token=0, evidence=\"\")\n        tool_call = planner_evidences[e]\n        if \"[\" not in tool_call:\n            result[\"evidence\"] = tool_call\n        else:\n            tool, tool_input = tool_call.split(\"[\", 1)\n            tool_input = tool_input[:-1]\n            # find variables in input and replace with previous evidences\n            for var in re.findall(r\"#E\\d+\", tool_input):\n                if var in worker_evidences:\n                    tool_input = tool_input.replace(var, worker_evidences.get(var, \"\"))\n            try:\n                selected_plugin = self._find_plugin(tool)\n                if selected_plugin is None:\n                    raise ValueError(\"Invalid plugin detected\")\n                tool_response = selected_plugin(tool_input)\n                result[\"evidence\"] = get_plugin_response_content(tool_response)\n            except ValueError:\n                result[\"evidence\"] = \"No evidence found.\"\n            finally:\n                output.panel_print(\n                    result[\"evidence\"], f\"[green] Function Response of [blue]{tool}: \"\n                )\n        return result\n\n    def _get_worker_evidence(\n        self,\n        planner_evidences: dict[str, str],\n        evidences_level: list[list[str]],\n        output=BaseScratchPad(),\n    ) -&gt; Any:\n        \"\"\"\n        Parallel execution of plugins in DAG for speedup.\n        This is one of core benefits of ReWOO agents.\n\n        Args:\n            planner_evidences: A mapping from #E to tool call.\n            evidences_level: A list of levels of evidences.\n                Calculated from DAG of plugin calls.\n            output: Output object, defaults to BaseOutput().\n        Returns:\n            A mapping from #E to tool call.\n        \"\"\"\n        worker_evidences: dict[str, str] = dict()\n        plugin_cost, plugin_token = 0.0, 0.0\n        with ThreadPoolExecutor() as pool:\n            for level in evidences_level:\n                results = []\n                for e in level:\n                    results.append(\n                        pool.submit(\n                            self._run_plugin,\n                            e,\n                            planner_evidences,\n                            worker_evidences,\n                            output,\n                        )\n                    )\n                if len(results) &gt; 1:\n                    output.update_status(f\"Running tasks {level} in parallel.\")\n                else:\n                    output.update_status(f\"Running task {level[0]}.\")\n                for r in results:\n                    resp = r.result()\n                    plugin_cost += resp[\"plugin_cost\"]\n                    plugin_token += resp[\"plugin_token\"]\n                    worker_evidences[resp[\"e\"]] = resp[\"evidence\"]\n                output.done()\n\n        return worker_evidences, plugin_cost, plugin_token\n\n    def _find_plugin(self, name: str):\n        for p in self.plugins:\n            if p.name == name:\n                return p\n\n    @BaseAgent.safeguard_run\n    def run(self, instruction: str, use_citation: bool = False) -&gt; AgentOutput:\n        \"\"\"\n        Run the agent with a given instruction.\n        \"\"\"\n        logging.info(f\"Running {self.name} with instruction: {instruction}\")\n        total_cost = 0.0\n        total_token = 0\n\n        # Plan\n        planner_output = self.planner(instruction)\n        planner_text_output = planner_output.text\n        plan_to_es, plans = self._parse_plan_map(planner_text_output)\n        planner_evidences, evidence_level = self._parse_planner_evidences(\n            planner_text_output\n        )\n\n        # Work\n        worker_evidences, plugin_cost, plugin_token = self._get_worker_evidence(\n            planner_evidences, evidence_level\n        )\n        worker_log = \"\"\n        for plan in plan_to_es:\n            worker_log += f\"{plan}: {plans[plan]}\\n\"\n            for e in plan_to_es[plan]:\n                worker_log += f\"{e}: {worker_evidences[e]}\\n\"\n\n        # Solve\n        solver_output = self.solver(instruction, worker_log)\n        solver_output_text = solver_output.text\n        if use_citation:\n            citation_pipeline = CitationPipeline(llm=self.solver_llm)\n            citation = citation_pipeline(context=worker_log, question=instruction)\n        else:\n            citation = None\n\n        return AgentOutput(\n            text=solver_output_text,\n            agent_type=self.agent_type,\n            status=\"finished\",\n            total_tokens=total_token,\n            total_cost=total_cost,\n            citation=citation,\n        )\n</code></pre>"},{"location":"reference/agents/#agents.RewooAgent.run","title":"run","text":"<pre><code>run(instruction, use_citation=False)\n</code></pre> <p>Run the agent with a given instruction.</p> Source code in <code>kotaemon\\agents\\rewoo\\agent.py</code> <pre><code>@BaseAgent.safeguard_run\ndef run(self, instruction: str, use_citation: bool = False) -&gt; AgentOutput:\n    \"\"\"\n    Run the agent with a given instruction.\n    \"\"\"\n    logging.info(f\"Running {self.name} with instruction: {instruction}\")\n    total_cost = 0.0\n    total_token = 0\n\n    # Plan\n    planner_output = self.planner(instruction)\n    planner_text_output = planner_output.text\n    plan_to_es, plans = self._parse_plan_map(planner_text_output)\n    planner_evidences, evidence_level = self._parse_planner_evidences(\n        planner_text_output\n    )\n\n    # Work\n    worker_evidences, plugin_cost, plugin_token = self._get_worker_evidence(\n        planner_evidences, evidence_level\n    )\n    worker_log = \"\"\n    for plan in plan_to_es:\n        worker_log += f\"{plan}: {plans[plan]}\\n\"\n        for e in plan_to_es[plan]:\n            worker_log += f\"{e}: {worker_evidences[e]}\\n\"\n\n    # Solve\n    solver_output = self.solver(instruction, worker_log)\n    solver_output_text = solver_output.text\n    if use_citation:\n        citation_pipeline = CitationPipeline(llm=self.solver_llm)\n        citation = citation_pipeline(context=worker_log, question=instruction)\n    else:\n        citation = None\n\n    return AgentOutput(\n        text=solver_output_text,\n        agent_type=self.agent_type,\n        status=\"finished\",\n        total_tokens=total_token,\n        total_cost=total_cost,\n        citation=citation,\n    )\n</code></pre>"},{"location":"reference/agents/#agents.BaseTool","title":"BaseTool","text":"<p>             Bases: <code>BaseComponent</code></p> Source code in <code>kotaemon\\agents\\tools\\base.py</code> <pre><code>class BaseTool(BaseComponent):\n    name: str\n    \"\"\"The unique name of the tool that clearly communicates its purpose.\"\"\"\n    description: str\n    \"\"\"Description used to tell the model how/when/why to use the tool.\n    You can provide few-shot examples as a part of the description. This will be\n    input to the prompt of LLM.\n    \"\"\"\n    args_schema: Optional[Type[BaseModel]] = None\n    \"\"\"Pydantic model class to validate and parse the tool's input arguments.\"\"\"\n    verbose: bool = False\n    \"\"\"Whether to log the tool's progress.\"\"\"\n    handle_tool_error: Optional[\n        Union[bool, str, Callable[[ToolException], str]]\n    ] = False\n    \"\"\"Handle the content of the ToolException thrown.\"\"\"\n\n    def _parse_input(\n        self,\n        tool_input: Union[str, Dict],\n    ) -&gt; Union[str, Dict[str, Any]]:\n        \"\"\"Convert tool input to pydantic model.\"\"\"\n        args_schema = self.args_schema\n        if isinstance(tool_input, str):\n            if args_schema is not None:\n                key_ = next(iter(args_schema.__fields__.keys()))\n                args_schema.validate({key_: tool_input})\n            return tool_input\n        else:\n            if args_schema is not None:\n                result = args_schema.parse_obj(tool_input)\n                return {k: v for k, v in result.dict().items() if k in tool_input}\n        return tool_input\n\n    def _run_tool(\n        self,\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"Call tool.\"\"\"\n        raise NotImplementedError(f\"_run_tool is not implemented for {self.name}\")\n\n    def _to_args_and_kwargs(self, tool_input: Union[str, Dict]) -&gt; Tuple[Tuple, Dict]:\n        # For backwards compatibility, if run_input is a string,\n        # pass as a positional argument.\n        if isinstance(tool_input, str):\n            return (tool_input,), {}\n        else:\n            return (), tool_input\n\n    def _handle_tool_error(self, e: ToolException) -&gt; Any:\n        \"\"\"Handle the content of the ToolException thrown.\"\"\"\n        observation = None\n        if not self.handle_tool_error:\n            raise e\n        elif isinstance(self.handle_tool_error, bool):\n            if e.args:\n                observation = e.args[0]\n            else:\n                observation = \"Tool execution error\"\n        elif isinstance(self.handle_tool_error, str):\n            observation = self.handle_tool_error\n        elif callable(self.handle_tool_error):\n            observation = self.handle_tool_error(e)\n        else:\n            raise ValueError(\n                f\"Got unexpected type of `handle_tool_error`. Expected bool, str \"\n                f\"or callable. Received: {self.handle_tool_error}\"\n            )\n        return observation\n\n    def to_langchain_format(self) -&gt; LCTool:\n        \"\"\"Convert this tool to Langchain format to use with its agent\"\"\"\n        return LCTool(name=self.name, description=self.description, func=self.run)\n\n    def run(\n        self,\n        tool_input: Union[str, Dict],\n        verbose: Optional[bool] = None,\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"Run the tool.\"\"\"\n        parsed_input = self._parse_input(tool_input)\n        # TODO (verbose_): Add logging\n        try:\n            tool_args, tool_kwargs = self._to_args_and_kwargs(parsed_input)\n            call_kwargs = {**kwargs, **tool_kwargs}\n            observation = self._run_tool(*tool_args, **call_kwargs)\n        except ToolException as e:\n            observation = self._handle_tool_error(e)\n            return observation\n        else:\n            return observation\n\n    @classmethod\n    def from_langchain_format(cls, langchain_tool: LCTool) -&gt; \"BaseTool\":\n        \"\"\"Wrapper for Langchain Tool\"\"\"\n        new_tool = BaseTool(\n            name=langchain_tool.name, description=langchain_tool.description\n        )\n        new_tool._run_tool = langchain_tool._run  # type: ignore\n        return new_tool\n</code></pre>"},{"location":"reference/agents/#agents.BaseTool.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name\n</code></pre> <p>The unique name of the tool that clearly communicates its purpose.</p>"},{"location":"reference/agents/#agents.BaseTool.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description\n</code></pre> <p>Description used to tell the model how/when/why to use the tool. You can provide few-shot examples as a part of the description. This will be input to the prompt of LLM.</p>"},{"location":"reference/agents/#agents.BaseTool.args_schema","title":"args_schema  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>args_schema = None\n</code></pre> <p>Pydantic model class to validate and parse the tool's input arguments.</p>"},{"location":"reference/agents/#agents.BaseTool.verbose","title":"verbose  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>verbose = False\n</code></pre> <p>Whether to log the tool's progress.</p>"},{"location":"reference/agents/#agents.BaseTool.handle_tool_error","title":"handle_tool_error  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>handle_tool_error = False\n</code></pre> <p>Handle the content of the ToolException thrown.</p>"},{"location":"reference/agents/#agents.BaseTool.to_langchain_format","title":"to_langchain_format","text":"<pre><code>to_langchain_format()\n</code></pre> <p>Convert this tool to Langchain format to use with its agent</p> Source code in <code>kotaemon\\agents\\tools\\base.py</code> <pre><code>def to_langchain_format(self) -&gt; LCTool:\n    \"\"\"Convert this tool to Langchain format to use with its agent\"\"\"\n    return LCTool(name=self.name, description=self.description, func=self.run)\n</code></pre>"},{"location":"reference/agents/#agents.BaseTool.run","title":"run","text":"<pre><code>run(tool_input, verbose=None, **kwargs)\n</code></pre> <p>Run the tool.</p> Source code in <code>kotaemon\\agents\\tools\\base.py</code> <pre><code>def run(\n    self,\n    tool_input: Union[str, Dict],\n    verbose: Optional[bool] = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Run the tool.\"\"\"\n    parsed_input = self._parse_input(tool_input)\n    # TODO (verbose_): Add logging\n    try:\n        tool_args, tool_kwargs = self._to_args_and_kwargs(parsed_input)\n        call_kwargs = {**kwargs, **tool_kwargs}\n        observation = self._run_tool(*tool_args, **call_kwargs)\n    except ToolException as e:\n        observation = self._handle_tool_error(e)\n        return observation\n    else:\n        return observation\n</code></pre>"},{"location":"reference/agents/#agents.BaseTool.from_langchain_format","title":"from_langchain_format  <code>classmethod</code>","text":"<pre><code>from_langchain_format(langchain_tool)\n</code></pre> <p>Wrapper for Langchain Tool</p> Source code in <code>kotaemon\\agents\\tools\\base.py</code> <pre><code>@classmethod\ndef from_langchain_format(cls, langchain_tool: LCTool) -&gt; \"BaseTool\":\n    \"\"\"Wrapper for Langchain Tool\"\"\"\n    new_tool = BaseTool(\n        name=langchain_tool.name, description=langchain_tool.description\n    )\n    new_tool._run_tool = langchain_tool._run  # type: ignore\n    return new_tool\n</code></pre>"},{"location":"reference/agents/#agents.ComponentTool","title":"ComponentTool","text":"<p>             Bases: <code>BaseTool</code></p> <p>A Tool based on another pipeline / BaseComponent to be used as its main entry point</p> Source code in <code>kotaemon\\agents\\tools\\base.py</code> <pre><code>class ComponentTool(BaseTool):\n    \"\"\"\n    A Tool based on another pipeline / BaseComponent to be used\n    as its main entry point\n    \"\"\"\n\n    component: BaseComponent\n    postprocessor: Optional[Callable] = None\n\n    def _run_tool(self, *args: Any, **kwargs: Any) -&gt; Any:\n        output = self.component(*args, **kwargs)\n        if self.postprocessor:\n            output = self.postprocessor(output)\n\n        return output\n</code></pre>"},{"location":"reference/agents/#agents.WikipediaTool","title":"WikipediaTool","text":"<p>             Bases: <code>BaseTool</code></p> <p>Tool that adds the capability to query the Wikipedia API.</p> Source code in <code>kotaemon\\agents\\tools\\wikipedia.py</code> <pre><code>class WikipediaTool(BaseTool):\n    \"\"\"Tool that adds the capability to query the Wikipedia API.\"\"\"\n\n    name: str = \"wikipedia\"\n    description: str = (\n        \"Search engine from Wikipedia, retrieving relevant wiki page. \"\n        \"Useful when you need to get holistic knowledge about people, \"\n        \"places, companies, historical events, or other subjects. \"\n        \"Input should be a search query.\"\n    )\n    args_schema: Optional[Type[BaseModel]] = WikipediaArgs\n    doc_store: Any = None\n\n    def _run_tool(self, query: AnyStr) -&gt; AnyStr:\n        if not self.doc_store:\n            self.doc_store = Wiki()\n        tool = self.doc_store\n        evidence = tool.search(query)\n        return evidence\n</code></pre>"},{"location":"reference/agents/base/","title":"Base","text":""},{"location":"reference/agents/base/#agents.base.BaseAgent","title":"BaseAgent","text":"<p>             Bases: <code>BaseComponent</code></p> <p>Define base agent interface</p> Source code in <code>kotaemon\\agents\\base.py</code> <pre><code>class BaseAgent(BaseComponent):\n    \"\"\"Define base agent interface\"\"\"\n\n    name: str = Param(help=\"Name of the agent.\")\n    agent_type: AgentType = Param(help=\"Agent type, must be one of AgentType\")\n    description: str = Param(\n        help=(\n            \"Description used to tell the model how/when/why to use the agent. You can\"\n            \" provide few-shot examples as a part of the description. This will be\"\n            \" input to the prompt of LLM.\"\n        )\n    )\n    llm: Optional[BaseLLM] = Node(\n        help=(\n            \"LLM to be used for the agent (optional). LLM must implement BaseLLM\"\n            \" interface.\"\n        )\n    )\n    prompt_template: Optional[Union[PromptTemplate, dict[str, PromptTemplate]]] = Param(\n        help=\"A prompt template or a dict to supply different prompt to the agent\"\n    )\n    plugins: list[BaseTool] = Param(\n        default_callback=lambda _: [],\n        help=\"List of plugins / tools to be used in the agent\",\n    )\n\n    @staticmethod\n    def safeguard_run(run_func, *args, **kwargs):\n        def wrapper(self, *args, **kwargs):\n            try:\n                return run_func(self, *args, **kwargs)\n            except Exception as e:\n                return AgentOutput(\n                    text=\"\",\n                    agent_type=self.agent_type,\n                    status=\"failed\",\n                    error=str(e),\n                )\n\n        return wrapper\n\n    def add_tools(self, tools: list[BaseTool]) -&gt; None:\n        \"\"\"Helper method to add tools and update agent state if needed\"\"\"\n        self.plugins.extend(tools)\n\n    def run(self, *args, **kwargs) -&gt; AgentOutput | list[AgentOutput]:\n        \"\"\"Run the component.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/agents/base/#agents.base.BaseAgent.add_tools","title":"add_tools","text":"<pre><code>add_tools(tools)\n</code></pre> <p>Helper method to add tools and update agent state if needed</p> Source code in <code>kotaemon\\agents\\base.py</code> <pre><code>def add_tools(self, tools: list[BaseTool]) -&gt; None:\n    \"\"\"Helper method to add tools and update agent state if needed\"\"\"\n    self.plugins.extend(tools)\n</code></pre>"},{"location":"reference/agents/base/#agents.base.BaseAgent.run","title":"run","text":"<pre><code>run(*args, **kwargs)\n</code></pre> <p>Run the component.</p> Source code in <code>kotaemon\\agents\\base.py</code> <pre><code>def run(self, *args, **kwargs) -&gt; AgentOutput | list[AgentOutput]:\n    \"\"\"Run the component.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/agents/langchain_based/","title":"Langchain Based","text":""},{"location":"reference/agents/langchain_based/#agents.langchain_based.LangchainAgent","title":"LangchainAgent","text":"<p>             Bases: <code>BaseAgent</code></p> <p>Wrapper for Langchain Agent</p> Source code in <code>kotaemon\\agents\\langchain_based.py</code> <pre><code>class LangchainAgent(BaseAgent):\n    \"\"\"Wrapper for Langchain Agent\"\"\"\n\n    name: str = \"LangchainAgent\"\n    agent_type: AgentType\n    description: str = \"LangchainAgent for answering multi-step reasoning questions\"\n    AGENT_TYPE_MAP = {\n        AgentType.openai: LCAgentType.OPENAI_FUNCTIONS,\n        AgentType.openai_multi: LCAgentType.OPENAI_MULTI_FUNCTIONS,\n        AgentType.react: LCAgentType.ZERO_SHOT_REACT_DESCRIPTION,\n        AgentType.self_ask: LCAgentType.SELF_ASK_WITH_SEARCH,\n    }\n    agent: Optional[LCAgentExecutor] = None\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        if self.agent_type not in self.AGENT_TYPE_MAP:\n            raise NotImplementedError(\n                f\"AgentType {self.agent_type } not supported by Langchain wrapper\"\n            )\n        self.update_agent_tools()\n\n    def update_agent_tools(self):\n        assert isinstance(self.llm, (ChatLLM, LLM))\n        langchain_plugins = [tool.to_langchain_format() for tool in self.plugins]\n\n        # a fix for search_doc tool name:\n        # use \"Intermediate Answer\" for self-ask agent\n        found_search_tool = False\n        if self.agent_type == AgentType.self_ask:\n            for plugin in langchain_plugins:\n                if plugin.name == \"search_doc\":\n                    plugin.name = \"Intermediate Answer\"\n                    langchain_plugins = [plugin]\n                    found_search_tool = True\n                    break\n\n        if self.agent_type != AgentType.self_ask or found_search_tool:\n            # reinit Langchain AgentExecutor\n            self.agent = initialize_agent(\n                langchain_plugins,\n                # TODO: could cause bugs for non-langchain llms\n                # related to https://github.com/Cinnamon/kotaemon/issues/73\n                self.llm._obj,  # type: ignore\n                agent=self.AGENT_TYPE_MAP[self.agent_type],\n                handle_parsing_errors=True,\n                verbose=True,\n            )\n\n    def add_tools(self, tools: List[BaseTool]) -&gt; None:\n        super().add_tools(tools)\n        self.update_agent_tools()\n        return\n\n    def run(self, instruction: str) -&gt; AgentOutput:\n        assert (\n            self.agent is not None\n        ), \"Lanchain AgentExecutor is not correclty initialized\"\n\n        # Langchain AgentExecutor call\n        output = self.agent(instruction)[\"output\"]\n\n        return AgentOutput(\n            text=output,\n            agent_type=self.agent_type,\n            status=\"finished\",\n        )\n</code></pre>"},{"location":"reference/agents/utils/","title":"Utils","text":""},{"location":"reference/agents/utils/#agents.utils.get_plugin_response_content","title":"get_plugin_response_content","text":"<pre><code>get_plugin_response_content(output)\n</code></pre> <p>Wrapper for AgentOutput content return</p> Source code in <code>kotaemon\\agents\\utils.py</code> <pre><code>def get_plugin_response_content(output) -&gt; str:\n    \"\"\"\n    Wrapper for AgentOutput content return\n    \"\"\"\n    if isinstance(output, Document):\n        return output.text\n    else:\n        return str(output)\n</code></pre>"},{"location":"reference/agents/utils/#agents.utils.calculate_cost","title":"calculate_cost","text":"<pre><code>calculate_cost(model_name, prompt_token, completion_token)\n</code></pre> <p>Calculate the cost of a prompt and completion.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Cost of the provided model name with provided token information</p> Source code in <code>kotaemon\\agents\\utils.py</code> <pre><code>def calculate_cost(model_name: str, prompt_token: int, completion_token: int) -&gt; float:\n    \"\"\"\n    Calculate the cost of a prompt and completion.\n\n    Returns:\n        float: Cost of the provided model name with provided token information\n    \"\"\"\n    # TODO: to be implemented\n    return 0.0\n</code></pre>"},{"location":"reference/agents/io/","title":"Io","text":""},{"location":"reference/agents/io/#agents.io.AgentAction","title":"AgentAction  <code>dataclass</code>","text":"<p>Agent's action to take.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>str</code> <p>The tool to invoke.</p> required <code>tool_input</code> <code>Union[str, dict]</code> <p>The input to the tool.</p> required <code>log</code> <code>str</code> <p>The log message.</p> required Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>@dataclass\nclass AgentAction:\n    \"\"\"Agent's action to take.\n\n    Args:\n        tool: The tool to invoke.\n        tool_input: The input to the tool.\n        log: The log message.\n    \"\"\"\n\n    tool: str\n    tool_input: Union[str, dict]\n    log: str\n</code></pre>"},{"location":"reference/agents/io/#agents.io.AgentFinish","title":"AgentFinish","text":"<p>             Bases: <code>NamedTuple</code></p> <p>Agent's return value when finishing execution.</p> <p>Parameters:</p> Name Type Description Default <code>return_values</code> <p>The return values of the agent.</p> required <code>log</code> <p>The log message.</p> required Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>class AgentFinish(NamedTuple):\n    \"\"\"Agent's return value when finishing execution.\n\n    Args:\n        return_values: The return values of the agent.\n        log: The log message.\n    \"\"\"\n\n    return_values: dict\n    log: str\n</code></pre>"},{"location":"reference/agents/io/#agents.io.AgentOutput","title":"AgentOutput","text":"<p>             Bases: <code>LLMInterface</code></p> <p>Output from an agent.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>The text output from the agent.</p> required <code>agent_type</code> <p>The type of agent.</p> required <code>status</code> <p>The status after executing the agent.</p> required <code>error</code> <p>The error message if any.</p> required Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>class AgentOutput(LLMInterface, extra=Extra.allow):  # type: ignore [call-arg]\n    \"\"\"Output from an agent.\n\n    Args:\n        text: The text output from the agent.\n        agent_type: The type of agent.\n        status: The status after executing the agent.\n        error: The error message if any.\n    \"\"\"\n\n    text: str\n    type: str = \"agent\"\n    agent_type: AgentType\n    status: Literal[\"finished\", \"stopped\", \"failed\"]\n    error: Optional[str] = None\n</code></pre>"},{"location":"reference/agents/io/#agents.io.AgentType","title":"AgentType","text":"<p>             Bases: <code>Enum</code></p> <p>Enumerated type for agent types.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>class AgentType(Enum):\n    \"\"\"\n    Enumerated type for agent types.\n    \"\"\"\n\n    openai = \"openai\"\n    openai_multi = \"openai_multi\"\n    openai_tool = \"openai_tool\"\n    self_ask = \"self_ask\"\n    react = \"react\"\n    rewoo = \"rewoo\"\n    vanilla = \"vanilla\"\n</code></pre>"},{"location":"reference/agents/io/#agents.io.BaseScratchPad","title":"BaseScratchPad","text":"<p>Base class for output handlers.</p>"},{"location":"reference/agents/io/#agents.io.BaseScratchPad--attributes","title":"Attributes:","text":"<p>logger : logging.Logger     The logger object to log messages.</p>"},{"location":"reference/agents/io/#agents.io.BaseScratchPad--methods","title":"Methods:","text":"<p>stop():     Stop the output.</p> <p>update_status(output: str, **kwargs):     Update the status of the output.</p> <p>thinking(name: str):     Log that a process is thinking.</p> <p>done(_all=False):     Log that the process is done.</p> <p>stream_print(item: str):     Not implemented.</p> <p>json_print(item: Dict[str, Any]):     Log a JSON object.</p> <p>panel_print(item: Any, title: str = \"Output\", stream: bool = False):     Log a panel output.</p> <p>clear():     Not implemented.</p> <p>print(content: str, **kwargs):     Log arbitrary content.</p> <p>format_json(json_obj: str):     Format a JSON object.</p> <p>debug(content: str, **kwargs):     Log a debug message.</p> <p>info(content: str, **kwargs):     Log an informational message.</p> <p>warning(content: str, **kwargs):     Log a warning message.</p> <p>error(content: str, **kwargs):     Log an error message.</p> <p>critical(content: str, **kwargs):     Log a critical message.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>class BaseScratchPad:\n    \"\"\"\n    Base class for output handlers.\n\n    Attributes:\n    -----------\n    logger : logging.Logger\n        The logger object to log messages.\n\n    Methods:\n    --------\n    stop():\n        Stop the output.\n\n    update_status(output: str, **kwargs):\n        Update the status of the output.\n\n    thinking(name: str):\n        Log that a process is thinking.\n\n    done(_all=False):\n        Log that the process is done.\n\n    stream_print(item: str):\n        Not implemented.\n\n    json_print(item: Dict[str, Any]):\n        Log a JSON object.\n\n    panel_print(item: Any, title: str = \"Output\", stream: bool = False):\n        Log a panel output.\n\n    clear():\n        Not implemented.\n\n    print(content: str, **kwargs):\n        Log arbitrary content.\n\n    format_json(json_obj: str):\n        Format a JSON object.\n\n    debug(content: str, **kwargs):\n        Log a debug message.\n\n    info(content: str, **kwargs):\n        Log an informational message.\n\n    warning(content: str, **kwargs):\n        Log a warning message.\n\n    error(content: str, **kwargs):\n        Log an error message.\n\n    critical(content: str, **kwargs):\n        Log a critical message.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the BaseOutput object.\n\n        \"\"\"\n        self.logger = logging\n        self.log = []\n\n    def stop(self):\n        \"\"\"\n        Stop the output.\n        \"\"\"\n\n    def update_status(self, output: str, **kwargs):\n        \"\"\"\n        Update the status of the output.\n        \"\"\"\n        if check_log():\n            self.logger.info(output)\n\n    def thinking(self, name: str):\n        \"\"\"\n        Log that a process is thinking.\n        \"\"\"\n        if check_log():\n            self.logger.info(f\"{name} is thinking...\")\n\n    def done(self, _all=False):\n        \"\"\"\n        Log that the process is done.\n        \"\"\"\n\n        if check_log():\n            self.logger.info(\"Done\")\n\n    def stream_print(self, item: str):\n        \"\"\"\n        Stream print.\n        \"\"\"\n\n    def json_print(self, item: Dict[str, Any]):\n        \"\"\"\n        Log a JSON object.\n        \"\"\"\n        if check_log():\n            self.logger.info(json.dumps(item, indent=2))\n\n    def panel_print(self, item: Any, title: str = \"Output\", stream: bool = False):\n        \"\"\"\n        Log a panel output.\n\n        Args:\n            item : Any\n                The item to log.\n            title : str, optional\n                The title of the panel, defaults to \"Output\".\n            stream : bool, optional\n        \"\"\"\n        if not stream:\n            self.log.append(item)\n        if check_log():\n            self.logger.info(\"-\" * 20)\n            self.logger.info(item)\n            self.logger.info(\"-\" * 20)\n\n    def clear(self):\n        \"\"\"\n        Not implemented.\n        \"\"\"\n\n    def print(self, content: str, **kwargs):\n        \"\"\"\n        Log arbitrary content.\n        \"\"\"\n        self.log.append(content)\n        if check_log():\n            self.logger.info(content)\n\n    def format_json(self, json_obj: str):\n        \"\"\"\n        Format a JSON object.\n        \"\"\"\n        formatted_json = json.dumps(json_obj, indent=2)\n        return formatted_json\n\n    def debug(self, content: str, **kwargs):\n        \"\"\"\n        Log a debug message.\n        \"\"\"\n        if check_log():\n            self.logger.debug(content, **kwargs)\n\n    def info(self, content: str, **kwargs):\n        \"\"\"\n        Log an informational message.\n        \"\"\"\n        if check_log():\n            self.logger.info(content, **kwargs)\n\n    def warning(self, content: str, **kwargs):\n        \"\"\"\n        Log a warning message.\n        \"\"\"\n        if check_log():\n            self.logger.warning(content, **kwargs)\n\n    def error(self, content: str, **kwargs):\n        \"\"\"\n        Log an error message.\n        \"\"\"\n        if check_log():\n            self.logger.error(content, **kwargs)\n\n    def critical(self, content: str, **kwargs):\n        \"\"\"\n        Log a critical message.\n        \"\"\"\n        if check_log():\n            self.logger.critical(content, **kwargs)\n</code></pre>"},{"location":"reference/agents/io/#agents.io.BaseScratchPad.stop","title":"stop","text":"<pre><code>stop()\n</code></pre> <p>Stop the output.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def stop(self):\n    \"\"\"\n    Stop the output.\n    \"\"\"\n</code></pre>"},{"location":"reference/agents/io/#agents.io.BaseScratchPad.update_status","title":"update_status","text":"<pre><code>update_status(output, **kwargs)\n</code></pre> <p>Update the status of the output.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def update_status(self, output: str, **kwargs):\n    \"\"\"\n    Update the status of the output.\n    \"\"\"\n    if check_log():\n        self.logger.info(output)\n</code></pre>"},{"location":"reference/agents/io/#agents.io.BaseScratchPad.thinking","title":"thinking","text":"<pre><code>thinking(name)\n</code></pre> <p>Log that a process is thinking.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def thinking(self, name: str):\n    \"\"\"\n    Log that a process is thinking.\n    \"\"\"\n    if check_log():\n        self.logger.info(f\"{name} is thinking...\")\n</code></pre>"},{"location":"reference/agents/io/#agents.io.BaseScratchPad.done","title":"done","text":"<pre><code>done(_all=False)\n</code></pre> <p>Log that the process is done.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def done(self, _all=False):\n    \"\"\"\n    Log that the process is done.\n    \"\"\"\n\n    if check_log():\n        self.logger.info(\"Done\")\n</code></pre>"},{"location":"reference/agents/io/#agents.io.BaseScratchPad.stream_print","title":"stream_print","text":"<pre><code>stream_print(item)\n</code></pre> <p>Stream print.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def stream_print(self, item: str):\n    \"\"\"\n    Stream print.\n    \"\"\"\n</code></pre>"},{"location":"reference/agents/io/#agents.io.BaseScratchPad.json_print","title":"json_print","text":"<pre><code>json_print(item)\n</code></pre> <p>Log a JSON object.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def json_print(self, item: Dict[str, Any]):\n    \"\"\"\n    Log a JSON object.\n    \"\"\"\n    if check_log():\n        self.logger.info(json.dumps(item, indent=2))\n</code></pre>"},{"location":"reference/agents/io/#agents.io.BaseScratchPad.panel_print","title":"panel_print","text":"<pre><code>panel_print(item, title='Output', stream=False)\n</code></pre> <p>Log a panel output.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <p>Any The item to log.</p> required <code>title</code> <p>str, optional The title of the panel, defaults to \"Output\".</p> <code>'Output'</code> <code>stream</code> <p>bool, optional</p> <code>False</code> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def panel_print(self, item: Any, title: str = \"Output\", stream: bool = False):\n    \"\"\"\n    Log a panel output.\n\n    Args:\n        item : Any\n            The item to log.\n        title : str, optional\n            The title of the panel, defaults to \"Output\".\n        stream : bool, optional\n    \"\"\"\n    if not stream:\n        self.log.append(item)\n    if check_log():\n        self.logger.info(\"-\" * 20)\n        self.logger.info(item)\n        self.logger.info(\"-\" * 20)\n</code></pre>"},{"location":"reference/agents/io/#agents.io.BaseScratchPad.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Not implemented.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def clear(self):\n    \"\"\"\n    Not implemented.\n    \"\"\"\n</code></pre>"},{"location":"reference/agents/io/#agents.io.BaseScratchPad.print","title":"print","text":"<pre><code>print(content, **kwargs)\n</code></pre> <p>Log arbitrary content.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def print(self, content: str, **kwargs):\n    \"\"\"\n    Log arbitrary content.\n    \"\"\"\n    self.log.append(content)\n    if check_log():\n        self.logger.info(content)\n</code></pre>"},{"location":"reference/agents/io/#agents.io.BaseScratchPad.format_json","title":"format_json","text":"<pre><code>format_json(json_obj)\n</code></pre> <p>Format a JSON object.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def format_json(self, json_obj: str):\n    \"\"\"\n    Format a JSON object.\n    \"\"\"\n    formatted_json = json.dumps(json_obj, indent=2)\n    return formatted_json\n</code></pre>"},{"location":"reference/agents/io/#agents.io.BaseScratchPad.debug","title":"debug","text":"<pre><code>debug(content, **kwargs)\n</code></pre> <p>Log a debug message.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def debug(self, content: str, **kwargs):\n    \"\"\"\n    Log a debug message.\n    \"\"\"\n    if check_log():\n        self.logger.debug(content, **kwargs)\n</code></pre>"},{"location":"reference/agents/io/#agents.io.BaseScratchPad.info","title":"info","text":"<pre><code>info(content, **kwargs)\n</code></pre> <p>Log an informational message.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def info(self, content: str, **kwargs):\n    \"\"\"\n    Log an informational message.\n    \"\"\"\n    if check_log():\n        self.logger.info(content, **kwargs)\n</code></pre>"},{"location":"reference/agents/io/#agents.io.BaseScratchPad.warning","title":"warning","text":"<pre><code>warning(content, **kwargs)\n</code></pre> <p>Log a warning message.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def warning(self, content: str, **kwargs):\n    \"\"\"\n    Log a warning message.\n    \"\"\"\n    if check_log():\n        self.logger.warning(content, **kwargs)\n</code></pre>"},{"location":"reference/agents/io/#agents.io.BaseScratchPad.error","title":"error","text":"<pre><code>error(content, **kwargs)\n</code></pre> <p>Log an error message.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def error(self, content: str, **kwargs):\n    \"\"\"\n    Log an error message.\n    \"\"\"\n    if check_log():\n        self.logger.error(content, **kwargs)\n</code></pre>"},{"location":"reference/agents/io/#agents.io.BaseScratchPad.critical","title":"critical","text":"<pre><code>critical(content, **kwargs)\n</code></pre> <p>Log a critical message.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def critical(self, content: str, **kwargs):\n    \"\"\"\n    Log a critical message.\n    \"\"\"\n    if check_log():\n        self.logger.critical(content, **kwargs)\n</code></pre>"},{"location":"reference/agents/io/base/","title":"Base","text":""},{"location":"reference/agents/io/base/#agents.io.base.AgentType","title":"AgentType","text":"<p>             Bases: <code>Enum</code></p> <p>Enumerated type for agent types.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>class AgentType(Enum):\n    \"\"\"\n    Enumerated type for agent types.\n    \"\"\"\n\n    openai = \"openai\"\n    openai_multi = \"openai_multi\"\n    openai_tool = \"openai_tool\"\n    self_ask = \"self_ask\"\n    react = \"react\"\n    rewoo = \"rewoo\"\n    vanilla = \"vanilla\"\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.BaseScratchPad","title":"BaseScratchPad","text":"<p>Base class for output handlers.</p>"},{"location":"reference/agents/io/base/#agents.io.base.BaseScratchPad--attributes","title":"Attributes:","text":"<p>logger : logging.Logger     The logger object to log messages.</p>"},{"location":"reference/agents/io/base/#agents.io.base.BaseScratchPad--methods","title":"Methods:","text":"<p>stop():     Stop the output.</p> <p>update_status(output: str, **kwargs):     Update the status of the output.</p> <p>thinking(name: str):     Log that a process is thinking.</p> <p>done(_all=False):     Log that the process is done.</p> <p>stream_print(item: str):     Not implemented.</p> <p>json_print(item: Dict[str, Any]):     Log a JSON object.</p> <p>panel_print(item: Any, title: str = \"Output\", stream: bool = False):     Log a panel output.</p> <p>clear():     Not implemented.</p> <p>print(content: str, **kwargs):     Log arbitrary content.</p> <p>format_json(json_obj: str):     Format a JSON object.</p> <p>debug(content: str, **kwargs):     Log a debug message.</p> <p>info(content: str, **kwargs):     Log an informational message.</p> <p>warning(content: str, **kwargs):     Log a warning message.</p> <p>error(content: str, **kwargs):     Log an error message.</p> <p>critical(content: str, **kwargs):     Log a critical message.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>class BaseScratchPad:\n    \"\"\"\n    Base class for output handlers.\n\n    Attributes:\n    -----------\n    logger : logging.Logger\n        The logger object to log messages.\n\n    Methods:\n    --------\n    stop():\n        Stop the output.\n\n    update_status(output: str, **kwargs):\n        Update the status of the output.\n\n    thinking(name: str):\n        Log that a process is thinking.\n\n    done(_all=False):\n        Log that the process is done.\n\n    stream_print(item: str):\n        Not implemented.\n\n    json_print(item: Dict[str, Any]):\n        Log a JSON object.\n\n    panel_print(item: Any, title: str = \"Output\", stream: bool = False):\n        Log a panel output.\n\n    clear():\n        Not implemented.\n\n    print(content: str, **kwargs):\n        Log arbitrary content.\n\n    format_json(json_obj: str):\n        Format a JSON object.\n\n    debug(content: str, **kwargs):\n        Log a debug message.\n\n    info(content: str, **kwargs):\n        Log an informational message.\n\n    warning(content: str, **kwargs):\n        Log a warning message.\n\n    error(content: str, **kwargs):\n        Log an error message.\n\n    critical(content: str, **kwargs):\n        Log a critical message.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the BaseOutput object.\n\n        \"\"\"\n        self.logger = logging\n        self.log = []\n\n    def stop(self):\n        \"\"\"\n        Stop the output.\n        \"\"\"\n\n    def update_status(self, output: str, **kwargs):\n        \"\"\"\n        Update the status of the output.\n        \"\"\"\n        if check_log():\n            self.logger.info(output)\n\n    def thinking(self, name: str):\n        \"\"\"\n        Log that a process is thinking.\n        \"\"\"\n        if check_log():\n            self.logger.info(f\"{name} is thinking...\")\n\n    def done(self, _all=False):\n        \"\"\"\n        Log that the process is done.\n        \"\"\"\n\n        if check_log():\n            self.logger.info(\"Done\")\n\n    def stream_print(self, item: str):\n        \"\"\"\n        Stream print.\n        \"\"\"\n\n    def json_print(self, item: Dict[str, Any]):\n        \"\"\"\n        Log a JSON object.\n        \"\"\"\n        if check_log():\n            self.logger.info(json.dumps(item, indent=2))\n\n    def panel_print(self, item: Any, title: str = \"Output\", stream: bool = False):\n        \"\"\"\n        Log a panel output.\n\n        Args:\n            item : Any\n                The item to log.\n            title : str, optional\n                The title of the panel, defaults to \"Output\".\n            stream : bool, optional\n        \"\"\"\n        if not stream:\n            self.log.append(item)\n        if check_log():\n            self.logger.info(\"-\" * 20)\n            self.logger.info(item)\n            self.logger.info(\"-\" * 20)\n\n    def clear(self):\n        \"\"\"\n        Not implemented.\n        \"\"\"\n\n    def print(self, content: str, **kwargs):\n        \"\"\"\n        Log arbitrary content.\n        \"\"\"\n        self.log.append(content)\n        if check_log():\n            self.logger.info(content)\n\n    def format_json(self, json_obj: str):\n        \"\"\"\n        Format a JSON object.\n        \"\"\"\n        formatted_json = json.dumps(json_obj, indent=2)\n        return formatted_json\n\n    def debug(self, content: str, **kwargs):\n        \"\"\"\n        Log a debug message.\n        \"\"\"\n        if check_log():\n            self.logger.debug(content, **kwargs)\n\n    def info(self, content: str, **kwargs):\n        \"\"\"\n        Log an informational message.\n        \"\"\"\n        if check_log():\n            self.logger.info(content, **kwargs)\n\n    def warning(self, content: str, **kwargs):\n        \"\"\"\n        Log a warning message.\n        \"\"\"\n        if check_log():\n            self.logger.warning(content, **kwargs)\n\n    def error(self, content: str, **kwargs):\n        \"\"\"\n        Log an error message.\n        \"\"\"\n        if check_log():\n            self.logger.error(content, **kwargs)\n\n    def critical(self, content: str, **kwargs):\n        \"\"\"\n        Log a critical message.\n        \"\"\"\n        if check_log():\n            self.logger.critical(content, **kwargs)\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.BaseScratchPad.stop","title":"stop","text":"<pre><code>stop()\n</code></pre> <p>Stop the output.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def stop(self):\n    \"\"\"\n    Stop the output.\n    \"\"\"\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.BaseScratchPad.update_status","title":"update_status","text":"<pre><code>update_status(output, **kwargs)\n</code></pre> <p>Update the status of the output.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def update_status(self, output: str, **kwargs):\n    \"\"\"\n    Update the status of the output.\n    \"\"\"\n    if check_log():\n        self.logger.info(output)\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.BaseScratchPad.thinking","title":"thinking","text":"<pre><code>thinking(name)\n</code></pre> <p>Log that a process is thinking.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def thinking(self, name: str):\n    \"\"\"\n    Log that a process is thinking.\n    \"\"\"\n    if check_log():\n        self.logger.info(f\"{name} is thinking...\")\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.BaseScratchPad.done","title":"done","text":"<pre><code>done(_all=False)\n</code></pre> <p>Log that the process is done.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def done(self, _all=False):\n    \"\"\"\n    Log that the process is done.\n    \"\"\"\n\n    if check_log():\n        self.logger.info(\"Done\")\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.BaseScratchPad.stream_print","title":"stream_print","text":"<pre><code>stream_print(item)\n</code></pre> <p>Stream print.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def stream_print(self, item: str):\n    \"\"\"\n    Stream print.\n    \"\"\"\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.BaseScratchPad.json_print","title":"json_print","text":"<pre><code>json_print(item)\n</code></pre> <p>Log a JSON object.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def json_print(self, item: Dict[str, Any]):\n    \"\"\"\n    Log a JSON object.\n    \"\"\"\n    if check_log():\n        self.logger.info(json.dumps(item, indent=2))\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.BaseScratchPad.panel_print","title":"panel_print","text":"<pre><code>panel_print(item, title='Output', stream=False)\n</code></pre> <p>Log a panel output.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <p>Any The item to log.</p> required <code>title</code> <p>str, optional The title of the panel, defaults to \"Output\".</p> <code>'Output'</code> <code>stream</code> <p>bool, optional</p> <code>False</code> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def panel_print(self, item: Any, title: str = \"Output\", stream: bool = False):\n    \"\"\"\n    Log a panel output.\n\n    Args:\n        item : Any\n            The item to log.\n        title : str, optional\n            The title of the panel, defaults to \"Output\".\n        stream : bool, optional\n    \"\"\"\n    if not stream:\n        self.log.append(item)\n    if check_log():\n        self.logger.info(\"-\" * 20)\n        self.logger.info(item)\n        self.logger.info(\"-\" * 20)\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.BaseScratchPad.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Not implemented.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def clear(self):\n    \"\"\"\n    Not implemented.\n    \"\"\"\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.BaseScratchPad.print","title":"print","text":"<pre><code>print(content, **kwargs)\n</code></pre> <p>Log arbitrary content.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def print(self, content: str, **kwargs):\n    \"\"\"\n    Log arbitrary content.\n    \"\"\"\n    self.log.append(content)\n    if check_log():\n        self.logger.info(content)\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.BaseScratchPad.format_json","title":"format_json","text":"<pre><code>format_json(json_obj)\n</code></pre> <p>Format a JSON object.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def format_json(self, json_obj: str):\n    \"\"\"\n    Format a JSON object.\n    \"\"\"\n    formatted_json = json.dumps(json_obj, indent=2)\n    return formatted_json\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.BaseScratchPad.debug","title":"debug","text":"<pre><code>debug(content, **kwargs)\n</code></pre> <p>Log a debug message.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def debug(self, content: str, **kwargs):\n    \"\"\"\n    Log a debug message.\n    \"\"\"\n    if check_log():\n        self.logger.debug(content, **kwargs)\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.BaseScratchPad.info","title":"info","text":"<pre><code>info(content, **kwargs)\n</code></pre> <p>Log an informational message.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def info(self, content: str, **kwargs):\n    \"\"\"\n    Log an informational message.\n    \"\"\"\n    if check_log():\n        self.logger.info(content, **kwargs)\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.BaseScratchPad.warning","title":"warning","text":"<pre><code>warning(content, **kwargs)\n</code></pre> <p>Log a warning message.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def warning(self, content: str, **kwargs):\n    \"\"\"\n    Log a warning message.\n    \"\"\"\n    if check_log():\n        self.logger.warning(content, **kwargs)\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.BaseScratchPad.error","title":"error","text":"<pre><code>error(content, **kwargs)\n</code></pre> <p>Log an error message.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def error(self, content: str, **kwargs):\n    \"\"\"\n    Log an error message.\n    \"\"\"\n    if check_log():\n        self.logger.error(content, **kwargs)\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.BaseScratchPad.critical","title":"critical","text":"<pre><code>critical(content, **kwargs)\n</code></pre> <p>Log a critical message.</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def critical(self, content: str, **kwargs):\n    \"\"\"\n    Log a critical message.\n    \"\"\"\n    if check_log():\n        self.logger.critical(content, **kwargs)\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.AgentAction","title":"AgentAction  <code>dataclass</code>","text":"<p>Agent's action to take.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>str</code> <p>The tool to invoke.</p> required <code>tool_input</code> <code>Union[str, dict]</code> <p>The input to the tool.</p> required <code>log</code> <code>str</code> <p>The log message.</p> required Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>@dataclass\nclass AgentAction:\n    \"\"\"Agent's action to take.\n\n    Args:\n        tool: The tool to invoke.\n        tool_input: The input to the tool.\n        log: The log message.\n    \"\"\"\n\n    tool: str\n    tool_input: Union[str, dict]\n    log: str\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.AgentFinish","title":"AgentFinish","text":"<p>             Bases: <code>NamedTuple</code></p> <p>Agent's return value when finishing execution.</p> <p>Parameters:</p> Name Type Description Default <code>return_values</code> <p>The return values of the agent.</p> required <code>log</code> <p>The log message.</p> required Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>class AgentFinish(NamedTuple):\n    \"\"\"Agent's return value when finishing execution.\n\n    Args:\n        return_values: The return values of the agent.\n        log: The log message.\n    \"\"\"\n\n    return_values: dict\n    log: str\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.AgentOutput","title":"AgentOutput","text":"<p>             Bases: <code>LLMInterface</code></p> <p>Output from an agent.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>The text output from the agent.</p> required <code>agent_type</code> <p>The type of agent.</p> required <code>status</code> <p>The status after executing the agent.</p> required <code>error</code> <p>The error message if any.</p> required Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>class AgentOutput(LLMInterface, extra=Extra.allow):  # type: ignore [call-arg]\n    \"\"\"Output from an agent.\n\n    Args:\n        text: The text output from the agent.\n        agent_type: The type of agent.\n        status: The status after executing the agent.\n        error: The error message if any.\n    \"\"\"\n\n    text: str\n    type: str = \"agent\"\n    agent_type: AgentType\n    status: Literal[\"finished\", \"stopped\", \"failed\"]\n    error: Optional[str] = None\n</code></pre>"},{"location":"reference/agents/io/base/#agents.io.base.check_log","title":"check_log","text":"<pre><code>check_log()\n</code></pre> <p>Checks if logging has been enabled. :return: True if logging has been enabled, False otherwise. :rtype: bool</p> Source code in <code>kotaemon\\agents\\io\\base.py</code> <pre><code>def check_log():\n    \"\"\"\n    Checks if logging has been enabled.\n    :return: True if logging has been enabled, False otherwise.\n    :rtype: bool\n    \"\"\"\n    return os.environ.get(\"LOG_PATH\", None) is not None\n</code></pre>"},{"location":"reference/agents/react/","title":"React","text":""},{"location":"reference/agents/react/#agents.react.ReactAgent","title":"ReactAgent","text":"<p>             Bases: <code>BaseAgent</code></p> <p>Sequential ReactAgent class inherited from BaseAgent. Implementing ReAct agent paradigm https://arxiv.org/pdf/2210.03629.pdf</p> Source code in <code>kotaemon\\agents\\react\\agent.py</code> <pre><code>class ReactAgent(BaseAgent):\n    \"\"\"\n    Sequential ReactAgent class inherited from BaseAgent.\n    Implementing ReAct agent paradigm https://arxiv.org/pdf/2210.03629.pdf\n    \"\"\"\n\n    name: str = \"ReactAgent\"\n    agent_type: AgentType = AgentType.react\n    description: str = \"ReactAgent for answering multi-step reasoning questions\"\n    llm: BaseLLM\n    prompt_template: Optional[PromptTemplate] = None\n    plugins: list[BaseTool] = Param(\n        default_callback=lambda _: [], help=\"List of tools to be used in the agent. \"\n    )\n    examples: dict[str, str | list[str]] = Param(\n        default_callback=lambda _: {}, help=\"Examples to be used in the agent. \"\n    )\n    intermediate_steps: list[tuple[AgentAction | AgentFinish, str]] = Param(\n        default_callback=lambda _: [],\n        help=\"List of AgentAction and observation (tool) output\",\n    )\n    max_iterations: int = 10\n    strict_decode: bool = False\n\n    def _compose_plugin_description(self) -&gt; str:\n        \"\"\"\n        Compose the worker prompt from the workers.\n\n        Example:\n        toolname1[input]: tool1 description\n        toolname2[input]: tool2 description\n        \"\"\"\n        prompt = \"\"\n        try:\n            for plugin in self.plugins:\n                prompt += f\"{plugin.name}[input]: {plugin.description}\\n\"\n        except Exception:\n            raise ValueError(\"Worker must have a name and description.\")\n        return prompt\n\n    def _construct_scratchpad(\n        self, intermediate_steps: list[tuple[AgentAction | AgentFinish, str]] = []\n    ) -&gt; str:\n        \"\"\"Construct the scratchpad that lets the agent continue its thought process.\"\"\"\n        thoughts = \"\"\n        for action, observation in intermediate_steps:\n            thoughts += action.log\n            thoughts += f\"\\nObservation: {observation}\\nThought:\"\n        return thoughts\n\n    def _parse_output(self, text: str) -&gt; Optional[AgentAction | AgentFinish]:\n        \"\"\"\n        Parse text output from LLM for the next Action or Final Answer\n        Using Regex to parse \"Action:\\n Action Input:\\n\" for the next Action\n        Using FINAL_ANSWER_ACTION to parse Final Answer\n\n        Args:\n            text[str]: input text to parse\n        \"\"\"\n        includes_answer = FINAL_ANSWER_ACTION in text\n        regex = (\n            r\"Action\\s*\\d*\\s*:[\\s]*(.*?)[\\s]*Action\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n        )\n        action_match = re.search(regex, text, re.DOTALL)\n        action_output: Optional[AgentAction | AgentFinish] = None\n        if action_match:\n            if includes_answer:\n                raise Exception(\n                    \"Parsing LLM output produced both a final answer \"\n                    f\"and a parse-able action: {text}\"\n                )\n            action = action_match.group(1).strip()\n            action_input = action_match.group(2)\n            tool_input = action_input.strip(\" \")\n            # ensure if its a well formed SQL query we don't remove any trailing \" chars\n            if tool_input.startswith(\"SELECT \") is False:\n                tool_input = tool_input.strip('\"')\n\n            action_output = AgentAction(action, tool_input, text)\n\n        elif includes_answer:\n            action_output = AgentFinish(\n                {\"output\": text.split(FINAL_ANSWER_ACTION)[-1].strip()}, text\n            )\n        else:\n            if self.strict_decode:\n                raise Exception(f\"Could not parse LLM output: `{text}`\")\n            else:\n                action_output = AgentFinish({\"output\": text}, text)\n\n        return action_output\n\n    def _compose_prompt(self, instruction) -&gt; str:\n        \"\"\"\n        Compose the prompt from template, worker description, examples and instruction.\n        \"\"\"\n        agent_scratchpad = self._construct_scratchpad(self.intermediate_steps)\n        tool_description = self._compose_plugin_description()\n        tool_names = \", \".join([plugin.name for plugin in self.plugins])\n        if self.prompt_template is None:\n            from .prompt import zero_shot_react_prompt\n\n            self.prompt_template = zero_shot_react_prompt\n        return self.prompt_template.populate(\n            instruction=instruction,\n            agent_scratchpad=agent_scratchpad,\n            tool_description=tool_description,\n            tool_names=tool_names,\n        )\n\n    def _format_function_map(self) -&gt; dict[str, BaseTool]:\n        \"\"\"Format the function map for the open AI function API.\n\n        Return:\n            Dict[str, Callable]: The function map.\n        \"\"\"\n        # Map the function name to the real function object.\n        function_map = {}\n        for plugin in self.plugins:\n            function_map[plugin.name] = plugin\n        return function_map\n\n    def clear(self):\n        \"\"\"\n        Clear and reset the agent.\n        \"\"\"\n        self.intermediate_steps = []\n\n    def run(self, instruction, max_iterations=None) -&gt; AgentOutput:\n        \"\"\"\n        Run the agent with the given instruction.\n\n        Args:\n            instruction: Instruction to run the agent with.\n            max_iterations: Maximum number of iterations\n                of reasoning steps, defaults to 10.\n\n        Return:\n            AgentOutput object.\n        \"\"\"\n        if not max_iterations:\n            max_iterations = self.max_iterations\n        assert max_iterations &gt; 0\n\n        self.clear()\n        logging.info(f\"Running {self.name} with instruction: {instruction}\")\n        total_cost = 0.0\n        total_token = 0\n        status = \"failed\"\n        response_text = None\n\n        for step_count in range(1, max_iterations + 1):\n            prompt = self._compose_prompt(instruction)\n            logging.info(f\"Prompt: {prompt}\")\n            response = self.llm(\n                prompt, stop=[\"Observation:\"]\n            )  # could cause bugs if llm doesn't have `stop` as a parameter\n            response_text = response.text\n            logging.info(f\"Response: {response_text}\")\n            action_step = self._parse_output(response_text)\n            if action_step is None:\n                raise ValueError(\"Invalid action\")\n            is_finished_chain = isinstance(action_step, AgentFinish)\n            if is_finished_chain:\n                result = \"\"\n            else:\n                assert isinstance(action_step, AgentAction)\n                action_name = action_step.tool\n                tool_input = action_step.tool_input\n                logging.info(f\"Action: {action_name}\")\n                logging.info(f\"Tool Input: {tool_input}\")\n                result = self._format_function_map()[action_name](tool_input)\n                logging.info(f\"Result: {result}\")\n\n            self.intermediate_steps.append((action_step, result))\n            if is_finished_chain:\n                logging.info(f\"Finished after {step_count} steps.\")\n                status = \"finished\"\n                break\n        else:\n            status = \"stopped\"\n\n        return AgentOutput(\n            text=response_text,\n            agent_type=self.agent_type,\n            status=status,\n            total_tokens=total_token,\n            total_cost=total_cost,\n            intermediate_steps=self.intermediate_steps,\n            max_iterations=max_iterations,\n        )\n</code></pre>"},{"location":"reference/agents/react/#agents.react.ReactAgent.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Clear and reset the agent.</p> Source code in <code>kotaemon\\agents\\react\\agent.py</code> <pre><code>def clear(self):\n    \"\"\"\n    Clear and reset the agent.\n    \"\"\"\n    self.intermediate_steps = []\n</code></pre>"},{"location":"reference/agents/react/#agents.react.ReactAgent.run","title":"run","text":"<pre><code>run(instruction, max_iterations=None)\n</code></pre> <p>Run the agent with the given instruction.</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <p>Instruction to run the agent with.</p> required <code>max_iterations</code> <p>Maximum number of iterations of reasoning steps, defaults to 10.</p> <code>None</code> Return <p>AgentOutput object.</p> Source code in <code>kotaemon\\agents\\react\\agent.py</code> <pre><code>def run(self, instruction, max_iterations=None) -&gt; AgentOutput:\n    \"\"\"\n    Run the agent with the given instruction.\n\n    Args:\n        instruction: Instruction to run the agent with.\n        max_iterations: Maximum number of iterations\n            of reasoning steps, defaults to 10.\n\n    Return:\n        AgentOutput object.\n    \"\"\"\n    if not max_iterations:\n        max_iterations = self.max_iterations\n    assert max_iterations &gt; 0\n\n    self.clear()\n    logging.info(f\"Running {self.name} with instruction: {instruction}\")\n    total_cost = 0.0\n    total_token = 0\n    status = \"failed\"\n    response_text = None\n\n    for step_count in range(1, max_iterations + 1):\n        prompt = self._compose_prompt(instruction)\n        logging.info(f\"Prompt: {prompt}\")\n        response = self.llm(\n            prompt, stop=[\"Observation:\"]\n        )  # could cause bugs if llm doesn't have `stop` as a parameter\n        response_text = response.text\n        logging.info(f\"Response: {response_text}\")\n        action_step = self._parse_output(response_text)\n        if action_step is None:\n            raise ValueError(\"Invalid action\")\n        is_finished_chain = isinstance(action_step, AgentFinish)\n        if is_finished_chain:\n            result = \"\"\n        else:\n            assert isinstance(action_step, AgentAction)\n            action_name = action_step.tool\n            tool_input = action_step.tool_input\n            logging.info(f\"Action: {action_name}\")\n            logging.info(f\"Tool Input: {tool_input}\")\n            result = self._format_function_map()[action_name](tool_input)\n            logging.info(f\"Result: {result}\")\n\n        self.intermediate_steps.append((action_step, result))\n        if is_finished_chain:\n            logging.info(f\"Finished after {step_count} steps.\")\n            status = \"finished\"\n            break\n    else:\n        status = \"stopped\"\n\n    return AgentOutput(\n        text=response_text,\n        agent_type=self.agent_type,\n        status=status,\n        total_tokens=total_token,\n        total_cost=total_cost,\n        intermediate_steps=self.intermediate_steps,\n        max_iterations=max_iterations,\n    )\n</code></pre>"},{"location":"reference/agents/react/agent/","title":"Agent","text":""},{"location":"reference/agents/react/agent/#agents.react.agent.ReactAgent","title":"ReactAgent","text":"<p>             Bases: <code>BaseAgent</code></p> <p>Sequential ReactAgent class inherited from BaseAgent. Implementing ReAct agent paradigm https://arxiv.org/pdf/2210.03629.pdf</p> Source code in <code>kotaemon\\agents\\react\\agent.py</code> <pre><code>class ReactAgent(BaseAgent):\n    \"\"\"\n    Sequential ReactAgent class inherited from BaseAgent.\n    Implementing ReAct agent paradigm https://arxiv.org/pdf/2210.03629.pdf\n    \"\"\"\n\n    name: str = \"ReactAgent\"\n    agent_type: AgentType = AgentType.react\n    description: str = \"ReactAgent for answering multi-step reasoning questions\"\n    llm: BaseLLM\n    prompt_template: Optional[PromptTemplate] = None\n    plugins: list[BaseTool] = Param(\n        default_callback=lambda _: [], help=\"List of tools to be used in the agent. \"\n    )\n    examples: dict[str, str | list[str]] = Param(\n        default_callback=lambda _: {}, help=\"Examples to be used in the agent. \"\n    )\n    intermediate_steps: list[tuple[AgentAction | AgentFinish, str]] = Param(\n        default_callback=lambda _: [],\n        help=\"List of AgentAction and observation (tool) output\",\n    )\n    max_iterations: int = 10\n    strict_decode: bool = False\n\n    def _compose_plugin_description(self) -&gt; str:\n        \"\"\"\n        Compose the worker prompt from the workers.\n\n        Example:\n        toolname1[input]: tool1 description\n        toolname2[input]: tool2 description\n        \"\"\"\n        prompt = \"\"\n        try:\n            for plugin in self.plugins:\n                prompt += f\"{plugin.name}[input]: {plugin.description}\\n\"\n        except Exception:\n            raise ValueError(\"Worker must have a name and description.\")\n        return prompt\n\n    def _construct_scratchpad(\n        self, intermediate_steps: list[tuple[AgentAction | AgentFinish, str]] = []\n    ) -&gt; str:\n        \"\"\"Construct the scratchpad that lets the agent continue its thought process.\"\"\"\n        thoughts = \"\"\n        for action, observation in intermediate_steps:\n            thoughts += action.log\n            thoughts += f\"\\nObservation: {observation}\\nThought:\"\n        return thoughts\n\n    def _parse_output(self, text: str) -&gt; Optional[AgentAction | AgentFinish]:\n        \"\"\"\n        Parse text output from LLM for the next Action or Final Answer\n        Using Regex to parse \"Action:\\n Action Input:\\n\" for the next Action\n        Using FINAL_ANSWER_ACTION to parse Final Answer\n\n        Args:\n            text[str]: input text to parse\n        \"\"\"\n        includes_answer = FINAL_ANSWER_ACTION in text\n        regex = (\n            r\"Action\\s*\\d*\\s*:[\\s]*(.*?)[\\s]*Action\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n        )\n        action_match = re.search(regex, text, re.DOTALL)\n        action_output: Optional[AgentAction | AgentFinish] = None\n        if action_match:\n            if includes_answer:\n                raise Exception(\n                    \"Parsing LLM output produced both a final answer \"\n                    f\"and a parse-able action: {text}\"\n                )\n            action = action_match.group(1).strip()\n            action_input = action_match.group(2)\n            tool_input = action_input.strip(\" \")\n            # ensure if its a well formed SQL query we don't remove any trailing \" chars\n            if tool_input.startswith(\"SELECT \") is False:\n                tool_input = tool_input.strip('\"')\n\n            action_output = AgentAction(action, tool_input, text)\n\n        elif includes_answer:\n            action_output = AgentFinish(\n                {\"output\": text.split(FINAL_ANSWER_ACTION)[-1].strip()}, text\n            )\n        else:\n            if self.strict_decode:\n                raise Exception(f\"Could not parse LLM output: `{text}`\")\n            else:\n                action_output = AgentFinish({\"output\": text}, text)\n\n        return action_output\n\n    def _compose_prompt(self, instruction) -&gt; str:\n        \"\"\"\n        Compose the prompt from template, worker description, examples and instruction.\n        \"\"\"\n        agent_scratchpad = self._construct_scratchpad(self.intermediate_steps)\n        tool_description = self._compose_plugin_description()\n        tool_names = \", \".join([plugin.name for plugin in self.plugins])\n        if self.prompt_template is None:\n            from .prompt import zero_shot_react_prompt\n\n            self.prompt_template = zero_shot_react_prompt\n        return self.prompt_template.populate(\n            instruction=instruction,\n            agent_scratchpad=agent_scratchpad,\n            tool_description=tool_description,\n            tool_names=tool_names,\n        )\n\n    def _format_function_map(self) -&gt; dict[str, BaseTool]:\n        \"\"\"Format the function map for the open AI function API.\n\n        Return:\n            Dict[str, Callable]: The function map.\n        \"\"\"\n        # Map the function name to the real function object.\n        function_map = {}\n        for plugin in self.plugins:\n            function_map[plugin.name] = plugin\n        return function_map\n\n    def clear(self):\n        \"\"\"\n        Clear and reset the agent.\n        \"\"\"\n        self.intermediate_steps = []\n\n    def run(self, instruction, max_iterations=None) -&gt; AgentOutput:\n        \"\"\"\n        Run the agent with the given instruction.\n\n        Args:\n            instruction: Instruction to run the agent with.\n            max_iterations: Maximum number of iterations\n                of reasoning steps, defaults to 10.\n\n        Return:\n            AgentOutput object.\n        \"\"\"\n        if not max_iterations:\n            max_iterations = self.max_iterations\n        assert max_iterations &gt; 0\n\n        self.clear()\n        logging.info(f\"Running {self.name} with instruction: {instruction}\")\n        total_cost = 0.0\n        total_token = 0\n        status = \"failed\"\n        response_text = None\n\n        for step_count in range(1, max_iterations + 1):\n            prompt = self._compose_prompt(instruction)\n            logging.info(f\"Prompt: {prompt}\")\n            response = self.llm(\n                prompt, stop=[\"Observation:\"]\n            )  # could cause bugs if llm doesn't have `stop` as a parameter\n            response_text = response.text\n            logging.info(f\"Response: {response_text}\")\n            action_step = self._parse_output(response_text)\n            if action_step is None:\n                raise ValueError(\"Invalid action\")\n            is_finished_chain = isinstance(action_step, AgentFinish)\n            if is_finished_chain:\n                result = \"\"\n            else:\n                assert isinstance(action_step, AgentAction)\n                action_name = action_step.tool\n                tool_input = action_step.tool_input\n                logging.info(f\"Action: {action_name}\")\n                logging.info(f\"Tool Input: {tool_input}\")\n                result = self._format_function_map()[action_name](tool_input)\n                logging.info(f\"Result: {result}\")\n\n            self.intermediate_steps.append((action_step, result))\n            if is_finished_chain:\n                logging.info(f\"Finished after {step_count} steps.\")\n                status = \"finished\"\n                break\n        else:\n            status = \"stopped\"\n\n        return AgentOutput(\n            text=response_text,\n            agent_type=self.agent_type,\n            status=status,\n            total_tokens=total_token,\n            total_cost=total_cost,\n            intermediate_steps=self.intermediate_steps,\n            max_iterations=max_iterations,\n        )\n</code></pre>"},{"location":"reference/agents/react/agent/#agents.react.agent.ReactAgent.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Clear and reset the agent.</p> Source code in <code>kotaemon\\agents\\react\\agent.py</code> <pre><code>def clear(self):\n    \"\"\"\n    Clear and reset the agent.\n    \"\"\"\n    self.intermediate_steps = []\n</code></pre>"},{"location":"reference/agents/react/agent/#agents.react.agent.ReactAgent.run","title":"run","text":"<pre><code>run(instruction, max_iterations=None)\n</code></pre> <p>Run the agent with the given instruction.</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <p>Instruction to run the agent with.</p> required <code>max_iterations</code> <p>Maximum number of iterations of reasoning steps, defaults to 10.</p> <code>None</code> Return <p>AgentOutput object.</p> Source code in <code>kotaemon\\agents\\react\\agent.py</code> <pre><code>def run(self, instruction, max_iterations=None) -&gt; AgentOutput:\n    \"\"\"\n    Run the agent with the given instruction.\n\n    Args:\n        instruction: Instruction to run the agent with.\n        max_iterations: Maximum number of iterations\n            of reasoning steps, defaults to 10.\n\n    Return:\n        AgentOutput object.\n    \"\"\"\n    if not max_iterations:\n        max_iterations = self.max_iterations\n    assert max_iterations &gt; 0\n\n    self.clear()\n    logging.info(f\"Running {self.name} with instruction: {instruction}\")\n    total_cost = 0.0\n    total_token = 0\n    status = \"failed\"\n    response_text = None\n\n    for step_count in range(1, max_iterations + 1):\n        prompt = self._compose_prompt(instruction)\n        logging.info(f\"Prompt: {prompt}\")\n        response = self.llm(\n            prompt, stop=[\"Observation:\"]\n        )  # could cause bugs if llm doesn't have `stop` as a parameter\n        response_text = response.text\n        logging.info(f\"Response: {response_text}\")\n        action_step = self._parse_output(response_text)\n        if action_step is None:\n            raise ValueError(\"Invalid action\")\n        is_finished_chain = isinstance(action_step, AgentFinish)\n        if is_finished_chain:\n            result = \"\"\n        else:\n            assert isinstance(action_step, AgentAction)\n            action_name = action_step.tool\n            tool_input = action_step.tool_input\n            logging.info(f\"Action: {action_name}\")\n            logging.info(f\"Tool Input: {tool_input}\")\n            result = self._format_function_map()[action_name](tool_input)\n            logging.info(f\"Result: {result}\")\n\n        self.intermediate_steps.append((action_step, result))\n        if is_finished_chain:\n            logging.info(f\"Finished after {step_count} steps.\")\n            status = \"finished\"\n            break\n    else:\n        status = \"stopped\"\n\n    return AgentOutput(\n        text=response_text,\n        agent_type=self.agent_type,\n        status=status,\n        total_tokens=total_token,\n        total_cost=total_cost,\n        intermediate_steps=self.intermediate_steps,\n        max_iterations=max_iterations,\n    )\n</code></pre>"},{"location":"reference/agents/react/prompt/","title":"Prompt","text":""},{"location":"reference/agents/rewoo/","title":"Rewoo","text":""},{"location":"reference/agents/rewoo/#agents.rewoo.RewooAgent","title":"RewooAgent","text":"<p>             Bases: <code>BaseAgent</code></p> <p>Distributive RewooAgent class inherited from BaseAgent. Implementing ReWOO paradigm https://arxiv.org/pdf/2305.18323.pdf</p> Source code in <code>kotaemon\\agents\\rewoo\\agent.py</code> <pre><code>class RewooAgent(BaseAgent):\n    \"\"\"Distributive RewooAgent class inherited from BaseAgent.\n    Implementing ReWOO paradigm https://arxiv.org/pdf/2305.18323.pdf\"\"\"\n\n    name: str = \"RewooAgent\"\n    agent_type: AgentType = AgentType.rewoo\n    description: str = \"RewooAgent for answering multi-step reasoning questions\"\n    planner_llm: BaseLLM\n    solver_llm: BaseLLM\n    prompt_template: dict[str, PromptTemplate] = Param(\n        default_callback=lambda _: {},\n        help=\"A dict to supply different prompt to the agent.\",\n    )\n    plugins: list[BaseTool] = Param(\n        default_callback=lambda _: [], help=\"A list of plugins to be used in the model.\"\n    )\n    examples: dict[str, str | list[str]] = Param(\n        default_callback=lambda _: {}, help=\"Examples to be used in the agent.\"\n    )\n\n    @Node.auto(depends_on=[\"planner_llm\", \"plugins\", \"prompt_template\", \"examples\"])\n    def planner(self):\n        return Planner(\n            model=self.planner_llm,\n            plugins=self.plugins,\n            prompt_template=self.prompt_template.get(\"Planner\", None),\n            examples=self.examples.get(\"Planner\", None),\n        )\n\n    @Node.auto(depends_on=[\"solver_llm\", \"prompt_template\", \"examples\"])\n    def solver(self):\n        return Solver(\n            model=self.solver_llm,\n            prompt_template=self.prompt_template.get(\"Solver\", None),\n            examples=self.examples.get(\"Solver\", None),\n        )\n\n    def _parse_plan_map(\n        self, planner_response: str\n    ) -&gt; tuple[dict[str, list[str]], dict[str, str]]:\n        \"\"\"\n        Parse planner output. It should be an n-to-n mapping from Plans to #Es.\n        This is because sometimes LLM cannot follow the strict output format.\n        Example:\n            #Plan1\n            #E1\n            #E2\n        should result in: {\"#Plan1\": [\"#E1\", \"#E2\"]}\n        Or:\n            #Plan1\n            #Plan2\n            #E1\n        should result in: {\"#Plan1\": [], \"#Plan2\": [\"#E1\"]}\n        This function should also return a plan map.\n\n        Returns:\n            tuple[Dict[str, List[str]], Dict[str, str]]: A list of plan map\n        \"\"\"\n        valid_chunk = [\n            line\n            for line in planner_response.splitlines()\n            if line.startswith(\"#Plan\") or line.startswith(\"#E\")\n        ]\n\n        plan_to_es: dict[str, list[str]] = dict()\n        plans: dict[str, str] = dict()\n        prev_key = \"\"\n        for line in valid_chunk:\n            key, description = line.split(\":\", 1)\n            key = key.strip()\n            if key.startswith(\"#Plan\"):\n                plans[key] = description.strip()\n                plan_to_es[key] = []\n                prev_key = key\n            elif key.startswith(\"#E\"):\n                plan_to_es[prev_key].append(key)\n\n        return plan_to_es, plans\n\n    def _parse_planner_evidences(\n        self, planner_response: str\n    ) -&gt; tuple[dict[str, str], list[list[str]]]:\n        \"\"\"\n        Parse planner output. This should return a mapping from #E to tool call.\n        It should also identify the level of each #E in dependency map.\n        Example:\n            {\n            \"#E1\": \"Tool1\", \"#E2\": \"Tool2\",\n            \"#E3\": \"Tool3\", \"#E4\": \"Tool4\"\n            }, [[#E1, #E2], [#E3, #E4]]\n\n        Returns:\n            tuple[dict[str, str], List[List[str]]]:\n            A mapping from #E to tool call and a list of levels.\n        \"\"\"\n        evidences: dict[str, str] = dict()\n        dependence: dict[str, list[str]] = dict()\n        for line in planner_response.splitlines():\n            if line.startswith(\"#E\") and line[2].isdigit():\n                e, tool_call = line.split(\":\", 1)\n                e, tool_call = e.strip(), tool_call.strip()\n                if len(e) == 3:\n                    dependence[e] = []\n                    evidences[e] = tool_call\n                    for var in re.findall(r\"#E\\d+\", tool_call):\n                        if var in evidences:\n                            dependence[e].append(var)\n                else:\n                    evidences[e] = \"No evidence found\"\n        level = []\n        while dependence:\n            select = [i for i in dependence if not dependence[i]]\n            if len(select) == 0:\n                raise ValueError(\"Circular dependency detected.\")\n            level.append(select)\n            for item in select:\n                dependence.pop(item)\n            for item in dependence:\n                for i in select:\n                    if i in dependence[item]:\n                        dependence[item].remove(i)\n\n        return evidences, level\n\n    def _run_plugin(\n        self,\n        e: str,\n        planner_evidences: dict[str, str],\n        worker_evidences: dict[str, str],\n        output=BaseScratchPad(),\n    ):\n        \"\"\"\n        Run a plugin for a given evidence.\n        This function should also cumulate the cost and tokens.\n        \"\"\"\n        result = dict(e=e, plugin_cost=0, plugin_token=0, evidence=\"\")\n        tool_call = planner_evidences[e]\n        if \"[\" not in tool_call:\n            result[\"evidence\"] = tool_call\n        else:\n            tool, tool_input = tool_call.split(\"[\", 1)\n            tool_input = tool_input[:-1]\n            # find variables in input and replace with previous evidences\n            for var in re.findall(r\"#E\\d+\", tool_input):\n                if var in worker_evidences:\n                    tool_input = tool_input.replace(var, worker_evidences.get(var, \"\"))\n            try:\n                selected_plugin = self._find_plugin(tool)\n                if selected_plugin is None:\n                    raise ValueError(\"Invalid plugin detected\")\n                tool_response = selected_plugin(tool_input)\n                result[\"evidence\"] = get_plugin_response_content(tool_response)\n            except ValueError:\n                result[\"evidence\"] = \"No evidence found.\"\n            finally:\n                output.panel_print(\n                    result[\"evidence\"], f\"[green] Function Response of [blue]{tool}: \"\n                )\n        return result\n\n    def _get_worker_evidence(\n        self,\n        planner_evidences: dict[str, str],\n        evidences_level: list[list[str]],\n        output=BaseScratchPad(),\n    ) -&gt; Any:\n        \"\"\"\n        Parallel execution of plugins in DAG for speedup.\n        This is one of core benefits of ReWOO agents.\n\n        Args:\n            planner_evidences: A mapping from #E to tool call.\n            evidences_level: A list of levels of evidences.\n                Calculated from DAG of plugin calls.\n            output: Output object, defaults to BaseOutput().\n        Returns:\n            A mapping from #E to tool call.\n        \"\"\"\n        worker_evidences: dict[str, str] = dict()\n        plugin_cost, plugin_token = 0.0, 0.0\n        with ThreadPoolExecutor() as pool:\n            for level in evidences_level:\n                results = []\n                for e in level:\n                    results.append(\n                        pool.submit(\n                            self._run_plugin,\n                            e,\n                            planner_evidences,\n                            worker_evidences,\n                            output,\n                        )\n                    )\n                if len(results) &gt; 1:\n                    output.update_status(f\"Running tasks {level} in parallel.\")\n                else:\n                    output.update_status(f\"Running task {level[0]}.\")\n                for r in results:\n                    resp = r.result()\n                    plugin_cost += resp[\"plugin_cost\"]\n                    plugin_token += resp[\"plugin_token\"]\n                    worker_evidences[resp[\"e\"]] = resp[\"evidence\"]\n                output.done()\n\n        return worker_evidences, plugin_cost, plugin_token\n\n    def _find_plugin(self, name: str):\n        for p in self.plugins:\n            if p.name == name:\n                return p\n\n    @BaseAgent.safeguard_run\n    def run(self, instruction: str, use_citation: bool = False) -&gt; AgentOutput:\n        \"\"\"\n        Run the agent with a given instruction.\n        \"\"\"\n        logging.info(f\"Running {self.name} with instruction: {instruction}\")\n        total_cost = 0.0\n        total_token = 0\n\n        # Plan\n        planner_output = self.planner(instruction)\n        planner_text_output = planner_output.text\n        plan_to_es, plans = self._parse_plan_map(planner_text_output)\n        planner_evidences, evidence_level = self._parse_planner_evidences(\n            planner_text_output\n        )\n\n        # Work\n        worker_evidences, plugin_cost, plugin_token = self._get_worker_evidence(\n            planner_evidences, evidence_level\n        )\n        worker_log = \"\"\n        for plan in plan_to_es:\n            worker_log += f\"{plan}: {plans[plan]}\\n\"\n            for e in plan_to_es[plan]:\n                worker_log += f\"{e}: {worker_evidences[e]}\\n\"\n\n        # Solve\n        solver_output = self.solver(instruction, worker_log)\n        solver_output_text = solver_output.text\n        if use_citation:\n            citation_pipeline = CitationPipeline(llm=self.solver_llm)\n            citation = citation_pipeline(context=worker_log, question=instruction)\n        else:\n            citation = None\n\n        return AgentOutput(\n            text=solver_output_text,\n            agent_type=self.agent_type,\n            status=\"finished\",\n            total_tokens=total_token,\n            total_cost=total_cost,\n            citation=citation,\n        )\n</code></pre>"},{"location":"reference/agents/rewoo/#agents.rewoo.RewooAgent.run","title":"run","text":"<pre><code>run(instruction, use_citation=False)\n</code></pre> <p>Run the agent with a given instruction.</p> Source code in <code>kotaemon\\agents\\rewoo\\agent.py</code> <pre><code>@BaseAgent.safeguard_run\ndef run(self, instruction: str, use_citation: bool = False) -&gt; AgentOutput:\n    \"\"\"\n    Run the agent with a given instruction.\n    \"\"\"\n    logging.info(f\"Running {self.name} with instruction: {instruction}\")\n    total_cost = 0.0\n    total_token = 0\n\n    # Plan\n    planner_output = self.planner(instruction)\n    planner_text_output = planner_output.text\n    plan_to_es, plans = self._parse_plan_map(planner_text_output)\n    planner_evidences, evidence_level = self._parse_planner_evidences(\n        planner_text_output\n    )\n\n    # Work\n    worker_evidences, plugin_cost, plugin_token = self._get_worker_evidence(\n        planner_evidences, evidence_level\n    )\n    worker_log = \"\"\n    for plan in plan_to_es:\n        worker_log += f\"{plan}: {plans[plan]}\\n\"\n        for e in plan_to_es[plan]:\n            worker_log += f\"{e}: {worker_evidences[e]}\\n\"\n\n    # Solve\n    solver_output = self.solver(instruction, worker_log)\n    solver_output_text = solver_output.text\n    if use_citation:\n        citation_pipeline = CitationPipeline(llm=self.solver_llm)\n        citation = citation_pipeline(context=worker_log, question=instruction)\n    else:\n        citation = None\n\n    return AgentOutput(\n        text=solver_output_text,\n        agent_type=self.agent_type,\n        status=\"finished\",\n        total_tokens=total_token,\n        total_cost=total_cost,\n        citation=citation,\n    )\n</code></pre>"},{"location":"reference/agents/rewoo/agent/","title":"Agent","text":""},{"location":"reference/agents/rewoo/agent/#agents.rewoo.agent.RewooAgent","title":"RewooAgent","text":"<p>             Bases: <code>BaseAgent</code></p> <p>Distributive RewooAgent class inherited from BaseAgent. Implementing ReWOO paradigm https://arxiv.org/pdf/2305.18323.pdf</p> Source code in <code>kotaemon\\agents\\rewoo\\agent.py</code> <pre><code>class RewooAgent(BaseAgent):\n    \"\"\"Distributive RewooAgent class inherited from BaseAgent.\n    Implementing ReWOO paradigm https://arxiv.org/pdf/2305.18323.pdf\"\"\"\n\n    name: str = \"RewooAgent\"\n    agent_type: AgentType = AgentType.rewoo\n    description: str = \"RewooAgent for answering multi-step reasoning questions\"\n    planner_llm: BaseLLM\n    solver_llm: BaseLLM\n    prompt_template: dict[str, PromptTemplate] = Param(\n        default_callback=lambda _: {},\n        help=\"A dict to supply different prompt to the agent.\",\n    )\n    plugins: list[BaseTool] = Param(\n        default_callback=lambda _: [], help=\"A list of plugins to be used in the model.\"\n    )\n    examples: dict[str, str | list[str]] = Param(\n        default_callback=lambda _: {}, help=\"Examples to be used in the agent.\"\n    )\n\n    @Node.auto(depends_on=[\"planner_llm\", \"plugins\", \"prompt_template\", \"examples\"])\n    def planner(self):\n        return Planner(\n            model=self.planner_llm,\n            plugins=self.plugins,\n            prompt_template=self.prompt_template.get(\"Planner\", None),\n            examples=self.examples.get(\"Planner\", None),\n        )\n\n    @Node.auto(depends_on=[\"solver_llm\", \"prompt_template\", \"examples\"])\n    def solver(self):\n        return Solver(\n            model=self.solver_llm,\n            prompt_template=self.prompt_template.get(\"Solver\", None),\n            examples=self.examples.get(\"Solver\", None),\n        )\n\n    def _parse_plan_map(\n        self, planner_response: str\n    ) -&gt; tuple[dict[str, list[str]], dict[str, str]]:\n        \"\"\"\n        Parse planner output. It should be an n-to-n mapping from Plans to #Es.\n        This is because sometimes LLM cannot follow the strict output format.\n        Example:\n            #Plan1\n            #E1\n            #E2\n        should result in: {\"#Plan1\": [\"#E1\", \"#E2\"]}\n        Or:\n            #Plan1\n            #Plan2\n            #E1\n        should result in: {\"#Plan1\": [], \"#Plan2\": [\"#E1\"]}\n        This function should also return a plan map.\n\n        Returns:\n            tuple[Dict[str, List[str]], Dict[str, str]]: A list of plan map\n        \"\"\"\n        valid_chunk = [\n            line\n            for line in planner_response.splitlines()\n            if line.startswith(\"#Plan\") or line.startswith(\"#E\")\n        ]\n\n        plan_to_es: dict[str, list[str]] = dict()\n        plans: dict[str, str] = dict()\n        prev_key = \"\"\n        for line in valid_chunk:\n            key, description = line.split(\":\", 1)\n            key = key.strip()\n            if key.startswith(\"#Plan\"):\n                plans[key] = description.strip()\n                plan_to_es[key] = []\n                prev_key = key\n            elif key.startswith(\"#E\"):\n                plan_to_es[prev_key].append(key)\n\n        return plan_to_es, plans\n\n    def _parse_planner_evidences(\n        self, planner_response: str\n    ) -&gt; tuple[dict[str, str], list[list[str]]]:\n        \"\"\"\n        Parse planner output. This should return a mapping from #E to tool call.\n        It should also identify the level of each #E in dependency map.\n        Example:\n            {\n            \"#E1\": \"Tool1\", \"#E2\": \"Tool2\",\n            \"#E3\": \"Tool3\", \"#E4\": \"Tool4\"\n            }, [[#E1, #E2], [#E3, #E4]]\n\n        Returns:\n            tuple[dict[str, str], List[List[str]]]:\n            A mapping from #E to tool call and a list of levels.\n        \"\"\"\n        evidences: dict[str, str] = dict()\n        dependence: dict[str, list[str]] = dict()\n        for line in planner_response.splitlines():\n            if line.startswith(\"#E\") and line[2].isdigit():\n                e, tool_call = line.split(\":\", 1)\n                e, tool_call = e.strip(), tool_call.strip()\n                if len(e) == 3:\n                    dependence[e] = []\n                    evidences[e] = tool_call\n                    for var in re.findall(r\"#E\\d+\", tool_call):\n                        if var in evidences:\n                            dependence[e].append(var)\n                else:\n                    evidences[e] = \"No evidence found\"\n        level = []\n        while dependence:\n            select = [i for i in dependence if not dependence[i]]\n            if len(select) == 0:\n                raise ValueError(\"Circular dependency detected.\")\n            level.append(select)\n            for item in select:\n                dependence.pop(item)\n            for item in dependence:\n                for i in select:\n                    if i in dependence[item]:\n                        dependence[item].remove(i)\n\n        return evidences, level\n\n    def _run_plugin(\n        self,\n        e: str,\n        planner_evidences: dict[str, str],\n        worker_evidences: dict[str, str],\n        output=BaseScratchPad(),\n    ):\n        \"\"\"\n        Run a plugin for a given evidence.\n        This function should also cumulate the cost and tokens.\n        \"\"\"\n        result = dict(e=e, plugin_cost=0, plugin_token=0, evidence=\"\")\n        tool_call = planner_evidences[e]\n        if \"[\" not in tool_call:\n            result[\"evidence\"] = tool_call\n        else:\n            tool, tool_input = tool_call.split(\"[\", 1)\n            tool_input = tool_input[:-1]\n            # find variables in input and replace with previous evidences\n            for var in re.findall(r\"#E\\d+\", tool_input):\n                if var in worker_evidences:\n                    tool_input = tool_input.replace(var, worker_evidences.get(var, \"\"))\n            try:\n                selected_plugin = self._find_plugin(tool)\n                if selected_plugin is None:\n                    raise ValueError(\"Invalid plugin detected\")\n                tool_response = selected_plugin(tool_input)\n                result[\"evidence\"] = get_plugin_response_content(tool_response)\n            except ValueError:\n                result[\"evidence\"] = \"No evidence found.\"\n            finally:\n                output.panel_print(\n                    result[\"evidence\"], f\"[green] Function Response of [blue]{tool}: \"\n                )\n        return result\n\n    def _get_worker_evidence(\n        self,\n        planner_evidences: dict[str, str],\n        evidences_level: list[list[str]],\n        output=BaseScratchPad(),\n    ) -&gt; Any:\n        \"\"\"\n        Parallel execution of plugins in DAG for speedup.\n        This is one of core benefits of ReWOO agents.\n\n        Args:\n            planner_evidences: A mapping from #E to tool call.\n            evidences_level: A list of levels of evidences.\n                Calculated from DAG of plugin calls.\n            output: Output object, defaults to BaseOutput().\n        Returns:\n            A mapping from #E to tool call.\n        \"\"\"\n        worker_evidences: dict[str, str] = dict()\n        plugin_cost, plugin_token = 0.0, 0.0\n        with ThreadPoolExecutor() as pool:\n            for level in evidences_level:\n                results = []\n                for e in level:\n                    results.append(\n                        pool.submit(\n                            self._run_plugin,\n                            e,\n                            planner_evidences,\n                            worker_evidences,\n                            output,\n                        )\n                    )\n                if len(results) &gt; 1:\n                    output.update_status(f\"Running tasks {level} in parallel.\")\n                else:\n                    output.update_status(f\"Running task {level[0]}.\")\n                for r in results:\n                    resp = r.result()\n                    plugin_cost += resp[\"plugin_cost\"]\n                    plugin_token += resp[\"plugin_token\"]\n                    worker_evidences[resp[\"e\"]] = resp[\"evidence\"]\n                output.done()\n\n        return worker_evidences, plugin_cost, plugin_token\n\n    def _find_plugin(self, name: str):\n        for p in self.plugins:\n            if p.name == name:\n                return p\n\n    @BaseAgent.safeguard_run\n    def run(self, instruction: str, use_citation: bool = False) -&gt; AgentOutput:\n        \"\"\"\n        Run the agent with a given instruction.\n        \"\"\"\n        logging.info(f\"Running {self.name} with instruction: {instruction}\")\n        total_cost = 0.0\n        total_token = 0\n\n        # Plan\n        planner_output = self.planner(instruction)\n        planner_text_output = planner_output.text\n        plan_to_es, plans = self._parse_plan_map(planner_text_output)\n        planner_evidences, evidence_level = self._parse_planner_evidences(\n            planner_text_output\n        )\n\n        # Work\n        worker_evidences, plugin_cost, plugin_token = self._get_worker_evidence(\n            planner_evidences, evidence_level\n        )\n        worker_log = \"\"\n        for plan in plan_to_es:\n            worker_log += f\"{plan}: {plans[plan]}\\n\"\n            for e in plan_to_es[plan]:\n                worker_log += f\"{e}: {worker_evidences[e]}\\n\"\n\n        # Solve\n        solver_output = self.solver(instruction, worker_log)\n        solver_output_text = solver_output.text\n        if use_citation:\n            citation_pipeline = CitationPipeline(llm=self.solver_llm)\n            citation = citation_pipeline(context=worker_log, question=instruction)\n        else:\n            citation = None\n\n        return AgentOutput(\n            text=solver_output_text,\n            agent_type=self.agent_type,\n            status=\"finished\",\n            total_tokens=total_token,\n            total_cost=total_cost,\n            citation=citation,\n        )\n</code></pre>"},{"location":"reference/agents/rewoo/agent/#agents.rewoo.agent.RewooAgent.run","title":"run","text":"<pre><code>run(instruction, use_citation=False)\n</code></pre> <p>Run the agent with a given instruction.</p> Source code in <code>kotaemon\\agents\\rewoo\\agent.py</code> <pre><code>@BaseAgent.safeguard_run\ndef run(self, instruction: str, use_citation: bool = False) -&gt; AgentOutput:\n    \"\"\"\n    Run the agent with a given instruction.\n    \"\"\"\n    logging.info(f\"Running {self.name} with instruction: {instruction}\")\n    total_cost = 0.0\n    total_token = 0\n\n    # Plan\n    planner_output = self.planner(instruction)\n    planner_text_output = planner_output.text\n    plan_to_es, plans = self._parse_plan_map(planner_text_output)\n    planner_evidences, evidence_level = self._parse_planner_evidences(\n        planner_text_output\n    )\n\n    # Work\n    worker_evidences, plugin_cost, plugin_token = self._get_worker_evidence(\n        planner_evidences, evidence_level\n    )\n    worker_log = \"\"\n    for plan in plan_to_es:\n        worker_log += f\"{plan}: {plans[plan]}\\n\"\n        for e in plan_to_es[plan]:\n            worker_log += f\"{e}: {worker_evidences[e]}\\n\"\n\n    # Solve\n    solver_output = self.solver(instruction, worker_log)\n    solver_output_text = solver_output.text\n    if use_citation:\n        citation_pipeline = CitationPipeline(llm=self.solver_llm)\n        citation = citation_pipeline(context=worker_log, question=instruction)\n    else:\n        citation = None\n\n    return AgentOutput(\n        text=solver_output_text,\n        agent_type=self.agent_type,\n        status=\"finished\",\n        total_tokens=total_token,\n        total_cost=total_cost,\n        citation=citation,\n    )\n</code></pre>"},{"location":"reference/agents/rewoo/planner/","title":"Planner","text":""},{"location":"reference/agents/rewoo/planner/#agents.rewoo.planner.Planner","title":"Planner","text":"<p>             Bases: <code>BaseComponent</code></p> Source code in <code>kotaemon\\agents\\rewoo\\planner.py</code> <pre><code>class Planner(BaseComponent):\n    model: BaseLLM\n    prompt_template: Optional[PromptTemplate] = None\n    examples: Optional[Union[str, List[str]]] = None\n    plugins: List[BaseTool]\n\n    def _compose_worker_description(self) -&gt; str:\n        \"\"\"\n        Compose the worker prompt from the workers.\n\n        Example:\n        toolname1[input]: tool1 description\n        toolname2[input]: tool2 description\n        \"\"\"\n        prompt = \"\"\n        try:\n            for worker in self.plugins:\n                prompt += f\"{worker.name}[input]: {worker.description}\\n\"\n        except Exception:\n            raise ValueError(\"Worker must have a name and description.\")\n        return prompt\n\n    def _compose_fewshot_prompt(self) -&gt; str:\n        if self.examples is None:\n            return \"\"\n        if isinstance(self.examples, str):\n            return self.examples\n        else:\n            return \"\\n\\n\".join([e.strip(\"\\n\") for e in self.examples])\n\n    def _compose_prompt(self, instruction) -&gt; str:\n        \"\"\"\n        Compose the prompt from template, worker description, examples and instruction.\n        \"\"\"\n        worker_desctription = self._compose_worker_description()\n        fewshot = self._compose_fewshot_prompt()\n        if self.prompt_template is not None:\n            if \"fewshot\" in self.prompt_template.placeholders:\n                return self.prompt_template.populate(\n                    tool_description=worker_desctription,\n                    fewshot=fewshot,\n                    task=instruction,\n                )\n            else:\n                return self.prompt_template.populate(\n                    tool_description=worker_desctription, task=instruction\n                )\n        else:\n            if self.examples is not None:\n                return few_shot_planner_prompt.populate(\n                    tool_description=worker_desctription,\n                    fewshot=fewshot,\n                    task=instruction,\n                )\n            else:\n                return zero_shot_planner_prompt.populate(\n                    tool_description=worker_desctription, task=instruction\n                )\n\n    def run(self, instruction: str, output: BaseScratchPad = BaseScratchPad()) -&gt; Any:\n        response = None\n        output.info(\"Running Planner\")\n        prompt = self._compose_prompt(instruction)\n        output.debug(f\"Prompt: {prompt}\")\n        try:\n            response = self.model(prompt)\n            self.log_progress(\".planner\", response=response)\n            output.info(\"Planner run successful.\")\n        except ValueError as e:\n            output.error(\"Planner failed to retrieve response from LLM\")\n            raise ValueError(\"Planner failed to retrieve response from LLM\") from e\n\n        return response\n</code></pre>"},{"location":"reference/agents/rewoo/prompt/","title":"Prompt","text":""},{"location":"reference/agents/rewoo/solver/","title":"Solver","text":""},{"location":"reference/agents/rewoo/solver/#agents.rewoo.solver.Solver","title":"Solver","text":"<p>             Bases: <code>BaseComponent</code></p> Source code in <code>kotaemon\\agents\\rewoo\\solver.py</code> <pre><code>class Solver(BaseComponent):\n    model: BaseLLM\n    prompt_template: Optional[PromptTemplate] = None\n    examples: Optional[Union[str, List[str]]] = None\n\n    def _compose_fewshot_prompt(self) -&gt; str:\n        if self.examples is None:\n            return \"\"\n        if isinstance(self.examples, str):\n            return self.examples\n        else:\n            return \"\\n\\n\".join([e.strip(\"\\n\") for e in self.examples])\n\n    def _compose_prompt(self, instruction, plan_evidence) -&gt; str:\n        \"\"\"\n        Compose the prompt from template, plan&amp;evidence, examples and instruction.\n        \"\"\"\n        fewshot = self._compose_fewshot_prompt()\n        if self.prompt_template is not None:\n            if \"fewshot\" in self.prompt_template.placeholders:\n                return self.prompt_template.populate(\n                    plan_evidence=plan_evidence, fewshot=fewshot, task=instruction\n                )\n            else:\n                return self.prompt_template.populate(\n                    plan_evidence=plan_evidence, task=instruction\n                )\n        else:\n            if self.examples is not None:\n                return few_shot_solver_prompt.populate(\n                    plan_evidence=plan_evidence, fewshot=fewshot, task=instruction\n                )\n            else:\n                return zero_shot_solver_prompt.populate(\n                    plan_evidence=plan_evidence, task=instruction\n                )\n\n    def run(\n        self,\n        instruction: str,\n        plan_evidence: str,\n        output: BaseScratchPad = BaseScratchPad(),\n    ) -&gt; Any:\n        response = None\n        output.info(\"Running Solver\")\n        output.debug(f\"Instruction: {instruction}\")\n        output.debug(f\"Plan Evidence: {plan_evidence}\")\n        prompt = self._compose_prompt(instruction, plan_evidence)\n        output.debug(f\"Prompt: {prompt}\")\n        try:\n            response = self.model(prompt)\n            output.info(\"Solver run successful.\")\n        except ValueError:\n            output.error(\"Solver failed to retrieve response from LLM\")\n\n        return response\n</code></pre>"},{"location":"reference/agents/tools/","title":"Tools","text":""},{"location":"reference/agents/tools/#agents.tools.BaseTool","title":"BaseTool","text":"<p>             Bases: <code>BaseComponent</code></p> Source code in <code>kotaemon\\agents\\tools\\base.py</code> <pre><code>class BaseTool(BaseComponent):\n    name: str\n    \"\"\"The unique name of the tool that clearly communicates its purpose.\"\"\"\n    description: str\n    \"\"\"Description used to tell the model how/when/why to use the tool.\n    You can provide few-shot examples as a part of the description. This will be\n    input to the prompt of LLM.\n    \"\"\"\n    args_schema: Optional[Type[BaseModel]] = None\n    \"\"\"Pydantic model class to validate and parse the tool's input arguments.\"\"\"\n    verbose: bool = False\n    \"\"\"Whether to log the tool's progress.\"\"\"\n    handle_tool_error: Optional[\n        Union[bool, str, Callable[[ToolException], str]]\n    ] = False\n    \"\"\"Handle the content of the ToolException thrown.\"\"\"\n\n    def _parse_input(\n        self,\n        tool_input: Union[str, Dict],\n    ) -&gt; Union[str, Dict[str, Any]]:\n        \"\"\"Convert tool input to pydantic model.\"\"\"\n        args_schema = self.args_schema\n        if isinstance(tool_input, str):\n            if args_schema is not None:\n                key_ = next(iter(args_schema.__fields__.keys()))\n                args_schema.validate({key_: tool_input})\n            return tool_input\n        else:\n            if args_schema is not None:\n                result = args_schema.parse_obj(tool_input)\n                return {k: v for k, v in result.dict().items() if k in tool_input}\n        return tool_input\n\n    def _run_tool(\n        self,\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"Call tool.\"\"\"\n        raise NotImplementedError(f\"_run_tool is not implemented for {self.name}\")\n\n    def _to_args_and_kwargs(self, tool_input: Union[str, Dict]) -&gt; Tuple[Tuple, Dict]:\n        # For backwards compatibility, if run_input is a string,\n        # pass as a positional argument.\n        if isinstance(tool_input, str):\n            return (tool_input,), {}\n        else:\n            return (), tool_input\n\n    def _handle_tool_error(self, e: ToolException) -&gt; Any:\n        \"\"\"Handle the content of the ToolException thrown.\"\"\"\n        observation = None\n        if not self.handle_tool_error:\n            raise e\n        elif isinstance(self.handle_tool_error, bool):\n            if e.args:\n                observation = e.args[0]\n            else:\n                observation = \"Tool execution error\"\n        elif isinstance(self.handle_tool_error, str):\n            observation = self.handle_tool_error\n        elif callable(self.handle_tool_error):\n            observation = self.handle_tool_error(e)\n        else:\n            raise ValueError(\n                f\"Got unexpected type of `handle_tool_error`. Expected bool, str \"\n                f\"or callable. Received: {self.handle_tool_error}\"\n            )\n        return observation\n\n    def to_langchain_format(self) -&gt; LCTool:\n        \"\"\"Convert this tool to Langchain format to use with its agent\"\"\"\n        return LCTool(name=self.name, description=self.description, func=self.run)\n\n    def run(\n        self,\n        tool_input: Union[str, Dict],\n        verbose: Optional[bool] = None,\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"Run the tool.\"\"\"\n        parsed_input = self._parse_input(tool_input)\n        # TODO (verbose_): Add logging\n        try:\n            tool_args, tool_kwargs = self._to_args_and_kwargs(parsed_input)\n            call_kwargs = {**kwargs, **tool_kwargs}\n            observation = self._run_tool(*tool_args, **call_kwargs)\n        except ToolException as e:\n            observation = self._handle_tool_error(e)\n            return observation\n        else:\n            return observation\n\n    @classmethod\n    def from_langchain_format(cls, langchain_tool: LCTool) -&gt; \"BaseTool\":\n        \"\"\"Wrapper for Langchain Tool\"\"\"\n        new_tool = BaseTool(\n            name=langchain_tool.name, description=langchain_tool.description\n        )\n        new_tool._run_tool = langchain_tool._run  # type: ignore\n        return new_tool\n</code></pre>"},{"location":"reference/agents/tools/#agents.tools.BaseTool.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name\n</code></pre> <p>The unique name of the tool that clearly communicates its purpose.</p>"},{"location":"reference/agents/tools/#agents.tools.BaseTool.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description\n</code></pre> <p>Description used to tell the model how/when/why to use the tool. You can provide few-shot examples as a part of the description. This will be input to the prompt of LLM.</p>"},{"location":"reference/agents/tools/#agents.tools.BaseTool.args_schema","title":"args_schema  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>args_schema = None\n</code></pre> <p>Pydantic model class to validate and parse the tool's input arguments.</p>"},{"location":"reference/agents/tools/#agents.tools.BaseTool.verbose","title":"verbose  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>verbose = False\n</code></pre> <p>Whether to log the tool's progress.</p>"},{"location":"reference/agents/tools/#agents.tools.BaseTool.handle_tool_error","title":"handle_tool_error  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>handle_tool_error = False\n</code></pre> <p>Handle the content of the ToolException thrown.</p>"},{"location":"reference/agents/tools/#agents.tools.BaseTool.to_langchain_format","title":"to_langchain_format","text":"<pre><code>to_langchain_format()\n</code></pre> <p>Convert this tool to Langchain format to use with its agent</p> Source code in <code>kotaemon\\agents\\tools\\base.py</code> <pre><code>def to_langchain_format(self) -&gt; LCTool:\n    \"\"\"Convert this tool to Langchain format to use with its agent\"\"\"\n    return LCTool(name=self.name, description=self.description, func=self.run)\n</code></pre>"},{"location":"reference/agents/tools/#agents.tools.BaseTool.run","title":"run","text":"<pre><code>run(tool_input, verbose=None, **kwargs)\n</code></pre> <p>Run the tool.</p> Source code in <code>kotaemon\\agents\\tools\\base.py</code> <pre><code>def run(\n    self,\n    tool_input: Union[str, Dict],\n    verbose: Optional[bool] = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Run the tool.\"\"\"\n    parsed_input = self._parse_input(tool_input)\n    # TODO (verbose_): Add logging\n    try:\n        tool_args, tool_kwargs = self._to_args_and_kwargs(parsed_input)\n        call_kwargs = {**kwargs, **tool_kwargs}\n        observation = self._run_tool(*tool_args, **call_kwargs)\n    except ToolException as e:\n        observation = self._handle_tool_error(e)\n        return observation\n    else:\n        return observation\n</code></pre>"},{"location":"reference/agents/tools/#agents.tools.BaseTool.from_langchain_format","title":"from_langchain_format  <code>classmethod</code>","text":"<pre><code>from_langchain_format(langchain_tool)\n</code></pre> <p>Wrapper for Langchain Tool</p> Source code in <code>kotaemon\\agents\\tools\\base.py</code> <pre><code>@classmethod\ndef from_langchain_format(cls, langchain_tool: LCTool) -&gt; \"BaseTool\":\n    \"\"\"Wrapper for Langchain Tool\"\"\"\n    new_tool = BaseTool(\n        name=langchain_tool.name, description=langchain_tool.description\n    )\n    new_tool._run_tool = langchain_tool._run  # type: ignore\n    return new_tool\n</code></pre>"},{"location":"reference/agents/tools/#agents.tools.ComponentTool","title":"ComponentTool","text":"<p>             Bases: <code>BaseTool</code></p> <p>A Tool based on another pipeline / BaseComponent to be used as its main entry point</p> Source code in <code>kotaemon\\agents\\tools\\base.py</code> <pre><code>class ComponentTool(BaseTool):\n    \"\"\"\n    A Tool based on another pipeline / BaseComponent to be used\n    as its main entry point\n    \"\"\"\n\n    component: BaseComponent\n    postprocessor: Optional[Callable] = None\n\n    def _run_tool(self, *args: Any, **kwargs: Any) -&gt; Any:\n        output = self.component(*args, **kwargs)\n        if self.postprocessor:\n            output = self.postprocessor(output)\n\n        return output\n</code></pre>"},{"location":"reference/agents/tools/#agents.tools.WikipediaTool","title":"WikipediaTool","text":"<p>             Bases: <code>BaseTool</code></p> <p>Tool that adds the capability to query the Wikipedia API.</p> Source code in <code>kotaemon\\agents\\tools\\wikipedia.py</code> <pre><code>class WikipediaTool(BaseTool):\n    \"\"\"Tool that adds the capability to query the Wikipedia API.\"\"\"\n\n    name: str = \"wikipedia\"\n    description: str = (\n        \"Search engine from Wikipedia, retrieving relevant wiki page. \"\n        \"Useful when you need to get holistic knowledge about people, \"\n        \"places, companies, historical events, or other subjects. \"\n        \"Input should be a search query.\"\n    )\n    args_schema: Optional[Type[BaseModel]] = WikipediaArgs\n    doc_store: Any = None\n\n    def _run_tool(self, query: AnyStr) -&gt; AnyStr:\n        if not self.doc_store:\n            self.doc_store = Wiki()\n        tool = self.doc_store\n        evidence = tool.search(query)\n        return evidence\n</code></pre>"},{"location":"reference/agents/tools/base/","title":"Base","text":""},{"location":"reference/agents/tools/base/#agents.tools.base.ToolException","title":"ToolException","text":"<p>             Bases: <code>Exception</code></p> <p>An optional exception that tool throws when execution error occurs.</p> <p>When this exception is thrown, the agent will not stop working, but will handle the exception according to the handle_tool_error variable of the tool, and the processing result will be returned to the agent as observation, and printed in red on the console.</p> Source code in <code>kotaemon\\agents\\tools\\base.py</code> <pre><code>class ToolException(Exception):\n    \"\"\"An optional exception that tool throws when execution error occurs.\n\n    When this exception is thrown, the agent will not stop working,\n    but will handle the exception according to the handle_tool_error\n    variable of the tool, and the processing result will be returned\n    to the agent as observation, and printed in red on the console.\n    \"\"\"\n</code></pre>"},{"location":"reference/agents/tools/base/#agents.tools.base.BaseTool","title":"BaseTool","text":"<p>             Bases: <code>BaseComponent</code></p> Source code in <code>kotaemon\\agents\\tools\\base.py</code> <pre><code>class BaseTool(BaseComponent):\n    name: str\n    \"\"\"The unique name of the tool that clearly communicates its purpose.\"\"\"\n    description: str\n    \"\"\"Description used to tell the model how/when/why to use the tool.\n    You can provide few-shot examples as a part of the description. This will be\n    input to the prompt of LLM.\n    \"\"\"\n    args_schema: Optional[Type[BaseModel]] = None\n    \"\"\"Pydantic model class to validate and parse the tool's input arguments.\"\"\"\n    verbose: bool = False\n    \"\"\"Whether to log the tool's progress.\"\"\"\n    handle_tool_error: Optional[\n        Union[bool, str, Callable[[ToolException], str]]\n    ] = False\n    \"\"\"Handle the content of the ToolException thrown.\"\"\"\n\n    def _parse_input(\n        self,\n        tool_input: Union[str, Dict],\n    ) -&gt; Union[str, Dict[str, Any]]:\n        \"\"\"Convert tool input to pydantic model.\"\"\"\n        args_schema = self.args_schema\n        if isinstance(tool_input, str):\n            if args_schema is not None:\n                key_ = next(iter(args_schema.__fields__.keys()))\n                args_schema.validate({key_: tool_input})\n            return tool_input\n        else:\n            if args_schema is not None:\n                result = args_schema.parse_obj(tool_input)\n                return {k: v for k, v in result.dict().items() if k in tool_input}\n        return tool_input\n\n    def _run_tool(\n        self,\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"Call tool.\"\"\"\n        raise NotImplementedError(f\"_run_tool is not implemented for {self.name}\")\n\n    def _to_args_and_kwargs(self, tool_input: Union[str, Dict]) -&gt; Tuple[Tuple, Dict]:\n        # For backwards compatibility, if run_input is a string,\n        # pass as a positional argument.\n        if isinstance(tool_input, str):\n            return (tool_input,), {}\n        else:\n            return (), tool_input\n\n    def _handle_tool_error(self, e: ToolException) -&gt; Any:\n        \"\"\"Handle the content of the ToolException thrown.\"\"\"\n        observation = None\n        if not self.handle_tool_error:\n            raise e\n        elif isinstance(self.handle_tool_error, bool):\n            if e.args:\n                observation = e.args[0]\n            else:\n                observation = \"Tool execution error\"\n        elif isinstance(self.handle_tool_error, str):\n            observation = self.handle_tool_error\n        elif callable(self.handle_tool_error):\n            observation = self.handle_tool_error(e)\n        else:\n            raise ValueError(\n                f\"Got unexpected type of `handle_tool_error`. Expected bool, str \"\n                f\"or callable. Received: {self.handle_tool_error}\"\n            )\n        return observation\n\n    def to_langchain_format(self) -&gt; LCTool:\n        \"\"\"Convert this tool to Langchain format to use with its agent\"\"\"\n        return LCTool(name=self.name, description=self.description, func=self.run)\n\n    def run(\n        self,\n        tool_input: Union[str, Dict],\n        verbose: Optional[bool] = None,\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"Run the tool.\"\"\"\n        parsed_input = self._parse_input(tool_input)\n        # TODO (verbose_): Add logging\n        try:\n            tool_args, tool_kwargs = self._to_args_and_kwargs(parsed_input)\n            call_kwargs = {**kwargs, **tool_kwargs}\n            observation = self._run_tool(*tool_args, **call_kwargs)\n        except ToolException as e:\n            observation = self._handle_tool_error(e)\n            return observation\n        else:\n            return observation\n\n    @classmethod\n    def from_langchain_format(cls, langchain_tool: LCTool) -&gt; \"BaseTool\":\n        \"\"\"Wrapper for Langchain Tool\"\"\"\n        new_tool = BaseTool(\n            name=langchain_tool.name, description=langchain_tool.description\n        )\n        new_tool._run_tool = langchain_tool._run  # type: ignore\n        return new_tool\n</code></pre>"},{"location":"reference/agents/tools/base/#agents.tools.base.BaseTool.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name\n</code></pre> <p>The unique name of the tool that clearly communicates its purpose.</p>"},{"location":"reference/agents/tools/base/#agents.tools.base.BaseTool.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description\n</code></pre> <p>Description used to tell the model how/when/why to use the tool. You can provide few-shot examples as a part of the description. This will be input to the prompt of LLM.</p>"},{"location":"reference/agents/tools/base/#agents.tools.base.BaseTool.args_schema","title":"args_schema  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>args_schema = None\n</code></pre> <p>Pydantic model class to validate and parse the tool's input arguments.</p>"},{"location":"reference/agents/tools/base/#agents.tools.base.BaseTool.verbose","title":"verbose  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>verbose = False\n</code></pre> <p>Whether to log the tool's progress.</p>"},{"location":"reference/agents/tools/base/#agents.tools.base.BaseTool.handle_tool_error","title":"handle_tool_error  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>handle_tool_error = False\n</code></pre> <p>Handle the content of the ToolException thrown.</p>"},{"location":"reference/agents/tools/base/#agents.tools.base.BaseTool.to_langchain_format","title":"to_langchain_format","text":"<pre><code>to_langchain_format()\n</code></pre> <p>Convert this tool to Langchain format to use with its agent</p> Source code in <code>kotaemon\\agents\\tools\\base.py</code> <pre><code>def to_langchain_format(self) -&gt; LCTool:\n    \"\"\"Convert this tool to Langchain format to use with its agent\"\"\"\n    return LCTool(name=self.name, description=self.description, func=self.run)\n</code></pre>"},{"location":"reference/agents/tools/base/#agents.tools.base.BaseTool.run","title":"run","text":"<pre><code>run(tool_input, verbose=None, **kwargs)\n</code></pre> <p>Run the tool.</p> Source code in <code>kotaemon\\agents\\tools\\base.py</code> <pre><code>def run(\n    self,\n    tool_input: Union[str, Dict],\n    verbose: Optional[bool] = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Run the tool.\"\"\"\n    parsed_input = self._parse_input(tool_input)\n    # TODO (verbose_): Add logging\n    try:\n        tool_args, tool_kwargs = self._to_args_and_kwargs(parsed_input)\n        call_kwargs = {**kwargs, **tool_kwargs}\n        observation = self._run_tool(*tool_args, **call_kwargs)\n    except ToolException as e:\n        observation = self._handle_tool_error(e)\n        return observation\n    else:\n        return observation\n</code></pre>"},{"location":"reference/agents/tools/base/#agents.tools.base.BaseTool.from_langchain_format","title":"from_langchain_format  <code>classmethod</code>","text":"<pre><code>from_langchain_format(langchain_tool)\n</code></pre> <p>Wrapper for Langchain Tool</p> Source code in <code>kotaemon\\agents\\tools\\base.py</code> <pre><code>@classmethod\ndef from_langchain_format(cls, langchain_tool: LCTool) -&gt; \"BaseTool\":\n    \"\"\"Wrapper for Langchain Tool\"\"\"\n    new_tool = BaseTool(\n        name=langchain_tool.name, description=langchain_tool.description\n    )\n    new_tool._run_tool = langchain_tool._run  # type: ignore\n    return new_tool\n</code></pre>"},{"location":"reference/agents/tools/base/#agents.tools.base.ComponentTool","title":"ComponentTool","text":"<p>             Bases: <code>BaseTool</code></p> <p>A Tool based on another pipeline / BaseComponent to be used as its main entry point</p> Source code in <code>kotaemon\\agents\\tools\\base.py</code> <pre><code>class ComponentTool(BaseTool):\n    \"\"\"\n    A Tool based on another pipeline / BaseComponent to be used\n    as its main entry point\n    \"\"\"\n\n    component: BaseComponent\n    postprocessor: Optional[Callable] = None\n\n    def _run_tool(self, *args: Any, **kwargs: Any) -&gt; Any:\n        output = self.component(*args, **kwargs)\n        if self.postprocessor:\n            output = self.postprocessor(output)\n\n        return output\n</code></pre>"},{"location":"reference/agents/tools/google/","title":"Google","text":""},{"location":"reference/agents/tools/llm/","title":"Llm","text":""},{"location":"reference/agents/tools/wikipedia/","title":"Wikipedia","text":""},{"location":"reference/agents/tools/wikipedia/#agents.tools.wikipedia.Wiki","title":"Wiki","text":"<p>Wrapper around wikipedia API.</p> Source code in <code>kotaemon\\agents\\tools\\wikipedia.py</code> <pre><code>class Wiki:\n    \"\"\"Wrapper around wikipedia API.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Check that wikipedia package is installed.\"\"\"\n        try:\n            import wikipedia  # noqa: F401\n        except ImportError:\n            raise ValueError(\n                \"Could not import wikipedia python package. \"\n                \"Please install it with `pip install wikipedia`.\"\n            )\n\n    def search(self, search: str) -&gt; Union[str, Document]:\n        \"\"\"Try to search for wiki page.\n\n        If page exists, return the page summary, and a PageWithLookups object.\n        If page does not exist, return similar entries.\n        \"\"\"\n        import wikipedia\n\n        try:\n            page_content = wikipedia.page(search).content\n            url = wikipedia.page(search).url\n            result: Union[str, Document] = Document(\n                text=page_content, metadata={\"page\": url}\n            )\n        except wikipedia.PageError:\n            result = f\"Could not find [{search}]. Similar: {wikipedia.search(search)}\"\n        except wikipedia.DisambiguationError:\n            result = f\"Could not find [{search}]. Similar: {wikipedia.search(search)}\"\n        return result\n</code></pre>"},{"location":"reference/agents/tools/wikipedia/#agents.tools.wikipedia.Wiki.search","title":"search","text":"<pre><code>search(search)\n</code></pre> <p>Try to search for wiki page.</p> <p>If page exists, return the page summary, and a PageWithLookups object. If page does not exist, return similar entries.</p> Source code in <code>kotaemon\\agents\\tools\\wikipedia.py</code> <pre><code>def search(self, search: str) -&gt; Union[str, Document]:\n    \"\"\"Try to search for wiki page.\n\n    If page exists, return the page summary, and a PageWithLookups object.\n    If page does not exist, return similar entries.\n    \"\"\"\n    import wikipedia\n\n    try:\n        page_content = wikipedia.page(search).content\n        url = wikipedia.page(search).url\n        result: Union[str, Document] = Document(\n            text=page_content, metadata={\"page\": url}\n        )\n    except wikipedia.PageError:\n        result = f\"Could not find [{search}]. Similar: {wikipedia.search(search)}\"\n    except wikipedia.DisambiguationError:\n        result = f\"Could not find [{search}]. Similar: {wikipedia.search(search)}\"\n    return result\n</code></pre>"},{"location":"reference/agents/tools/wikipedia/#agents.tools.wikipedia.WikipediaTool","title":"WikipediaTool","text":"<p>             Bases: <code>BaseTool</code></p> <p>Tool that adds the capability to query the Wikipedia API.</p> Source code in <code>kotaemon\\agents\\tools\\wikipedia.py</code> <pre><code>class WikipediaTool(BaseTool):\n    \"\"\"Tool that adds the capability to query the Wikipedia API.\"\"\"\n\n    name: str = \"wikipedia\"\n    description: str = (\n        \"Search engine from Wikipedia, retrieving relevant wiki page. \"\n        \"Useful when you need to get holistic knowledge about people, \"\n        \"places, companies, historical events, or other subjects. \"\n        \"Input should be a search query.\"\n    )\n    args_schema: Optional[Type[BaseModel]] = WikipediaArgs\n    doc_store: Any = None\n\n    def _run_tool(self, query: AnyStr) -&gt; AnyStr:\n        if not self.doc_store:\n            self.doc_store = Wiki()\n        tool = self.doc_store\n        evidence = tool.search(query)\n        return evidence\n</code></pre>"},{"location":"reference/base/","title":"Base","text":""},{"location":"reference/base/#base.BaseComponent","title":"BaseComponent","text":"<p>             Bases: <code>Function</code></p> <p>A component is a class that can be used to compose a pipeline.</p> <p>Benefits of component</p> <ul> <li>Auto caching, logging</li> <li>Allow deployment</li> </ul> <p>For each component, the spirit is</p> <ul> <li>Tolerate multiple input types, e.g. str, Document, List[str], List[Document]</li> <li>Enforce single output type. Hence, the output type of a component should be</li> </ul> <p>as generic as possible.</p> Source code in <code>kotaemon\\base\\component.py</code> <pre><code>class BaseComponent(Function):\n    \"\"\"A component is a class that can be used to compose a pipeline.\n\n    !!!tip \"Benefits of component\"\n        - Auto caching, logging\n        - Allow deployment\n\n    !!! tip \"For each component, the spirit is\"\n        - Tolerate multiple input types, e.g. str, Document, List[str], List[Document]\n        - Enforce single output type. Hence, the output type of a component should be\n    as generic as possible.\n    \"\"\"\n\n    inflow = None\n\n    def flow(self):\n        if self.inflow is None:\n            raise ValueError(\"No inflow provided.\")\n\n        if not isinstance(self.inflow, BaseComponent):\n            raise ValueError(\n                f\"inflow must be a BaseComponent, found {type(self.inflow)}\"\n            )\n\n        return self.__call__(self.inflow.flow())\n\n    @abstractmethod\n    def run(self, *args, **kwargs) -&gt; Document | list[Document] | None:\n        \"\"\"Run the component.\"\"\"\n        ...\n</code></pre>"},{"location":"reference/base/#base.BaseComponent.run","title":"run  <code>abstractmethod</code>","text":"<pre><code>run(*args, **kwargs)\n</code></pre> <p>Run the component.</p> Source code in <code>kotaemon\\base\\component.py</code> <pre><code>@abstractmethod\ndef run(self, *args, **kwargs) -&gt; Document | list[Document] | None:\n    \"\"\"Run the component.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/base/#base.Document","title":"Document","text":"<p>             Bases: <code>Document</code></p> <p>Base document class, mostly inherited from Document class from llama-index.</p> <p>This class accept one positional argument <code>content</code> of an arbitrary type, which will     store the raw content of the document. If specified, the class will use     <code>content</code> to initialize the base llama_index class.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Optional[Any]</code> <p>the raw content of the document.</p> <code>None</code> Source code in <code>kotaemon\\base\\schema.py</code> <pre><code>class Document(BaseDocument):\n    \"\"\"\n    Base document class, mostly inherited from Document class from llama-index.\n\n    This class accept one positional argument `content` of an arbitrary type, which will\n        store the raw content of the document. If specified, the class will use\n        `content` to initialize the base llama_index class.\n\n    Args:\n        content: the raw content of the document.\n    \"\"\"\n\n    content: Any\n\n    def __init__(self, content: Optional[Any] = None, *args, **kwargs):\n        if content is None:\n            if kwargs.get(\"text\", None) is not None:\n                kwargs[\"content\"] = kwargs[\"text\"]\n            elif kwargs.get(\"embedding\", None) is not None:\n                kwargs[\"content\"] = kwargs[\"embedding\"]\n                # default text indicating this document only contains embedding\n                kwargs[\"text\"] = \"&lt;EMBEDDING&gt;\"\n        elif isinstance(content, Document):\n            kwargs = content.dict()\n        else:\n            kwargs[\"content\"] = content\n            if content:\n                kwargs[\"text\"] = str(content)\n            else:\n                kwargs[\"text\"] = \"\"\n        super().__init__(*args, **kwargs)\n\n    def __bool__(self):\n        return bool(self.content)\n\n    @classmethod\n    def example(cls) -&gt; \"Document\":\n        document = Document(\n            text=SAMPLE_TEXT,\n            metadata={\"filename\": \"README.md\", \"category\": \"codebase\"},\n        )\n        return document\n\n    def to_haystack_format(self) -&gt; \"HaystackDocument\":\n        \"\"\"Convert struct to Haystack document format.\"\"\"\n        from haystack.schema import Document as HaystackDocument\n\n        metadata = self.metadata or {}\n        text = self.text\n        return HaystackDocument(content=text, meta=metadata)\n\n    def __str__(self):\n        return str(self.content)\n</code></pre>"},{"location":"reference/base/#base.Document.to_haystack_format","title":"to_haystack_format","text":"<pre><code>to_haystack_format()\n</code></pre> <p>Convert struct to Haystack document format.</p> Source code in <code>kotaemon\\base\\schema.py</code> <pre><code>def to_haystack_format(self) -&gt; \"HaystackDocument\":\n    \"\"\"Convert struct to Haystack document format.\"\"\"\n    from haystack.schema import Document as HaystackDocument\n\n    metadata = self.metadata or {}\n    text = self.text\n    return HaystackDocument(content=text, meta=metadata)\n</code></pre>"},{"location":"reference/base/#base.DocumentWithEmbedding","title":"DocumentWithEmbedding","text":"<p>             Bases: <code>Document</code></p> <p>Subclass of Document which must contains embedding</p> <p>Use this if you want to enforce component's IOs to must contain embedding.</p> Source code in <code>kotaemon\\base\\schema.py</code> <pre><code>class DocumentWithEmbedding(Document):\n    \"\"\"Subclass of Document which must contains embedding\n\n    Use this if you want to enforce component's IOs to must contain embedding.\n    \"\"\"\n\n    def __init__(self, embedding: list[float], *args, **kwargs):\n        kwargs[\"embedding\"] = embedding\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/base/#base.ExtractorOutput","title":"ExtractorOutput","text":"<p>             Bases: <code>Document</code></p> <p>Represents the output of an extractor.</p> Source code in <code>kotaemon\\base\\schema.py</code> <pre><code>class ExtractorOutput(Document):\n    \"\"\"\n    Represents the output of an extractor.\n    \"\"\"\n\n    matches: list[str]\n</code></pre>"},{"location":"reference/base/#base.RetrievedDocument","title":"RetrievedDocument","text":"<p>             Bases: <code>Document</code></p> <p>Subclass of Document with retrieval-related information</p> <p>Attributes:</p> Name Type Description <code>score</code> <code>float</code> <p>score of the document (from 0.0 to 1.0)</p> <code>retrieval_metadata</code> <code>dict</code> <p>metadata from the retrieval process, can be used by different components in a retrieved pipeline to communicate with each other</p> Source code in <code>kotaemon\\base\\schema.py</code> <pre><code>class RetrievedDocument(Document):\n    \"\"\"Subclass of Document with retrieval-related information\n\n    Attributes:\n        score (float): score of the document (from 0.0 to 1.0)\n        retrieval_metadata (dict): metadata from the retrieval process, can be used\n            by different components in a retrieved pipeline to communicate with each\n            other\n    \"\"\"\n\n    score: float = Field(default=0.0)\n    retrieval_metadata: dict = Field(default={})\n</code></pre>"},{"location":"reference/base/component/","title":"Component","text":""},{"location":"reference/base/component/#base.component.BaseComponent","title":"BaseComponent","text":"<p>             Bases: <code>Function</code></p> <p>A component is a class that can be used to compose a pipeline.</p> <p>Benefits of component</p> <ul> <li>Auto caching, logging</li> <li>Allow deployment</li> </ul> <p>For each component, the spirit is</p> <ul> <li>Tolerate multiple input types, e.g. str, Document, List[str], List[Document]</li> <li>Enforce single output type. Hence, the output type of a component should be</li> </ul> <p>as generic as possible.</p> Source code in <code>kotaemon\\base\\component.py</code> <pre><code>class BaseComponent(Function):\n    \"\"\"A component is a class that can be used to compose a pipeline.\n\n    !!!tip \"Benefits of component\"\n        - Auto caching, logging\n        - Allow deployment\n\n    !!! tip \"For each component, the spirit is\"\n        - Tolerate multiple input types, e.g. str, Document, List[str], List[Document]\n        - Enforce single output type. Hence, the output type of a component should be\n    as generic as possible.\n    \"\"\"\n\n    inflow = None\n\n    def flow(self):\n        if self.inflow is None:\n            raise ValueError(\"No inflow provided.\")\n\n        if not isinstance(self.inflow, BaseComponent):\n            raise ValueError(\n                f\"inflow must be a BaseComponent, found {type(self.inflow)}\"\n            )\n\n        return self.__call__(self.inflow.flow())\n\n    @abstractmethod\n    def run(self, *args, **kwargs) -&gt; Document | list[Document] | None:\n        \"\"\"Run the component.\"\"\"\n        ...\n</code></pre>"},{"location":"reference/base/component/#base.component.BaseComponent.run","title":"run  <code>abstractmethod</code>","text":"<pre><code>run(*args, **kwargs)\n</code></pre> <p>Run the component.</p> Source code in <code>kotaemon\\base\\component.py</code> <pre><code>@abstractmethod\ndef run(self, *args, **kwargs) -&gt; Document | list[Document] | None:\n    \"\"\"Run the component.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/base/schema/","title":"Schema","text":""},{"location":"reference/base/schema/#base.schema.Document","title":"Document","text":"<p>             Bases: <code>Document</code></p> <p>Base document class, mostly inherited from Document class from llama-index.</p> <p>This class accept one positional argument <code>content</code> of an arbitrary type, which will     store the raw content of the document. If specified, the class will use     <code>content</code> to initialize the base llama_index class.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Optional[Any]</code> <p>the raw content of the document.</p> <code>None</code> Source code in <code>kotaemon\\base\\schema.py</code> <pre><code>class Document(BaseDocument):\n    \"\"\"\n    Base document class, mostly inherited from Document class from llama-index.\n\n    This class accept one positional argument `content` of an arbitrary type, which will\n        store the raw content of the document. If specified, the class will use\n        `content` to initialize the base llama_index class.\n\n    Args:\n        content: the raw content of the document.\n    \"\"\"\n\n    content: Any\n\n    def __init__(self, content: Optional[Any] = None, *args, **kwargs):\n        if content is None:\n            if kwargs.get(\"text\", None) is not None:\n                kwargs[\"content\"] = kwargs[\"text\"]\n            elif kwargs.get(\"embedding\", None) is not None:\n                kwargs[\"content\"] = kwargs[\"embedding\"]\n                # default text indicating this document only contains embedding\n                kwargs[\"text\"] = \"&lt;EMBEDDING&gt;\"\n        elif isinstance(content, Document):\n            kwargs = content.dict()\n        else:\n            kwargs[\"content\"] = content\n            if content:\n                kwargs[\"text\"] = str(content)\n            else:\n                kwargs[\"text\"] = \"\"\n        super().__init__(*args, **kwargs)\n\n    def __bool__(self):\n        return bool(self.content)\n\n    @classmethod\n    def example(cls) -&gt; \"Document\":\n        document = Document(\n            text=SAMPLE_TEXT,\n            metadata={\"filename\": \"README.md\", \"category\": \"codebase\"},\n        )\n        return document\n\n    def to_haystack_format(self) -&gt; \"HaystackDocument\":\n        \"\"\"Convert struct to Haystack document format.\"\"\"\n        from haystack.schema import Document as HaystackDocument\n\n        metadata = self.metadata or {}\n        text = self.text\n        return HaystackDocument(content=text, meta=metadata)\n\n    def __str__(self):\n        return str(self.content)\n</code></pre>"},{"location":"reference/base/schema/#base.schema.Document.to_haystack_format","title":"to_haystack_format","text":"<pre><code>to_haystack_format()\n</code></pre> <p>Convert struct to Haystack document format.</p> Source code in <code>kotaemon\\base\\schema.py</code> <pre><code>def to_haystack_format(self) -&gt; \"HaystackDocument\":\n    \"\"\"Convert struct to Haystack document format.\"\"\"\n    from haystack.schema import Document as HaystackDocument\n\n    metadata = self.metadata or {}\n    text = self.text\n    return HaystackDocument(content=text, meta=metadata)\n</code></pre>"},{"location":"reference/base/schema/#base.schema.DocumentWithEmbedding","title":"DocumentWithEmbedding","text":"<p>             Bases: <code>Document</code></p> <p>Subclass of Document which must contains embedding</p> <p>Use this if you want to enforce component's IOs to must contain embedding.</p> Source code in <code>kotaemon\\base\\schema.py</code> <pre><code>class DocumentWithEmbedding(Document):\n    \"\"\"Subclass of Document which must contains embedding\n\n    Use this if you want to enforce component's IOs to must contain embedding.\n    \"\"\"\n\n    def __init__(self, embedding: list[float], *args, **kwargs):\n        kwargs[\"embedding\"] = embedding\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/base/schema/#base.schema.RetrievedDocument","title":"RetrievedDocument","text":"<p>             Bases: <code>Document</code></p> <p>Subclass of Document with retrieval-related information</p> <p>Attributes:</p> Name Type Description <code>score</code> <code>float</code> <p>score of the document (from 0.0 to 1.0)</p> <code>retrieval_metadata</code> <code>dict</code> <p>metadata from the retrieval process, can be used by different components in a retrieved pipeline to communicate with each other</p> Source code in <code>kotaemon\\base\\schema.py</code> <pre><code>class RetrievedDocument(Document):\n    \"\"\"Subclass of Document with retrieval-related information\n\n    Attributes:\n        score (float): score of the document (from 0.0 to 1.0)\n        retrieval_metadata (dict): metadata from the retrieval process, can be used\n            by different components in a retrieved pipeline to communicate with each\n            other\n    \"\"\"\n\n    score: float = Field(default=0.0)\n    retrieval_metadata: dict = Field(default={})\n</code></pre>"},{"location":"reference/base/schema/#base.schema.ExtractorOutput","title":"ExtractorOutput","text":"<p>             Bases: <code>Document</code></p> <p>Represents the output of an extractor.</p> Source code in <code>kotaemon\\base\\schema.py</code> <pre><code>class ExtractorOutput(Document):\n    \"\"\"\n    Represents the output of an extractor.\n    \"\"\"\n\n    matches: list[str]\n</code></pre>"},{"location":"reference/chatbot/","title":"Chatbot","text":""},{"location":"reference/chatbot/#chatbot.ChatConversation","title":"ChatConversation","text":"<p>             Bases: <code>SessionFunction</code></p> <p>Base implementation of a chat bot component</p> A chatbot component should <ul> <li>handle internal state, including history messages</li> <li>return output for a given input</li> </ul> Source code in <code>kotaemon\\chatbot\\base.py</code> <pre><code>class ChatConversation(SessionFunction):\n    \"\"\"Base implementation of a chat bot component\n\n    A chatbot component should:\n        - handle internal state, including history messages\n        - return output for a given input\n    \"\"\"\n\n    class Config:\n        store_result = session_chat_storage\n\n    system_message: str = \"\"\n    bot: BaseChatBot\n\n    def __init__(self, *args, **kwargs):\n        self._history: List[BaseMessage] = []\n        self._store_result = (\n            f\"{self.__module__}.{self.__class__.__name__},uninitiated_bot\"\n        )\n        super().__init__(*args, **kwargs)\n\n    def run(self, message: HumanMessage) -&gt; Optional[BaseMessage]:\n        \"\"\"Chat, given a message, return a response\n\n        Args:\n            message: The message to respond to\n\n        Returns:\n            The response to the message. If None, no response is sent.\n        \"\"\"\n        user_message = (\n            HumanMessage(content=message) if isinstance(message, str) else message\n        )\n        self.history.append(user_message)\n\n        output = self.bot(self.history).text\n        output_message = None\n        if output is not None:\n            output_message = AIMessage(content=output)\n            self.history.append(output_message)\n\n        return output_message\n\n    def start_session(self):\n        self._store_result = self.bot.config.store_result\n        super().start_session()\n        if not self.history and self.system_message:\n            system_message = SystemMessage(content=self.system_message)\n            self.history.append(system_message)\n\n    def end_session(self):\n        super().end_session()\n        self._history = []\n\n    def check_end(\n        self,\n        history: Optional[List[BaseMessage]] = None,\n        user_message: Optional[HumanMessage] = None,\n        bot_message: Optional[AIMessage] = None,\n    ) -&gt; bool:\n        \"\"\"Check if a conversation should end\"\"\"\n        if user_message is not None and user_message.content == \"\":\n            return True\n\n        return False\n\n    def terminal_session(self):\n        \"\"\"Create a terminal session\"\"\"\n        self.start_session()\n        print(\"&gt;&gt; Start chat:\")\n\n        while True:\n            human = HumanMessage(content=input(\"Human: \"))\n            if self.check_end(history=self.history, user_message=human):\n                break\n\n            output = self(human)\n            if output is None:\n                print(\"AI: &lt;No response&gt;\")\n            else:\n                print(\"AI:\", output.content)\n\n            if self.check_end(history=self.history, bot_message=output):\n                break\n\n        self.end_session()\n\n    @property\n    def history(self):\n        return self._history\n\n    @history.setter\n    def history(self, value):\n        self._history = value\n        self._variablex()\n</code></pre>"},{"location":"reference/chatbot/#chatbot.ChatConversation.run","title":"run","text":"<pre><code>run(message)\n</code></pre> <p>Chat, given a message, return a response</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>HumanMessage</code> <p>The message to respond to</p> required <p>Returns:</p> Type Description <code>Optional[BaseMessage]</code> <p>The response to the message. If None, no response is sent.</p> Source code in <code>kotaemon\\chatbot\\base.py</code> <pre><code>def run(self, message: HumanMessage) -&gt; Optional[BaseMessage]:\n    \"\"\"Chat, given a message, return a response\n\n    Args:\n        message: The message to respond to\n\n    Returns:\n        The response to the message. If None, no response is sent.\n    \"\"\"\n    user_message = (\n        HumanMessage(content=message) if isinstance(message, str) else message\n    )\n    self.history.append(user_message)\n\n    output = self.bot(self.history).text\n    output_message = None\n    if output is not None:\n        output_message = AIMessage(content=output)\n        self.history.append(output_message)\n\n    return output_message\n</code></pre>"},{"location":"reference/chatbot/#chatbot.ChatConversation.check_end","title":"check_end","text":"<pre><code>check_end(history=None, user_message=None, bot_message=None)\n</code></pre> <p>Check if a conversation should end</p> Source code in <code>kotaemon\\chatbot\\base.py</code> <pre><code>def check_end(\n    self,\n    history: Optional[List[BaseMessage]] = None,\n    user_message: Optional[HumanMessage] = None,\n    bot_message: Optional[AIMessage] = None,\n) -&gt; bool:\n    \"\"\"Check if a conversation should end\"\"\"\n    if user_message is not None and user_message.content == \"\":\n        return True\n\n    return False\n</code></pre>"},{"location":"reference/chatbot/#chatbot.ChatConversation.terminal_session","title":"terminal_session","text":"<pre><code>terminal_session()\n</code></pre> <p>Create a terminal session</p> Source code in <code>kotaemon\\chatbot\\base.py</code> <pre><code>def terminal_session(self):\n    \"\"\"Create a terminal session\"\"\"\n    self.start_session()\n    print(\"&gt;&gt; Start chat:\")\n\n    while True:\n        human = HumanMessage(content=input(\"Human: \"))\n        if self.check_end(history=self.history, user_message=human):\n            break\n\n        output = self(human)\n        if output is None:\n            print(\"AI: &lt;No response&gt;\")\n        else:\n            print(\"AI:\", output.content)\n\n        if self.check_end(history=self.history, bot_message=output):\n            break\n\n    self.end_session()\n</code></pre>"},{"location":"reference/chatbot/#chatbot.SimpleRespondentChatbot","title":"SimpleRespondentChatbot","text":"<p>             Bases: <code>BaseChatBot</code></p> <p>Simple text respondent chatbot that essentially wraps around a chat LLM</p> Source code in <code>kotaemon\\chatbot\\simple_respondent.py</code> <pre><code>class SimpleRespondentChatbot(BaseChatBot):\n    \"\"\"Simple text respondent chatbot that essentially wraps around a chat LLM\"\"\"\n\n    llm: ChatLLM\n\n    def _get_message(self) -&gt; str:\n        return self.llm(self.history).text\n</code></pre>"},{"location":"reference/chatbot/base/","title":"Base","text":""},{"location":"reference/chatbot/base/#chatbot.base.ChatConversation","title":"ChatConversation","text":"<p>             Bases: <code>SessionFunction</code></p> <p>Base implementation of a chat bot component</p> A chatbot component should <ul> <li>handle internal state, including history messages</li> <li>return output for a given input</li> </ul> Source code in <code>kotaemon\\chatbot\\base.py</code> <pre><code>class ChatConversation(SessionFunction):\n    \"\"\"Base implementation of a chat bot component\n\n    A chatbot component should:\n        - handle internal state, including history messages\n        - return output for a given input\n    \"\"\"\n\n    class Config:\n        store_result = session_chat_storage\n\n    system_message: str = \"\"\n    bot: BaseChatBot\n\n    def __init__(self, *args, **kwargs):\n        self._history: List[BaseMessage] = []\n        self._store_result = (\n            f\"{self.__module__}.{self.__class__.__name__},uninitiated_bot\"\n        )\n        super().__init__(*args, **kwargs)\n\n    def run(self, message: HumanMessage) -&gt; Optional[BaseMessage]:\n        \"\"\"Chat, given a message, return a response\n\n        Args:\n            message: The message to respond to\n\n        Returns:\n            The response to the message. If None, no response is sent.\n        \"\"\"\n        user_message = (\n            HumanMessage(content=message) if isinstance(message, str) else message\n        )\n        self.history.append(user_message)\n\n        output = self.bot(self.history).text\n        output_message = None\n        if output is not None:\n            output_message = AIMessage(content=output)\n            self.history.append(output_message)\n\n        return output_message\n\n    def start_session(self):\n        self._store_result = self.bot.config.store_result\n        super().start_session()\n        if not self.history and self.system_message:\n            system_message = SystemMessage(content=self.system_message)\n            self.history.append(system_message)\n\n    def end_session(self):\n        super().end_session()\n        self._history = []\n\n    def check_end(\n        self,\n        history: Optional[List[BaseMessage]] = None,\n        user_message: Optional[HumanMessage] = None,\n        bot_message: Optional[AIMessage] = None,\n    ) -&gt; bool:\n        \"\"\"Check if a conversation should end\"\"\"\n        if user_message is not None and user_message.content == \"\":\n            return True\n\n        return False\n\n    def terminal_session(self):\n        \"\"\"Create a terminal session\"\"\"\n        self.start_session()\n        print(\"&gt;&gt; Start chat:\")\n\n        while True:\n            human = HumanMessage(content=input(\"Human: \"))\n            if self.check_end(history=self.history, user_message=human):\n                break\n\n            output = self(human)\n            if output is None:\n                print(\"AI: &lt;No response&gt;\")\n            else:\n                print(\"AI:\", output.content)\n\n            if self.check_end(history=self.history, bot_message=output):\n                break\n\n        self.end_session()\n\n    @property\n    def history(self):\n        return self._history\n\n    @history.setter\n    def history(self, value):\n        self._history = value\n        self._variablex()\n</code></pre>"},{"location":"reference/chatbot/base/#chatbot.base.ChatConversation.run","title":"run","text":"<pre><code>run(message)\n</code></pre> <p>Chat, given a message, return a response</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>HumanMessage</code> <p>The message to respond to</p> required <p>Returns:</p> Type Description <code>Optional[BaseMessage]</code> <p>The response to the message. If None, no response is sent.</p> Source code in <code>kotaemon\\chatbot\\base.py</code> <pre><code>def run(self, message: HumanMessage) -&gt; Optional[BaseMessage]:\n    \"\"\"Chat, given a message, return a response\n\n    Args:\n        message: The message to respond to\n\n    Returns:\n        The response to the message. If None, no response is sent.\n    \"\"\"\n    user_message = (\n        HumanMessage(content=message) if isinstance(message, str) else message\n    )\n    self.history.append(user_message)\n\n    output = self.bot(self.history).text\n    output_message = None\n    if output is not None:\n        output_message = AIMessage(content=output)\n        self.history.append(output_message)\n\n    return output_message\n</code></pre>"},{"location":"reference/chatbot/base/#chatbot.base.ChatConversation.check_end","title":"check_end","text":"<pre><code>check_end(history=None, user_message=None, bot_message=None)\n</code></pre> <p>Check if a conversation should end</p> Source code in <code>kotaemon\\chatbot\\base.py</code> <pre><code>def check_end(\n    self,\n    history: Optional[List[BaseMessage]] = None,\n    user_message: Optional[HumanMessage] = None,\n    bot_message: Optional[AIMessage] = None,\n) -&gt; bool:\n    \"\"\"Check if a conversation should end\"\"\"\n    if user_message is not None and user_message.content == \"\":\n        return True\n\n    return False\n</code></pre>"},{"location":"reference/chatbot/base/#chatbot.base.ChatConversation.terminal_session","title":"terminal_session","text":"<pre><code>terminal_session()\n</code></pre> <p>Create a terminal session</p> Source code in <code>kotaemon\\chatbot\\base.py</code> <pre><code>def terminal_session(self):\n    \"\"\"Create a terminal session\"\"\"\n    self.start_session()\n    print(\"&gt;&gt; Start chat:\")\n\n    while True:\n        human = HumanMessage(content=input(\"Human: \"))\n        if self.check_end(history=self.history, user_message=human):\n            break\n\n        output = self(human)\n        if output is None:\n            print(\"AI: &lt;No response&gt;\")\n        else:\n            print(\"AI:\", output.content)\n\n        if self.check_end(history=self.history, bot_message=output):\n            break\n\n    self.end_session()\n</code></pre>"},{"location":"reference/chatbot/base/#chatbot.base.session_chat_storage","title":"session_chat_storage","text":"<pre><code>session_chat_storage(obj)\n</code></pre> <p>Store using the bot location rather than the session location</p> Source code in <code>kotaemon\\chatbot\\base.py</code> <pre><code>def session_chat_storage(obj):\n    \"\"\"Store using the bot location rather than the session location\"\"\"\n    return obj._store_result\n</code></pre>"},{"location":"reference/chatbot/simple_respondent/","title":"Simple Respondent","text":""},{"location":"reference/chatbot/simple_respondent/#chatbot.simple_respondent.SimpleRespondentChatbot","title":"SimpleRespondentChatbot","text":"<p>             Bases: <code>BaseChatBot</code></p> <p>Simple text respondent chatbot that essentially wraps around a chat LLM</p> Source code in <code>kotaemon\\chatbot\\simple_respondent.py</code> <pre><code>class SimpleRespondentChatbot(BaseChatBot):\n    \"\"\"Simple text respondent chatbot that essentially wraps around a chat LLM\"\"\"\n\n    llm: ChatLLM\n\n    def _get_message(self) -&gt; str:\n        return self.llm(self.history).text\n</code></pre>"},{"location":"reference/embeddings/","title":"Embeddings","text":""},{"location":"reference/embeddings/#embeddings.AzureOpenAIEmbeddings","title":"AzureOpenAIEmbeddings","text":"<p>             Bases: <code>LCEmbeddingMixin</code>, <code>BaseEmbeddings</code></p> <p>Wrapper around Langchain's AzureOpenAI embedding, focusing on key parameters</p> Source code in <code>kotaemon\\embeddings\\langchain_based.py</code> <pre><code>class AzureOpenAIEmbeddings(LCEmbeddingMixin, BaseEmbeddings):\n    \"\"\"Wrapper around Langchain's AzureOpenAI embedding, focusing on key parameters\"\"\"\n\n    def __init__(\n        self,\n        azure_endpoint: Optional[str] = None,\n        deployment: Optional[str] = None,\n        openai_api_key: Optional[str] = None,\n        openai_api_version: Optional[str] = None,\n        request_timeout: Optional[float] = None,\n        **params,\n    ):\n        super().__init__(\n            azure_endpoint=azure_endpoint,\n            deployment=deployment,\n            openai_api_version=openai_api_version,\n            openai_api_key=openai_api_key,\n            request_timeout=request_timeout,\n            **params,\n        )\n\n    def _get_lc_class(self):\n        import langchain.embeddings\n\n        return langchain.embeddings.AzureOpenAIEmbeddings\n</code></pre>"},{"location":"reference/embeddings/#embeddings.CohereEmbdeddings","title":"CohereEmbdeddings","text":"<p>             Bases: <code>LCEmbeddingMixin</code>, <code>BaseEmbeddings</code></p> <p>Wrapper around Langchain's Cohere embedding, focusing on key parameters</p> Source code in <code>kotaemon\\embeddings\\langchain_based.py</code> <pre><code>class CohereEmbdeddings(LCEmbeddingMixin, BaseEmbeddings):\n    \"\"\"Wrapper around Langchain's Cohere embedding, focusing on key parameters\"\"\"\n\n    def __init__(\n        self,\n        model: str = \"embed-english-v2.0\",\n        cohere_api_key: Optional[str] = None,\n        truncate: Optional[str] = None,\n        request_timeout: Optional[float] = None,\n        **params,\n    ):\n        super().__init__(\n            model=model,\n            cohere_api_key=cohere_api_key,\n            truncate=truncate,\n            request_timeout=request_timeout,\n            **params,\n        )\n\n    def _get_lc_class(self):\n        import langchain.embeddings\n\n        return langchain.embeddings.CohereEmbeddings\n</code></pre>"},{"location":"reference/embeddings/#embeddings.HuggingFaceEmbeddings","title":"HuggingFaceEmbeddings","text":"<p>             Bases: <code>LCEmbeddingMixin</code>, <code>BaseEmbeddings</code></p> <p>Wrapper around Langchain's HuggingFace embedding, focusing on key parameters</p> Source code in <code>kotaemon\\embeddings\\langchain_based.py</code> <pre><code>class HuggingFaceEmbeddings(LCEmbeddingMixin, BaseEmbeddings):\n    \"\"\"Wrapper around Langchain's HuggingFace embedding, focusing on key parameters\"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\n        **params,\n    ):\n        super().__init__(\n            model_name=model_name,\n            **params,\n        )\n\n    def _get_lc_class(self):\n        import langchain.embeddings\n\n        return langchain.embeddings.HuggingFaceBgeEmbeddings\n</code></pre>"},{"location":"reference/embeddings/#embeddings.OpenAIEmbeddings","title":"OpenAIEmbeddings","text":"<p>             Bases: <code>LCEmbeddingMixin</code>, <code>BaseEmbeddings</code></p> <p>Wrapper around Langchain's OpenAI embedding, focusing on key parameters</p> Source code in <code>kotaemon\\embeddings\\langchain_based.py</code> <pre><code>class OpenAIEmbeddings(LCEmbeddingMixin, BaseEmbeddings):\n    \"\"\"Wrapper around Langchain's OpenAI embedding, focusing on key parameters\"\"\"\n\n    def __init__(\n        self,\n        model: str = \"text-embedding-ada-002\",\n        openai_api_version: Optional[str] = None,\n        openai_api_base: Optional[str] = None,\n        openai_api_type: Optional[str] = None,\n        openai_api_key: Optional[str] = None,\n        request_timeout: Optional[float] = None,\n        **params,\n    ):\n        super().__init__(\n            model=model,\n            openai_api_version=openai_api_version,\n            openai_api_base=openai_api_base,\n            openai_api_type=openai_api_type,\n            openai_api_key=openai_api_key,\n            request_timeout=request_timeout,\n            **params,\n        )\n\n    def _get_lc_class(self):\n        import langchain.embeddings\n\n        return langchain.emebddings.OpenAIEmbeddings\n</code></pre>"},{"location":"reference/embeddings/base/","title":"Base","text":""},{"location":"reference/embeddings/langchain_based/","title":"Langchain Based","text":""},{"location":"reference/embeddings/langchain_based/#embeddings.langchain_based.OpenAIEmbeddings","title":"OpenAIEmbeddings","text":"<p>             Bases: <code>LCEmbeddingMixin</code>, <code>BaseEmbeddings</code></p> <p>Wrapper around Langchain's OpenAI embedding, focusing on key parameters</p> Source code in <code>kotaemon\\embeddings\\langchain_based.py</code> <pre><code>class OpenAIEmbeddings(LCEmbeddingMixin, BaseEmbeddings):\n    \"\"\"Wrapper around Langchain's OpenAI embedding, focusing on key parameters\"\"\"\n\n    def __init__(\n        self,\n        model: str = \"text-embedding-ada-002\",\n        openai_api_version: Optional[str] = None,\n        openai_api_base: Optional[str] = None,\n        openai_api_type: Optional[str] = None,\n        openai_api_key: Optional[str] = None,\n        request_timeout: Optional[float] = None,\n        **params,\n    ):\n        super().__init__(\n            model=model,\n            openai_api_version=openai_api_version,\n            openai_api_base=openai_api_base,\n            openai_api_type=openai_api_type,\n            openai_api_key=openai_api_key,\n            request_timeout=request_timeout,\n            **params,\n        )\n\n    def _get_lc_class(self):\n        import langchain.embeddings\n\n        return langchain.emebddings.OpenAIEmbeddings\n</code></pre>"},{"location":"reference/embeddings/langchain_based/#embeddings.langchain_based.AzureOpenAIEmbeddings","title":"AzureOpenAIEmbeddings","text":"<p>             Bases: <code>LCEmbeddingMixin</code>, <code>BaseEmbeddings</code></p> <p>Wrapper around Langchain's AzureOpenAI embedding, focusing on key parameters</p> Source code in <code>kotaemon\\embeddings\\langchain_based.py</code> <pre><code>class AzureOpenAIEmbeddings(LCEmbeddingMixin, BaseEmbeddings):\n    \"\"\"Wrapper around Langchain's AzureOpenAI embedding, focusing on key parameters\"\"\"\n\n    def __init__(\n        self,\n        azure_endpoint: Optional[str] = None,\n        deployment: Optional[str] = None,\n        openai_api_key: Optional[str] = None,\n        openai_api_version: Optional[str] = None,\n        request_timeout: Optional[float] = None,\n        **params,\n    ):\n        super().__init__(\n            azure_endpoint=azure_endpoint,\n            deployment=deployment,\n            openai_api_version=openai_api_version,\n            openai_api_key=openai_api_key,\n            request_timeout=request_timeout,\n            **params,\n        )\n\n    def _get_lc_class(self):\n        import langchain.embeddings\n\n        return langchain.embeddings.AzureOpenAIEmbeddings\n</code></pre>"},{"location":"reference/embeddings/langchain_based/#embeddings.langchain_based.CohereEmbdeddings","title":"CohereEmbdeddings","text":"<p>             Bases: <code>LCEmbeddingMixin</code>, <code>BaseEmbeddings</code></p> <p>Wrapper around Langchain's Cohere embedding, focusing on key parameters</p> Source code in <code>kotaemon\\embeddings\\langchain_based.py</code> <pre><code>class CohereEmbdeddings(LCEmbeddingMixin, BaseEmbeddings):\n    \"\"\"Wrapper around Langchain's Cohere embedding, focusing on key parameters\"\"\"\n\n    def __init__(\n        self,\n        model: str = \"embed-english-v2.0\",\n        cohere_api_key: Optional[str] = None,\n        truncate: Optional[str] = None,\n        request_timeout: Optional[float] = None,\n        **params,\n    ):\n        super().__init__(\n            model=model,\n            cohere_api_key=cohere_api_key,\n            truncate=truncate,\n            request_timeout=request_timeout,\n            **params,\n        )\n\n    def _get_lc_class(self):\n        import langchain.embeddings\n\n        return langchain.embeddings.CohereEmbeddings\n</code></pre>"},{"location":"reference/embeddings/langchain_based/#embeddings.langchain_based.HuggingFaceEmbeddings","title":"HuggingFaceEmbeddings","text":"<p>             Bases: <code>LCEmbeddingMixin</code>, <code>BaseEmbeddings</code></p> <p>Wrapper around Langchain's HuggingFace embedding, focusing on key parameters</p> Source code in <code>kotaemon\\embeddings\\langchain_based.py</code> <pre><code>class HuggingFaceEmbeddings(LCEmbeddingMixin, BaseEmbeddings):\n    \"\"\"Wrapper around Langchain's HuggingFace embedding, focusing on key parameters\"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\n        **params,\n    ):\n        super().__init__(\n            model_name=model_name,\n            **params,\n        )\n\n    def _get_lc_class(self):\n        import langchain.embeddings\n\n        return langchain.embeddings.HuggingFaceBgeEmbeddings\n</code></pre>"},{"location":"reference/indices/","title":"Indices","text":""},{"location":"reference/indices/#indices.VectorIndexing","title":"VectorIndexing","text":"<p>             Bases: <code>BaseIndexing</code></p> <p>Ingest the document, run through the embedding, and store the embedding in a vector store.</p> This pipeline supports the following set of inputs <ul> <li>List of documents</li> <li>List of texts</li> </ul> Source code in <code>kotaemon\\indices\\vectorindex.py</code> <pre><code>class VectorIndexing(BaseIndexing):\n    \"\"\"Ingest the document, run through the embedding, and store the embedding in a\n    vector store.\n\n    This pipeline supports the following set of inputs:\n        - List of documents\n        - List of texts\n    \"\"\"\n\n    vector_store: BaseVectorStore\n    doc_store: Optional[BaseDocumentStore] = None\n    embedding: BaseEmbeddings\n\n    def to_retrieval_pipeline(self, *args, **kwargs):\n        \"\"\"Convert the indexing pipeline to a retrieval pipeline\"\"\"\n        return VectorRetrieval(\n            vector_store=self.vector_store,\n            doc_store=self.doc_store,\n            embedding=self.embedding,\n            **kwargs,\n        )\n\n    def to_qa_pipeline(self, *args, **kwargs):\n        from .qa import CitationQAPipeline\n\n        return TextVectorQA(\n            retrieving_pipeline=self.to_retrieval_pipeline(**kwargs),\n            qa_pipeline=CitationQAPipeline(**kwargs),\n        )\n\n    def run(self, text: str | list[str] | Document | list[Document]):\n        input_: list[Document] = []\n        if not isinstance(text, list):\n            text = [text]\n\n        for item in cast(list, text):\n            if isinstance(item, str):\n                input_.append(Document(text=item, id_=str(uuid.uuid4())))\n            elif isinstance(item, Document):\n                input_.append(item)\n            else:\n                raise ValueError(\n                    f\"Invalid input type {type(item)}, should be str or Document\"\n                )\n\n        embeddings = self.embedding(input_)\n        self.vector_store.add(\n            embeddings=embeddings,\n            ids=[t.id_ for t in input_],\n        )\n        if self.doc_store:\n            self.doc_store.add(input_)\n</code></pre>"},{"location":"reference/indices/#indices.VectorIndexing.to_retrieval_pipeline","title":"to_retrieval_pipeline","text":"<pre><code>to_retrieval_pipeline(*args, **kwargs)\n</code></pre> <p>Convert the indexing pipeline to a retrieval pipeline</p> Source code in <code>kotaemon\\indices\\vectorindex.py</code> <pre><code>def to_retrieval_pipeline(self, *args, **kwargs):\n    \"\"\"Convert the indexing pipeline to a retrieval pipeline\"\"\"\n    return VectorRetrieval(\n        vector_store=self.vector_store,\n        doc_store=self.doc_store,\n        embedding=self.embedding,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/indices/#indices.VectorRetrieval","title":"VectorRetrieval","text":"<p>             Bases: <code>BaseRetrieval</code></p> <p>Retrieve list of documents from vector store</p> Source code in <code>kotaemon\\indices\\vectorindex.py</code> <pre><code>class VectorRetrieval(BaseRetrieval):\n    \"\"\"Retrieve list of documents from vector store\"\"\"\n\n    vector_store: BaseVectorStore\n    doc_store: Optional[BaseDocumentStore] = None\n    embedding: BaseEmbeddings\n    rerankers: Sequence[BaseReranking] = []\n    top_k: int = 1\n\n    def run(\n        self, text: str | Document, top_k: Optional[int] = None, **kwargs\n    ) -&gt; list[RetrievedDocument]:\n        \"\"\"Retrieve a list of documents from vector store\n\n        Args:\n            text: the text to retrieve similar documents\n            top_k: number of top similar documents to return\n\n        Returns:\n            list[RetrievedDocument]: list of retrieved documents\n        \"\"\"\n        if top_k is None:\n            top_k = self.top_k\n\n        if self.doc_store is None:\n            raise ValueError(\n                \"doc_store is not provided. Please provide a doc_store to \"\n                \"retrieve the documents\"\n            )\n\n        emb: list[float] = self.embedding(text)[0].embedding\n        _, scores, ids = self.vector_store.query(embedding=emb, top_k=top_k)\n        docs = self.doc_store.get(ids)\n        result = [\n            RetrievedDocument(**doc.to_dict(), score=score)\n            for doc, score in zip(docs, scores)\n        ]\n        # use additional reranker to re-order the document list\n        if self.rerankers:\n            for reranker in self.rerankers:\n                result = reranker(documents=result, query=text)\n\n        return result\n</code></pre>"},{"location":"reference/indices/#indices.VectorRetrieval.run","title":"run","text":"<pre><code>run(text, top_k=None, **kwargs)\n</code></pre> <p>Retrieve a list of documents from vector store</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | Document</code> <p>the text to retrieve similar documents</p> required <code>top_k</code> <code>Optional[int]</code> <p>number of top similar documents to return</p> <code>None</code> <p>Returns:</p> Type Description <code>list[RetrievedDocument]</code> <p>list[RetrievedDocument]: list of retrieved documents</p> Source code in <code>kotaemon\\indices\\vectorindex.py</code> <pre><code>def run(\n    self, text: str | Document, top_k: Optional[int] = None, **kwargs\n) -&gt; list[RetrievedDocument]:\n    \"\"\"Retrieve a list of documents from vector store\n\n    Args:\n        text: the text to retrieve similar documents\n        top_k: number of top similar documents to return\n\n    Returns:\n        list[RetrievedDocument]: list of retrieved documents\n    \"\"\"\n    if top_k is None:\n        top_k = self.top_k\n\n    if self.doc_store is None:\n        raise ValueError(\n            \"doc_store is not provided. Please provide a doc_store to \"\n            \"retrieve the documents\"\n        )\n\n    emb: list[float] = self.embedding(text)[0].embedding\n    _, scores, ids = self.vector_store.query(embedding=emb, top_k=top_k)\n    docs = self.doc_store.get(ids)\n    result = [\n        RetrievedDocument(**doc.to_dict(), score=score)\n        for doc, score in zip(docs, scores)\n    ]\n    # use additional reranker to re-order the document list\n    if self.rerankers:\n        for reranker in self.rerankers:\n            result = reranker(documents=result, query=text)\n\n    return result\n</code></pre>"},{"location":"reference/indices/base/","title":"Base","text":""},{"location":"reference/indices/base/#indices.base.DocTransformer","title":"DocTransformer","text":"<p>             Bases: <code>BaseComponent</code></p> <p>This is a base class for document transformers</p> <p>A document transformer transforms a list of documents into another list of documents. Transforming can mean splitting a document into multiple documents, reducing a large list of documents into a smaller list of documents, or adding metadata to each document in a list of documents, etc.</p> Source code in <code>kotaemon\\indices\\base.py</code> <pre><code>class DocTransformer(BaseComponent):\n    \"\"\"This is a base class for document transformers\n\n    A document transformer transforms a list of documents into another list\n    of documents. Transforming can mean splitting a document into multiple documents,\n    reducing a large list of documents into a smaller list of documents, or adding\n    metadata to each document in a list of documents, etc.\n    \"\"\"\n\n    @abstractmethod\n    def run(\n        self,\n        documents: list[Document],\n        **kwargs,\n    ) -&gt; list[Document]:\n        ...\n</code></pre>"},{"location":"reference/indices/base/#indices.base.LlamaIndexDocTransformerMixin","title":"LlamaIndexDocTransformerMixin","text":"<p>Allow automatically wrapping a Llama-index component into kotaemon component</p> Example <p>class TokenSplitter(LlamaIndexMixin, BaseSplitter):     def _get_li_class(self):         from llama_index.text_splitter import TokenTextSplitter         return TokenTextSplitter</p> <p>To use this mixin, please:     1. Use this class as the 1st parent class, so that Python will prefer to use     the attributes and methods of this class whenever possible.     2. Overwrite <code>_get_li_class</code> to return the relevant LlamaIndex component.</p> Source code in <code>kotaemon\\indices\\base.py</code> <pre><code>class LlamaIndexDocTransformerMixin:\n    \"\"\"Allow automatically wrapping a Llama-index component into kotaemon component\n\n    Example:\n        class TokenSplitter(LlamaIndexMixin, BaseSplitter):\n            def _get_li_class(self):\n                from llama_index.text_splitter import TokenTextSplitter\n                return TokenTextSplitter\n\n    To use this mixin, please:\n        1. Use this class as the 1st parent class, so that Python will prefer to use\n        the attributes and methods of this class whenever possible.\n        2. Overwrite `_get_li_class` to return the relevant LlamaIndex component.\n    \"\"\"\n\n    def _get_li_class(self) -&gt; Type[NodeParser]:\n        raise NotImplementedError(\n            \"Please return the relevant LlamaIndex class in _get_li_class\"\n        )\n\n    def __init__(self, **params):\n        self._li_cls = self._get_li_class()\n        self._obj = self._li_cls(**params)\n        self._kwargs = params\n        super().__init__()\n\n    def __repr__(self):\n        kwargs = []\n        for key, value_obj in self._kwargs.items():\n            value = repr(value_obj)\n            kwargs.append(f\"{key}={value}\")\n        kwargs_repr = \", \".join(kwargs)\n        return f\"{self.__class__.__name__}({kwargs_repr})\"\n\n    def __str__(self):\n        kwargs = []\n        for key, value_obj in self._kwargs.items():\n            value = str(value_obj)\n            if len(value) &gt; 20:\n                value = f\"{value[:15]}...\"\n            kwargs.append(f\"{key}={value}\")\n        kwargs_repr = \", \".join(kwargs)\n        return f\"{self.__class__.__name__}({kwargs_repr})\"\n\n    def __setattr__(self, name: str, value: Any) -&gt; None:\n        if name.startswith(\"_\") or name in self._protected_keywords():\n            return super().__setattr__(name, value)\n\n        self._kwargs[name] = value\n        return setattr(self._obj, name, value)\n\n    def __getattr__(self, name: str) -&gt; Any:\n        if name in self._kwargs:\n            return self._kwargs[name]\n        return getattr(self._obj, name)\n\n    def dump(self, *args, **kwargs):\n        from theflow.utils.modules import serialize\n\n        params = {key: serialize(value) for key, value in self._kwargs.items()}\n        return {\n            \"__type__\": f\"{self.__module__}.{self.__class__.__qualname__}\",\n            **params,\n        }\n\n    def run(\n        self,\n        documents: list[Document],\n        **kwargs,\n    ) -&gt; list[Document]:\n        \"\"\"Run Llama-index node parser and convert the output to Document from\n        kotaemon\n        \"\"\"\n        docs = self._obj(documents, **kwargs)  # type: ignore\n        return [Document.from_dict(doc.to_dict()) for doc in docs]\n</code></pre>"},{"location":"reference/indices/base/#indices.base.LlamaIndexDocTransformerMixin.run","title":"run","text":"<pre><code>run(documents, **kwargs)\n</code></pre> <p>Run Llama-index node parser and convert the output to Document from kotaemon</p> Source code in <code>kotaemon\\indices\\base.py</code> <pre><code>def run(\n    self,\n    documents: list[Document],\n    **kwargs,\n) -&gt; list[Document]:\n    \"\"\"Run Llama-index node parser and convert the output to Document from\n    kotaemon\n    \"\"\"\n    docs = self._obj(documents, **kwargs)  # type: ignore\n    return [Document.from_dict(doc.to_dict()) for doc in docs]\n</code></pre>"},{"location":"reference/indices/base/#indices.base.BaseIndexing","title":"BaseIndexing","text":"<p>             Bases: <code>BaseComponent</code></p> <p>Define the base interface for indexing pipeline</p> Source code in <code>kotaemon\\indices\\base.py</code> <pre><code>class BaseIndexing(BaseComponent):\n    \"\"\"Define the base interface for indexing pipeline\"\"\"\n\n    def to_retrieval_pipeline(self, **kwargs):\n        \"\"\"Convert the indexing pipeline to a retrieval pipeline\"\"\"\n        raise NotImplementedError\n\n    def to_qa_pipeline(self, **kwargs):\n        \"\"\"Convert the indexing pipeline to a QA pipeline\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/indices/base/#indices.base.BaseIndexing.to_retrieval_pipeline","title":"to_retrieval_pipeline","text":"<pre><code>to_retrieval_pipeline(**kwargs)\n</code></pre> <p>Convert the indexing pipeline to a retrieval pipeline</p> Source code in <code>kotaemon\\indices\\base.py</code> <pre><code>def to_retrieval_pipeline(self, **kwargs):\n    \"\"\"Convert the indexing pipeline to a retrieval pipeline\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/indices/base/#indices.base.BaseIndexing.to_qa_pipeline","title":"to_qa_pipeline","text":"<pre><code>to_qa_pipeline(**kwargs)\n</code></pre> <p>Convert the indexing pipeline to a QA pipeline</p> Source code in <code>kotaemon\\indices\\base.py</code> <pre><code>def to_qa_pipeline(self, **kwargs):\n    \"\"\"Convert the indexing pipeline to a QA pipeline\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/indices/base/#indices.base.BaseRetrieval","title":"BaseRetrieval","text":"<p>             Bases: <code>BaseComponent</code></p> <p>Define the base interface for retrieval pipeline</p> Source code in <code>kotaemon\\indices\\base.py</code> <pre><code>class BaseRetrieval(BaseComponent):\n    \"\"\"Define the base interface for retrieval pipeline\"\"\"\n\n    @abstractmethod\n    def run(self, *args, **kwargs) -&gt; list[RetrievedDocument]:\n        ...\n</code></pre>"},{"location":"reference/indices/vectorindex/","title":"Vectorindex","text":""},{"location":"reference/indices/vectorindex/#indices.vectorindex.VectorIndexing","title":"VectorIndexing","text":"<p>             Bases: <code>BaseIndexing</code></p> <p>Ingest the document, run through the embedding, and store the embedding in a vector store.</p> This pipeline supports the following set of inputs <ul> <li>List of documents</li> <li>List of texts</li> </ul> Source code in <code>kotaemon\\indices\\vectorindex.py</code> <pre><code>class VectorIndexing(BaseIndexing):\n    \"\"\"Ingest the document, run through the embedding, and store the embedding in a\n    vector store.\n\n    This pipeline supports the following set of inputs:\n        - List of documents\n        - List of texts\n    \"\"\"\n\n    vector_store: BaseVectorStore\n    doc_store: Optional[BaseDocumentStore] = None\n    embedding: BaseEmbeddings\n\n    def to_retrieval_pipeline(self, *args, **kwargs):\n        \"\"\"Convert the indexing pipeline to a retrieval pipeline\"\"\"\n        return VectorRetrieval(\n            vector_store=self.vector_store,\n            doc_store=self.doc_store,\n            embedding=self.embedding,\n            **kwargs,\n        )\n\n    def to_qa_pipeline(self, *args, **kwargs):\n        from .qa import CitationQAPipeline\n\n        return TextVectorQA(\n            retrieving_pipeline=self.to_retrieval_pipeline(**kwargs),\n            qa_pipeline=CitationQAPipeline(**kwargs),\n        )\n\n    def run(self, text: str | list[str] | Document | list[Document]):\n        input_: list[Document] = []\n        if not isinstance(text, list):\n            text = [text]\n\n        for item in cast(list, text):\n            if isinstance(item, str):\n                input_.append(Document(text=item, id_=str(uuid.uuid4())))\n            elif isinstance(item, Document):\n                input_.append(item)\n            else:\n                raise ValueError(\n                    f\"Invalid input type {type(item)}, should be str or Document\"\n                )\n\n        embeddings = self.embedding(input_)\n        self.vector_store.add(\n            embeddings=embeddings,\n            ids=[t.id_ for t in input_],\n        )\n        if self.doc_store:\n            self.doc_store.add(input_)\n</code></pre>"},{"location":"reference/indices/vectorindex/#indices.vectorindex.VectorIndexing.to_retrieval_pipeline","title":"to_retrieval_pipeline","text":"<pre><code>to_retrieval_pipeline(*args, **kwargs)\n</code></pre> <p>Convert the indexing pipeline to a retrieval pipeline</p> Source code in <code>kotaemon\\indices\\vectorindex.py</code> <pre><code>def to_retrieval_pipeline(self, *args, **kwargs):\n    \"\"\"Convert the indexing pipeline to a retrieval pipeline\"\"\"\n    return VectorRetrieval(\n        vector_store=self.vector_store,\n        doc_store=self.doc_store,\n        embedding=self.embedding,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/indices/vectorindex/#indices.vectorindex.VectorRetrieval","title":"VectorRetrieval","text":"<p>             Bases: <code>BaseRetrieval</code></p> <p>Retrieve list of documents from vector store</p> Source code in <code>kotaemon\\indices\\vectorindex.py</code> <pre><code>class VectorRetrieval(BaseRetrieval):\n    \"\"\"Retrieve list of documents from vector store\"\"\"\n\n    vector_store: BaseVectorStore\n    doc_store: Optional[BaseDocumentStore] = None\n    embedding: BaseEmbeddings\n    rerankers: Sequence[BaseReranking] = []\n    top_k: int = 1\n\n    def run(\n        self, text: str | Document, top_k: Optional[int] = None, **kwargs\n    ) -&gt; list[RetrievedDocument]:\n        \"\"\"Retrieve a list of documents from vector store\n\n        Args:\n            text: the text to retrieve similar documents\n            top_k: number of top similar documents to return\n\n        Returns:\n            list[RetrievedDocument]: list of retrieved documents\n        \"\"\"\n        if top_k is None:\n            top_k = self.top_k\n\n        if self.doc_store is None:\n            raise ValueError(\n                \"doc_store is not provided. Please provide a doc_store to \"\n                \"retrieve the documents\"\n            )\n\n        emb: list[float] = self.embedding(text)[0].embedding\n        _, scores, ids = self.vector_store.query(embedding=emb, top_k=top_k)\n        docs = self.doc_store.get(ids)\n        result = [\n            RetrievedDocument(**doc.to_dict(), score=score)\n            for doc, score in zip(docs, scores)\n        ]\n        # use additional reranker to re-order the document list\n        if self.rerankers:\n            for reranker in self.rerankers:\n                result = reranker(documents=result, query=text)\n\n        return result\n</code></pre>"},{"location":"reference/indices/vectorindex/#indices.vectorindex.VectorRetrieval.run","title":"run","text":"<pre><code>run(text, top_k=None, **kwargs)\n</code></pre> <p>Retrieve a list of documents from vector store</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | Document</code> <p>the text to retrieve similar documents</p> required <code>top_k</code> <code>Optional[int]</code> <p>number of top similar documents to return</p> <code>None</code> <p>Returns:</p> Type Description <code>list[RetrievedDocument]</code> <p>list[RetrievedDocument]: list of retrieved documents</p> Source code in <code>kotaemon\\indices\\vectorindex.py</code> <pre><code>def run(\n    self, text: str | Document, top_k: Optional[int] = None, **kwargs\n) -&gt; list[RetrievedDocument]:\n    \"\"\"Retrieve a list of documents from vector store\n\n    Args:\n        text: the text to retrieve similar documents\n        top_k: number of top similar documents to return\n\n    Returns:\n        list[RetrievedDocument]: list of retrieved documents\n    \"\"\"\n    if top_k is None:\n        top_k = self.top_k\n\n    if self.doc_store is None:\n        raise ValueError(\n            \"doc_store is not provided. Please provide a doc_store to \"\n            \"retrieve the documents\"\n        )\n\n    emb: list[float] = self.embedding(text)[0].embedding\n    _, scores, ids = self.vector_store.query(embedding=emb, top_k=top_k)\n    docs = self.doc_store.get(ids)\n    result = [\n        RetrievedDocument(**doc.to_dict(), score=score)\n        for doc, score in zip(docs, scores)\n    ]\n    # use additional reranker to re-order the document list\n    if self.rerankers:\n        for reranker in self.rerankers:\n            result = reranker(documents=result, query=text)\n\n    return result\n</code></pre>"},{"location":"reference/indices/extractors/","title":"Extractors","text":""},{"location":"reference/indices/extractors/doc_parsers/","title":"Doc Parsers","text":""},{"location":"reference/indices/ingests/","title":"Ingests","text":""},{"location":"reference/indices/ingests/#indices.ingests.DocumentIngestor","title":"DocumentIngestor","text":"<p>             Bases: <code>BaseComponent</code></p> <p>Ingest common office document types into Document for indexing</p> Document types <ul> <li>pdf</li> <li>xlsx</li> <li>docx</li> </ul> Source code in <code>kotaemon\\indices\\ingests\\files.py</code> <pre><code>class DocumentIngestor(BaseComponent):\n    \"\"\"Ingest common office document types into Document for indexing\n\n    Document types:\n        - pdf\n        - xlsx\n        - docx\n    \"\"\"\n\n    pdf_mode: str = \"normal\"  # \"normal\", \"mathpix\", \"ocr\"\n    doc_parsers: list[BaseDocParser] = Param(default_callback=lambda _: [])\n    text_splitter: BaseSplitter = TokenSplitter.withx(\n        chunk_size=1024,\n        chunk_overlap=256,\n    )\n\n    def _get_reader(self, input_files: list[str | Path]):\n        \"\"\"Get appropriate readers for the input files based on file extension\"\"\"\n        file_extractor: dict[str, AutoReader | BaseReader] = {\n            \".xlsx\": PandasExcelReader(),\n        }\n\n        if self.pdf_mode == \"normal\":\n            file_extractor[\".pdf\"] = AutoReader(\"UnstructuredReader\")\n        elif self.pdf_mode == \"ocr\":\n            file_extractor[\".pdf\"] = OCRReader()\n        else:\n            file_extractor[\".pdf\"] = MathpixPDFReader()\n\n        main_reader = DirectoryReader(\n            input_files=input_files,\n            file_extractor=file_extractor,\n        )\n\n        return main_reader\n\n    def run(self, file_paths: list[str | Path] | str | Path) -&gt; list[Document]:\n        \"\"\"Ingest the file paths into Document\n\n        Args:\n            file_paths: list of file paths or a single file path\n\n        Returns:\n            list of parsed Documents\n        \"\"\"\n        if not isinstance(file_paths, list):\n            file_paths = [file_paths]\n\n        documents = self._get_reader(input_files=file_paths)()\n        nodes = self.text_splitter(documents)\n        self.log_progress(\".num_docs\", num_docs=len(nodes))\n\n        # document parsers call\n        if self.doc_parsers:\n            for parser in self.doc_parsers:\n                nodes = parser(nodes)\n\n        return nodes\n</code></pre>"},{"location":"reference/indices/ingests/#indices.ingests.DocumentIngestor.run","title":"run","text":"<pre><code>run(file_paths)\n</code></pre> <p>Ingest the file paths into Document</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>list[str | Path] | str | Path</code> <p>list of file paths or a single file path</p> required <p>Returns:</p> Type Description <code>list[Document]</code> <p>list of parsed Documents</p> Source code in <code>kotaemon\\indices\\ingests\\files.py</code> <pre><code>def run(self, file_paths: list[str | Path] | str | Path) -&gt; list[Document]:\n    \"\"\"Ingest the file paths into Document\n\n    Args:\n        file_paths: list of file paths or a single file path\n\n    Returns:\n        list of parsed Documents\n    \"\"\"\n    if not isinstance(file_paths, list):\n        file_paths = [file_paths]\n\n    documents = self._get_reader(input_files=file_paths)()\n    nodes = self.text_splitter(documents)\n    self.log_progress(\".num_docs\", num_docs=len(nodes))\n\n    # document parsers call\n    if self.doc_parsers:\n        for parser in self.doc_parsers:\n            nodes = parser(nodes)\n\n    return nodes\n</code></pre>"},{"location":"reference/indices/ingests/files/","title":"Files","text":""},{"location":"reference/indices/ingests/files/#indices.ingests.files.DocumentIngestor","title":"DocumentIngestor","text":"<p>             Bases: <code>BaseComponent</code></p> <p>Ingest common office document types into Document for indexing</p> Document types <ul> <li>pdf</li> <li>xlsx</li> <li>docx</li> </ul> Source code in <code>kotaemon\\indices\\ingests\\files.py</code> <pre><code>class DocumentIngestor(BaseComponent):\n    \"\"\"Ingest common office document types into Document for indexing\n\n    Document types:\n        - pdf\n        - xlsx\n        - docx\n    \"\"\"\n\n    pdf_mode: str = \"normal\"  # \"normal\", \"mathpix\", \"ocr\"\n    doc_parsers: list[BaseDocParser] = Param(default_callback=lambda _: [])\n    text_splitter: BaseSplitter = TokenSplitter.withx(\n        chunk_size=1024,\n        chunk_overlap=256,\n    )\n\n    def _get_reader(self, input_files: list[str | Path]):\n        \"\"\"Get appropriate readers for the input files based on file extension\"\"\"\n        file_extractor: dict[str, AutoReader | BaseReader] = {\n            \".xlsx\": PandasExcelReader(),\n        }\n\n        if self.pdf_mode == \"normal\":\n            file_extractor[\".pdf\"] = AutoReader(\"UnstructuredReader\")\n        elif self.pdf_mode == \"ocr\":\n            file_extractor[\".pdf\"] = OCRReader()\n        else:\n            file_extractor[\".pdf\"] = MathpixPDFReader()\n\n        main_reader = DirectoryReader(\n            input_files=input_files,\n            file_extractor=file_extractor,\n        )\n\n        return main_reader\n\n    def run(self, file_paths: list[str | Path] | str | Path) -&gt; list[Document]:\n        \"\"\"Ingest the file paths into Document\n\n        Args:\n            file_paths: list of file paths or a single file path\n\n        Returns:\n            list of parsed Documents\n        \"\"\"\n        if not isinstance(file_paths, list):\n            file_paths = [file_paths]\n\n        documents = self._get_reader(input_files=file_paths)()\n        nodes = self.text_splitter(documents)\n        self.log_progress(\".num_docs\", num_docs=len(nodes))\n\n        # document parsers call\n        if self.doc_parsers:\n            for parser in self.doc_parsers:\n                nodes = parser(nodes)\n\n        return nodes\n</code></pre>"},{"location":"reference/indices/ingests/files/#indices.ingests.files.DocumentIngestor.run","title":"run","text":"<pre><code>run(file_paths)\n</code></pre> <p>Ingest the file paths into Document</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>list[str | Path] | str | Path</code> <p>list of file paths or a single file path</p> required <p>Returns:</p> Type Description <code>list[Document]</code> <p>list of parsed Documents</p> Source code in <code>kotaemon\\indices\\ingests\\files.py</code> <pre><code>def run(self, file_paths: list[str | Path] | str | Path) -&gt; list[Document]:\n    \"\"\"Ingest the file paths into Document\n\n    Args:\n        file_paths: list of file paths or a single file path\n\n    Returns:\n        list of parsed Documents\n    \"\"\"\n    if not isinstance(file_paths, list):\n        file_paths = [file_paths]\n\n    documents = self._get_reader(input_files=file_paths)()\n    nodes = self.text_splitter(documents)\n    self.log_progress(\".num_docs\", num_docs=len(nodes))\n\n    # document parsers call\n    if self.doc_parsers:\n        for parser in self.doc_parsers:\n            nodes = parser(nodes)\n\n    return nodes\n</code></pre>"},{"location":"reference/indices/qa/","title":"Qa","text":""},{"location":"reference/indices/qa/#indices.qa.CitationPipeline","title":"CitationPipeline","text":"<p>             Bases: <code>BaseComponent</code></p> <p>Citation pipeline to extract cited evidences from source (based on input question)</p> Source code in <code>kotaemon\\indices\\qa\\citation.py</code> <pre><code>class CitationPipeline(BaseComponent):\n    \"\"\"Citation pipeline to extract cited evidences from source\n    (based on input question)\"\"\"\n\n    llm: BaseLLM\n\n    def run(\n        self,\n        context: str,\n        question: str,\n    ) -&gt; QuestionAnswer:\n        schema = QuestionAnswer.schema()\n        function = {\n            \"name\": schema[\"title\"],\n            \"description\": schema[\"description\"],\n            \"parameters\": schema,\n        }\n        llm_kwargs = {\n            \"functions\": [function],\n            \"function_call\": {\"name\": function[\"name\"]},\n        }\n        messages = [\n            SystemMessage(\n                content=(\n                    \"You are a world class algorithm to answer \"\n                    \"questions with correct and exact citations.\"\n                )\n            ),\n            HumanMessage(content=\"Answer question using the following context\"),\n            HumanMessage(content=context),\n            HumanMessage(content=f\"Question: {question}\"),\n            HumanMessage(\n                content=(\n                    \"Tips: Make sure to cite your sources, \"\n                    \"and use the exact words from the context.\"\n                )\n            ),\n        ]\n\n        llm_output = self.llm(messages, **llm_kwargs)\n        function_output = llm_output.messages[0].additional_kwargs[\"function_call\"][\n            \"arguments\"\n        ]\n        output = QuestionAnswer.parse_raw(function_output)\n\n        return output\n</code></pre>"},{"location":"reference/indices/qa/#indices.qa.CitationQAPipeline","title":"CitationQAPipeline","text":"<p>             Bases: <code>BaseComponent</code></p> <p>Answering question from a text corpus with citation</p> Source code in <code>kotaemon\\indices\\qa\\text_based.py</code> <pre><code>class CitationQAPipeline(BaseComponent):\n    \"\"\"Answering question from a text corpus with citation\"\"\"\n\n    qa_prompt_template: PromptTemplate = PromptTemplate(\n        'Answer the following question: \"{question}\". '\n        \"The context is: \\n{context}\\nAnswer: \"\n    )\n    llm: BaseLLM = AzureChatOpenAI.withx(\n        azure_endpoint=\"https://aurora-nlp.openai.azure.com/\",\n        openai_api_key=os.environ.get(\"OPENAI_API_KEY\", \"\"),\n        openai_api_version=\"2023-07-01-preview\",\n        deployment_name=\"nlp-q2-16k\",\n        temperature=0,\n        request_timeout=60,\n    )\n\n    def _format_doc_text(self, text: str) -&gt; str:\n        \"\"\"Format the text of each document\"\"\"\n        return text.replace(\"\\n\", \" \")\n\n    def _format_retrieved_context(self, documents: list[RetrievedDocument]) -&gt; str:\n        \"\"\"Format the texts between all documents\"\"\"\n        matched_texts: list[str] = [\n            self._format_doc_text(doc.text) for doc in documents\n        ]\n        return \"\\n\\n\".join(matched_texts)\n\n    def run(\n        self,\n        question: str,\n        documents: list[RetrievedDocument],\n        use_citation: bool = False,\n        **kwargs\n    ) -&gt; Document:\n        # retrieve relevant documents as context\n        context = self._format_retrieved_context(documents)\n        self.log_progress(\".context\", context=context)\n\n        # generate the answer\n        prompt = self.qa_prompt_template.populate(\n            context=context,\n            question=question,\n        )\n        self.log_progress(\".prompt\", prompt=prompt)\n        answer_text = self.llm(prompt).text\n        if use_citation:\n            # run citation pipeline\n            citation_pipeline = CitationPipeline(llm=self.llm)\n            citation = citation_pipeline(context=context, question=question)\n        else:\n            citation = None\n\n        answer = Document(text=answer_text, metadata={\"citation\": citation})\n        return answer\n</code></pre>"},{"location":"reference/indices/qa/citation/","title":"Citation","text":""},{"location":"reference/indices/qa/citation/#indices.qa.citation.FactWithEvidence","title":"FactWithEvidence","text":"<p>             Bases: <code>BaseModel</code></p> <p>Class representing a single statement.</p> <p>Each fact has a body and a list of sources. If there are multiple facts make sure to break them apart such that each one only uses a set of sources that are relevant to it.</p> Source code in <code>kotaemon\\indices\\qa\\citation.py</code> <pre><code>class FactWithEvidence(BaseModel):\n    \"\"\"Class representing a single statement.\n\n    Each fact has a body and a list of sources.\n    If there are multiple facts make sure to break them apart\n    such that each one only uses a set of sources that are relevant to it.\n    \"\"\"\n\n    fact: str = Field(..., description=\"Body of the sentence, as part of a response\")\n    substring_quote: List[str] = Field(\n        ...,\n        description=(\n            \"Each source should be a direct quote from the context, \"\n            \"as a substring of the original content\"\n        ),\n    )\n\n    def _get_span(self, quote: str, context: str, errs: int = 100) -&gt; Iterator[str]:\n        import regex\n\n        minor = quote\n        major = context\n\n        errs_ = 0\n        s = regex.search(f\"({minor}){{e&lt;={errs_}}}\", major)\n        while s is None and errs_ &lt;= errs:\n            errs_ += 1\n            s = regex.search(f\"({minor}){{e&lt;={errs_}}}\", major)\n\n        if s is not None:\n            yield from s.spans()\n\n    def get_spans(self, context: str) -&gt; Iterator[str]:\n        for quote in self.substring_quote:\n            yield from self._get_span(quote, context)\n</code></pre>"},{"location":"reference/indices/qa/citation/#indices.qa.citation.QuestionAnswer","title":"QuestionAnswer","text":"<p>             Bases: <code>BaseModel</code></p> <p>A question and its answer as a list of facts each one should have a source. each sentence contains a body and a list of sources.</p> Source code in <code>kotaemon\\indices\\qa\\citation.py</code> <pre><code>class QuestionAnswer(BaseModel):\n    \"\"\"A question and its answer as a list of facts each one should have a source.\n    each sentence contains a body and a list of sources.\"\"\"\n\n    question: str = Field(..., description=\"Question that was asked\")\n    answer: List[FactWithEvidence] = Field(\n        ...,\n        description=(\n            \"Body of the answer, each fact should be \"\n            \"its separate object with a body and a list of sources\"\n        ),\n    )\n</code></pre>"},{"location":"reference/indices/qa/citation/#indices.qa.citation.CitationPipeline","title":"CitationPipeline","text":"<p>             Bases: <code>BaseComponent</code></p> <p>Citation pipeline to extract cited evidences from source (based on input question)</p> Source code in <code>kotaemon\\indices\\qa\\citation.py</code> <pre><code>class CitationPipeline(BaseComponent):\n    \"\"\"Citation pipeline to extract cited evidences from source\n    (based on input question)\"\"\"\n\n    llm: BaseLLM\n\n    def run(\n        self,\n        context: str,\n        question: str,\n    ) -&gt; QuestionAnswer:\n        schema = QuestionAnswer.schema()\n        function = {\n            \"name\": schema[\"title\"],\n            \"description\": schema[\"description\"],\n            \"parameters\": schema,\n        }\n        llm_kwargs = {\n            \"functions\": [function],\n            \"function_call\": {\"name\": function[\"name\"]},\n        }\n        messages = [\n            SystemMessage(\n                content=(\n                    \"You are a world class algorithm to answer \"\n                    \"questions with correct and exact citations.\"\n                )\n            ),\n            HumanMessage(content=\"Answer question using the following context\"),\n            HumanMessage(content=context),\n            HumanMessage(content=f\"Question: {question}\"),\n            HumanMessage(\n                content=(\n                    \"Tips: Make sure to cite your sources, \"\n                    \"and use the exact words from the context.\"\n                )\n            ),\n        ]\n\n        llm_output = self.llm(messages, **llm_kwargs)\n        function_output = llm_output.messages[0].additional_kwargs[\"function_call\"][\n            \"arguments\"\n        ]\n        output = QuestionAnswer.parse_raw(function_output)\n\n        return output\n</code></pre>"},{"location":"reference/indices/qa/text_based/","title":"Text Based","text":""},{"location":"reference/indices/qa/text_based/#indices.qa.text_based.CitationQAPipeline","title":"CitationQAPipeline","text":"<p>             Bases: <code>BaseComponent</code></p> <p>Answering question from a text corpus with citation</p> Source code in <code>kotaemon\\indices\\qa\\text_based.py</code> <pre><code>class CitationQAPipeline(BaseComponent):\n    \"\"\"Answering question from a text corpus with citation\"\"\"\n\n    qa_prompt_template: PromptTemplate = PromptTemplate(\n        'Answer the following question: \"{question}\". '\n        \"The context is: \\n{context}\\nAnswer: \"\n    )\n    llm: BaseLLM = AzureChatOpenAI.withx(\n        azure_endpoint=\"https://aurora-nlp.openai.azure.com/\",\n        openai_api_key=os.environ.get(\"OPENAI_API_KEY\", \"\"),\n        openai_api_version=\"2023-07-01-preview\",\n        deployment_name=\"nlp-q2-16k\",\n        temperature=0,\n        request_timeout=60,\n    )\n\n    def _format_doc_text(self, text: str) -&gt; str:\n        \"\"\"Format the text of each document\"\"\"\n        return text.replace(\"\\n\", \" \")\n\n    def _format_retrieved_context(self, documents: list[RetrievedDocument]) -&gt; str:\n        \"\"\"Format the texts between all documents\"\"\"\n        matched_texts: list[str] = [\n            self._format_doc_text(doc.text) for doc in documents\n        ]\n        return \"\\n\\n\".join(matched_texts)\n\n    def run(\n        self,\n        question: str,\n        documents: list[RetrievedDocument],\n        use_citation: bool = False,\n        **kwargs\n    ) -&gt; Document:\n        # retrieve relevant documents as context\n        context = self._format_retrieved_context(documents)\n        self.log_progress(\".context\", context=context)\n\n        # generate the answer\n        prompt = self.qa_prompt_template.populate(\n            context=context,\n            question=question,\n        )\n        self.log_progress(\".prompt\", prompt=prompt)\n        answer_text = self.llm(prompt).text\n        if use_citation:\n            # run citation pipeline\n            citation_pipeline = CitationPipeline(llm=self.llm)\n            citation = citation_pipeline(context=context, question=question)\n        else:\n            citation = None\n\n        answer = Document(text=answer_text, metadata={\"citation\": citation})\n        return answer\n</code></pre>"},{"location":"reference/indices/rankings/","title":"Rankings","text":""},{"location":"reference/indices/rankings/#indices.rankings.BaseReranking","title":"BaseReranking","text":"<p>             Bases: <code>BaseComponent</code></p> Source code in <code>kotaemon\\indices\\rankings\\base.py</code> <pre><code>class BaseReranking(BaseComponent):\n    @abstractmethod\n    def run(self, documents: list[Document], query: str) -&gt; list[Document]:\n        \"\"\"Main method to transform list of documents\n        (re-ranking, filtering, etc)\"\"\"\n        ...\n</code></pre>"},{"location":"reference/indices/rankings/#indices.rankings.BaseReranking.run","title":"run  <code>abstractmethod</code>","text":"<pre><code>run(documents, query)\n</code></pre> <p>Main method to transform list of documents (re-ranking, filtering, etc)</p> Source code in <code>kotaemon\\indices\\rankings\\base.py</code> <pre><code>@abstractmethod\ndef run(self, documents: list[Document], query: str) -&gt; list[Document]:\n    \"\"\"Main method to transform list of documents\n    (re-ranking, filtering, etc)\"\"\"\n    ...\n</code></pre>"},{"location":"reference/indices/rankings/#indices.rankings.CohereReranking","title":"CohereReranking","text":"<p>             Bases: <code>BaseReranking</code></p> Source code in <code>kotaemon\\indices\\rankings\\cohere.py</code> <pre><code>class CohereReranking(BaseReranking):\n    model_name: str = \"rerank-multilingual-v2.0\"\n    cohere_api_key: str = os.environ.get(\"COHERE_API_KEY\", \"\")\n    top_k: int = 1\n\n    def run(self, documents: list[Document], query: str) -&gt; list[Document]:\n        \"\"\"Use Cohere Reranker model to re-order documents\n        with their relevance score\"\"\"\n        try:\n            import cohere\n        except ImportError:\n            raise ImportError(\n                \"Please install Cohere \" \"`pip install cohere` to use Cohere Reranking\"\n            )\n\n        cohere_client = cohere.Client(self.cohere_api_key)\n\n        # output documents\n        compressed_docs = []\n        if len(documents) &gt; 0:  # to avoid empty api call\n            _docs = [d.content for d in documents]\n            results = cohere_client.rerank(\n                model=self.model_name, query=query, documents=_docs, top_n=self.top_k\n            )\n            for r in results:\n                doc = documents[r.index]\n                doc.metadata[\"relevance_score\"] = r.relevance_score\n                compressed_docs.append(doc)\n\n        return compressed_docs\n</code></pre>"},{"location":"reference/indices/rankings/#indices.rankings.CohereReranking.run","title":"run","text":"<pre><code>run(documents, query)\n</code></pre> <p>Use Cohere Reranker model to re-order documents with their relevance score</p> Source code in <code>kotaemon\\indices\\rankings\\cohere.py</code> <pre><code>def run(self, documents: list[Document], query: str) -&gt; list[Document]:\n    \"\"\"Use Cohere Reranker model to re-order documents\n    with their relevance score\"\"\"\n    try:\n        import cohere\n    except ImportError:\n        raise ImportError(\n            \"Please install Cohere \" \"`pip install cohere` to use Cohere Reranking\"\n        )\n\n    cohere_client = cohere.Client(self.cohere_api_key)\n\n    # output documents\n    compressed_docs = []\n    if len(documents) &gt; 0:  # to avoid empty api call\n        _docs = [d.content for d in documents]\n        results = cohere_client.rerank(\n            model=self.model_name, query=query, documents=_docs, top_n=self.top_k\n        )\n        for r in results:\n            doc = documents[r.index]\n            doc.metadata[\"relevance_score\"] = r.relevance_score\n            compressed_docs.append(doc)\n\n    return compressed_docs\n</code></pre>"},{"location":"reference/indices/rankings/#indices.rankings.LLMReranking","title":"LLMReranking","text":"<p>             Bases: <code>BaseReranking</code></p> Source code in <code>kotaemon\\indices\\rankings\\llm.py</code> <pre><code>class LLMReranking(BaseReranking):\n    llm: BaseLLM\n    prompt_template: PromptTemplate = PromptTemplate(template=RERANK_PROMPT_TEMPLATE)\n    top_k: int = 3\n    concurrent: bool = True\n\n    def run(\n        self,\n        documents: list[Document],\n        query: str,\n    ) -&gt; list[Document]:\n        \"\"\"Filter down documents based on their relevance to the query.\"\"\"\n        filtered_docs = []\n        output_parser = BooleanOutputParser()\n\n        if self.concurrent:\n            with ThreadPoolExecutor() as executor:\n                futures = []\n                for doc in documents:\n                    _prompt = self.prompt_template.populate(\n                        question=query, context=doc.get_content()\n                    )\n                    futures.append(executor.submit(lambda: self.llm(_prompt).text))\n\n                results = [future.result() for future in futures]\n        else:\n            results = []\n            for doc in documents:\n                _prompt = self.prompt_template.populate(\n                    question=query, context=doc.get_content()\n                )\n                results.append(self.llm(_prompt).text)\n\n        # use Boolean parser to extract relevancy output from LLM\n        results = [output_parser.parse(result) for result in results]\n        for include_doc, doc in zip(results, documents):\n            if include_doc:\n                filtered_docs.append(doc)\n\n        # prevent returning empty result\n        if len(filtered_docs) == 0:\n            filtered_docs = documents[: self.top_k]\n\n        return filtered_docs\n</code></pre>"},{"location":"reference/indices/rankings/#indices.rankings.LLMReranking.run","title":"run","text":"<pre><code>run(documents, query)\n</code></pre> <p>Filter down documents based on their relevance to the query.</p> Source code in <code>kotaemon\\indices\\rankings\\llm.py</code> <pre><code>def run(\n    self,\n    documents: list[Document],\n    query: str,\n) -&gt; list[Document]:\n    \"\"\"Filter down documents based on their relevance to the query.\"\"\"\n    filtered_docs = []\n    output_parser = BooleanOutputParser()\n\n    if self.concurrent:\n        with ThreadPoolExecutor() as executor:\n            futures = []\n            for doc in documents:\n                _prompt = self.prompt_template.populate(\n                    question=query, context=doc.get_content()\n                )\n                futures.append(executor.submit(lambda: self.llm(_prompt).text))\n\n            results = [future.result() for future in futures]\n    else:\n        results = []\n        for doc in documents:\n            _prompt = self.prompt_template.populate(\n                question=query, context=doc.get_content()\n            )\n            results.append(self.llm(_prompt).text)\n\n    # use Boolean parser to extract relevancy output from LLM\n    results = [output_parser.parse(result) for result in results]\n    for include_doc, doc in zip(results, documents):\n        if include_doc:\n            filtered_docs.append(doc)\n\n    # prevent returning empty result\n    if len(filtered_docs) == 0:\n        filtered_docs = documents[: self.top_k]\n\n    return filtered_docs\n</code></pre>"},{"location":"reference/indices/rankings/base/","title":"Base","text":""},{"location":"reference/indices/rankings/base/#indices.rankings.base.BaseReranking","title":"BaseReranking","text":"<p>             Bases: <code>BaseComponent</code></p> Source code in <code>kotaemon\\indices\\rankings\\base.py</code> <pre><code>class BaseReranking(BaseComponent):\n    @abstractmethod\n    def run(self, documents: list[Document], query: str) -&gt; list[Document]:\n        \"\"\"Main method to transform list of documents\n        (re-ranking, filtering, etc)\"\"\"\n        ...\n</code></pre>"},{"location":"reference/indices/rankings/base/#indices.rankings.base.BaseReranking.run","title":"run  <code>abstractmethod</code>","text":"<pre><code>run(documents, query)\n</code></pre> <p>Main method to transform list of documents (re-ranking, filtering, etc)</p> Source code in <code>kotaemon\\indices\\rankings\\base.py</code> <pre><code>@abstractmethod\ndef run(self, documents: list[Document], query: str) -&gt; list[Document]:\n    \"\"\"Main method to transform list of documents\n    (re-ranking, filtering, etc)\"\"\"\n    ...\n</code></pre>"},{"location":"reference/indices/rankings/cohere/","title":"Cohere","text":""},{"location":"reference/indices/rankings/cohere/#indices.rankings.cohere.CohereReranking","title":"CohereReranking","text":"<p>             Bases: <code>BaseReranking</code></p> Source code in <code>kotaemon\\indices\\rankings\\cohere.py</code> <pre><code>class CohereReranking(BaseReranking):\n    model_name: str = \"rerank-multilingual-v2.0\"\n    cohere_api_key: str = os.environ.get(\"COHERE_API_KEY\", \"\")\n    top_k: int = 1\n\n    def run(self, documents: list[Document], query: str) -&gt; list[Document]:\n        \"\"\"Use Cohere Reranker model to re-order documents\n        with their relevance score\"\"\"\n        try:\n            import cohere\n        except ImportError:\n            raise ImportError(\n                \"Please install Cohere \" \"`pip install cohere` to use Cohere Reranking\"\n            )\n\n        cohere_client = cohere.Client(self.cohere_api_key)\n\n        # output documents\n        compressed_docs = []\n        if len(documents) &gt; 0:  # to avoid empty api call\n            _docs = [d.content for d in documents]\n            results = cohere_client.rerank(\n                model=self.model_name, query=query, documents=_docs, top_n=self.top_k\n            )\n            for r in results:\n                doc = documents[r.index]\n                doc.metadata[\"relevance_score\"] = r.relevance_score\n                compressed_docs.append(doc)\n\n        return compressed_docs\n</code></pre>"},{"location":"reference/indices/rankings/cohere/#indices.rankings.cohere.CohereReranking.run","title":"run","text":"<pre><code>run(documents, query)\n</code></pre> <p>Use Cohere Reranker model to re-order documents with their relevance score</p> Source code in <code>kotaemon\\indices\\rankings\\cohere.py</code> <pre><code>def run(self, documents: list[Document], query: str) -&gt; list[Document]:\n    \"\"\"Use Cohere Reranker model to re-order documents\n    with their relevance score\"\"\"\n    try:\n        import cohere\n    except ImportError:\n        raise ImportError(\n            \"Please install Cohere \" \"`pip install cohere` to use Cohere Reranking\"\n        )\n\n    cohere_client = cohere.Client(self.cohere_api_key)\n\n    # output documents\n    compressed_docs = []\n    if len(documents) &gt; 0:  # to avoid empty api call\n        _docs = [d.content for d in documents]\n        results = cohere_client.rerank(\n            model=self.model_name, query=query, documents=_docs, top_n=self.top_k\n        )\n        for r in results:\n            doc = documents[r.index]\n            doc.metadata[\"relevance_score\"] = r.relevance_score\n            compressed_docs.append(doc)\n\n    return compressed_docs\n</code></pre>"},{"location":"reference/indices/rankings/llm/","title":"Llm","text":""},{"location":"reference/indices/rankings/llm/#indices.rankings.llm.LLMReranking","title":"LLMReranking","text":"<p>             Bases: <code>BaseReranking</code></p> Source code in <code>kotaemon\\indices\\rankings\\llm.py</code> <pre><code>class LLMReranking(BaseReranking):\n    llm: BaseLLM\n    prompt_template: PromptTemplate = PromptTemplate(template=RERANK_PROMPT_TEMPLATE)\n    top_k: int = 3\n    concurrent: bool = True\n\n    def run(\n        self,\n        documents: list[Document],\n        query: str,\n    ) -&gt; list[Document]:\n        \"\"\"Filter down documents based on their relevance to the query.\"\"\"\n        filtered_docs = []\n        output_parser = BooleanOutputParser()\n\n        if self.concurrent:\n            with ThreadPoolExecutor() as executor:\n                futures = []\n                for doc in documents:\n                    _prompt = self.prompt_template.populate(\n                        question=query, context=doc.get_content()\n                    )\n                    futures.append(executor.submit(lambda: self.llm(_prompt).text))\n\n                results = [future.result() for future in futures]\n        else:\n            results = []\n            for doc in documents:\n                _prompt = self.prompt_template.populate(\n                    question=query, context=doc.get_content()\n                )\n                results.append(self.llm(_prompt).text)\n\n        # use Boolean parser to extract relevancy output from LLM\n        results = [output_parser.parse(result) for result in results]\n        for include_doc, doc in zip(results, documents):\n            if include_doc:\n                filtered_docs.append(doc)\n\n        # prevent returning empty result\n        if len(filtered_docs) == 0:\n            filtered_docs = documents[: self.top_k]\n\n        return filtered_docs\n</code></pre>"},{"location":"reference/indices/rankings/llm/#indices.rankings.llm.LLMReranking.run","title":"run","text":"<pre><code>run(documents, query)\n</code></pre> <p>Filter down documents based on their relevance to the query.</p> Source code in <code>kotaemon\\indices\\rankings\\llm.py</code> <pre><code>def run(\n    self,\n    documents: list[Document],\n    query: str,\n) -&gt; list[Document]:\n    \"\"\"Filter down documents based on their relevance to the query.\"\"\"\n    filtered_docs = []\n    output_parser = BooleanOutputParser()\n\n    if self.concurrent:\n        with ThreadPoolExecutor() as executor:\n            futures = []\n            for doc in documents:\n                _prompt = self.prompt_template.populate(\n                    question=query, context=doc.get_content()\n                )\n                futures.append(executor.submit(lambda: self.llm(_prompt).text))\n\n            results = [future.result() for future in futures]\n    else:\n        results = []\n        for doc in documents:\n            _prompt = self.prompt_template.populate(\n                question=query, context=doc.get_content()\n            )\n            results.append(self.llm(_prompt).text)\n\n    # use Boolean parser to extract relevancy output from LLM\n    results = [output_parser.parse(result) for result in results]\n    for include_doc, doc in zip(results, documents):\n        if include_doc:\n            filtered_docs.append(doc)\n\n    # prevent returning empty result\n    if len(filtered_docs) == 0:\n        filtered_docs = documents[: self.top_k]\n\n    return filtered_docs\n</code></pre>"},{"location":"reference/indices/splitters/","title":"Splitters","text":""},{"location":"reference/indices/splitters/#indices.splitters.BaseSplitter","title":"BaseSplitter","text":"<p>             Bases: <code>DocTransformer</code></p> <p>Represent base splitter class</p> Source code in <code>kotaemon\\indices\\splitters\\__init__.py</code> <pre><code>class BaseSplitter(DocTransformer):\n    \"\"\"Represent base splitter class\"\"\"\n\n    ...\n</code></pre>"},{"location":"reference/llms/","title":"Llms","text":""},{"location":"reference/llms/#llms.GatedBranchingPipeline","title":"GatedBranchingPipeline","text":"<p>             Bases: <code>SimpleBranchingPipeline</code></p> <p>A simple gated branching pipeline for executing multiple branches based on a     condition.</p> <p>This class extends the SimpleBranchingPipeline class and adds the ability to execute     the branches until a branch returns a non-empty output based on a condition.</p> <p>Attributes:</p> Name Type Description <code>branches</code> <code>List[BaseComponent]</code> <p>The list of branches to be executed.</p> Usage <pre><code>from kotaemon.llms import (\n    AzureChatOpenAI,\n    BasePromptComponent,\n    GatedLinearPipeline,\n)\nfrom kotaemon.parsers import RegexExtractor\n\ndef identity(x):\n    return x\n\npipeline = GatedBranchingPipeline()\nllm = AzureChatOpenAI(\n    openai_api_base=\"your openai api base\",\n    openai_api_key=\"your openai api key\",\n    openai_api_version=\"your openai api version\",\n    deployment_name=\"nlp-q2-gpt35\",\n    temperature=0,\n    request_timeout=600,\n)\n\nfor i in range(3):\n    pipeline.add_branch(\n        GatedLinearPipeline(\n            prompt=BasePromptComponent(template=f\"what is {i} in Japanese ?\"),\n            condition=RegexExtractor(pattern=f\"{i}\"),\n            llm=llm,\n            post_processor=identity,\n        )\n    )\nprint(pipeline(condition_text=\"1\"))\nprint(pipeline(condition_text=\"2\"))\n</code></pre> Source code in <code>kotaemon\\llms\\branching.py</code> <pre><code>class GatedBranchingPipeline(SimpleBranchingPipeline):\n    \"\"\"\n    A simple gated branching pipeline for executing multiple branches based on a\n        condition.\n\n    This class extends the SimpleBranchingPipeline class and adds the ability to execute\n        the branches until a branch returns a non-empty output based on a condition.\n\n    Attributes:\n        branches (List[BaseComponent]): The list of branches to be executed.\n\n    Usage:\n        ```python\n        from kotaemon.llms import (\n            AzureChatOpenAI,\n            BasePromptComponent,\n            GatedLinearPipeline,\n        )\n        from kotaemon.parsers import RegexExtractor\n\n        def identity(x):\n            return x\n\n        pipeline = GatedBranchingPipeline()\n        llm = AzureChatOpenAI(\n            openai_api_base=\"your openai api base\",\n            openai_api_key=\"your openai api key\",\n            openai_api_version=\"your openai api version\",\n            deployment_name=\"nlp-q2-gpt35\",\n            temperature=0,\n            request_timeout=600,\n        )\n\n        for i in range(3):\n            pipeline.add_branch(\n                GatedLinearPipeline(\n                    prompt=BasePromptComponent(template=f\"what is {i} in Japanese ?\"),\n                    condition=RegexExtractor(pattern=f\"{i}\"),\n                    llm=llm,\n                    post_processor=identity,\n                )\n            )\n        print(pipeline(condition_text=\"1\"))\n        print(pipeline(condition_text=\"2\"))\n        ```\n    \"\"\"\n\n    def run(self, *, condition_text: Optional[str] = None, **prompt_kwargs):\n        \"\"\"\n        Execute the pipeline by running each branch and return the output of the first\n            branch that returns a non-empty output based on the provided condition.\n\n        Args:\n            condition_text (str): The condition text to evaluate for each branch.\n                Default to None.\n            **prompt_kwargs: Keyword arguments for the branches.\n\n        Returns:\n            Union[OutputType, None]: The output of the first branch that satisfies the\n            condition, or None if no branch satisfies the condition.\n\n        Raises:\n            ValueError: If condition_text is None\n        \"\"\"\n        if condition_text is None:\n            raise ValueError(\"`condition_text` must be provided.\")\n\n        for i, branch in enumerate(self.branches):\n            self._prepare_child(branch, name=f\"branch-{i}\")\n            output = branch(condition_text=condition_text, **prompt_kwargs)\n            if output:\n                return output\n\n        return Document(None)\n</code></pre>"},{"location":"reference/llms/#llms.GatedBranchingPipeline.run","title":"run","text":"<pre><code>run(*, condition_text=None, **prompt_kwargs)\n</code></pre> <p>Execute the pipeline by running each branch and return the output of the first     branch that returns a non-empty output based on the provided condition.</p> <p>Parameters:</p> Name Type Description Default <code>condition_text</code> <code>str</code> <p>The condition text to evaluate for each branch. Default to None.</p> <code>None</code> <code>**prompt_kwargs</code> <p>Keyword arguments for the branches.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Union[OutputType, None]: The output of the first branch that satisfies the</p> <p>condition, or None if no branch satisfies the condition.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If condition_text is None</p> Source code in <code>kotaemon\\llms\\branching.py</code> <pre><code>def run(self, *, condition_text: Optional[str] = None, **prompt_kwargs):\n    \"\"\"\n    Execute the pipeline by running each branch and return the output of the first\n        branch that returns a non-empty output based on the provided condition.\n\n    Args:\n        condition_text (str): The condition text to evaluate for each branch.\n            Default to None.\n        **prompt_kwargs: Keyword arguments for the branches.\n\n    Returns:\n        Union[OutputType, None]: The output of the first branch that satisfies the\n        condition, or None if no branch satisfies the condition.\n\n    Raises:\n        ValueError: If condition_text is None\n    \"\"\"\n    if condition_text is None:\n        raise ValueError(\"`condition_text` must be provided.\")\n\n    for i, branch in enumerate(self.branches):\n        self._prepare_child(branch, name=f\"branch-{i}\")\n        output = branch(condition_text=condition_text, **prompt_kwargs)\n        if output:\n            return output\n\n    return Document(None)\n</code></pre>"},{"location":"reference/llms/#llms.SimpleBranchingPipeline","title":"SimpleBranchingPipeline","text":"<p>             Bases: <code>BaseComponent</code></p> <p>A simple branching pipeline for executing multiple branches.</p> <p>Attributes:</p> Name Type Description <code>branches</code> <code>List[BaseComponent]</code> <p>The list of branches to be executed.</p> Example Usage <pre><code>from kotaemon.llms import (\n    AzureChatOpenAI,\n    BasePromptComponent,\n    GatedLinearPipeline,\n)\nfrom kotaemon.parsers import RegexExtractor\n\ndef identity(x):\n    return x\n\npipeline = SimpleBranchingPipeline()\nllm = AzureChatOpenAI(\n    openai_api_base=\"your openai api base\",\n    openai_api_key=\"your openai api key\",\n    openai_api_version=\"your openai api version\",\n    deployment_name=\"nlp-q2-gpt35\",\n    temperature=0,\n    request_timeout=600,\n)\n\nfor i in range(3):\n    pipeline.add_branch(\n        GatedLinearPipeline(\n            prompt=BasePromptComponent(template=f\"what is {i} in Japanese ?\"),\n            condition=RegexExtractor(pattern=f\"{i}\"),\n            llm=llm,\n            post_processor=identity,\n        )\n    )\nprint(pipeline(condition_text=\"1\"))\nprint(pipeline(condition_text=\"2\"))\nprint(pipeline(condition_text=\"12\"))\n</code></pre> Source code in <code>kotaemon\\llms\\branching.py</code> <pre><code>class SimpleBranchingPipeline(BaseComponent):\n    \"\"\"\n    A simple branching pipeline for executing multiple branches.\n\n    Attributes:\n        branches (List[BaseComponent]): The list of branches to be executed.\n\n    Example Usage:\n        ```python\n        from kotaemon.llms import (\n            AzureChatOpenAI,\n            BasePromptComponent,\n            GatedLinearPipeline,\n        )\n        from kotaemon.parsers import RegexExtractor\n\n        def identity(x):\n            return x\n\n        pipeline = SimpleBranchingPipeline()\n        llm = AzureChatOpenAI(\n            openai_api_base=\"your openai api base\",\n            openai_api_key=\"your openai api key\",\n            openai_api_version=\"your openai api version\",\n            deployment_name=\"nlp-q2-gpt35\",\n            temperature=0,\n            request_timeout=600,\n        )\n\n        for i in range(3):\n            pipeline.add_branch(\n                GatedLinearPipeline(\n                    prompt=BasePromptComponent(template=f\"what is {i} in Japanese ?\"),\n                    condition=RegexExtractor(pattern=f\"{i}\"),\n                    llm=llm,\n                    post_processor=identity,\n                )\n            )\n        print(pipeline(condition_text=\"1\"))\n        print(pipeline(condition_text=\"2\"))\n        print(pipeline(condition_text=\"12\"))\n        ```\n    \"\"\"\n\n    branches: List[BaseComponent] = Param(default_callback=lambda *_: [])\n\n    def add_branch(self, component: BaseComponent):\n        \"\"\"\n        Add a new branch to the pipeline.\n\n        Args:\n            component (BaseComponent): The branch component to be added.\n        \"\"\"\n        self.branches.append(component)\n\n    def run(self, **prompt_kwargs):\n        \"\"\"\n        Execute the pipeline by running each branch and return the outputs as a list.\n\n        Args:\n            **prompt_kwargs: Keyword arguments for the branches.\n\n        Returns:\n            List: The outputs of each branch as a list.\n        \"\"\"\n        output = []\n        for i, branch in enumerate(self.branches):\n            self._prepare_child(branch, name=f\"branch-{i}\")\n            output.append(branch(**prompt_kwargs))\n\n        return output\n</code></pre>"},{"location":"reference/llms/#llms.SimpleBranchingPipeline.add_branch","title":"add_branch","text":"<pre><code>add_branch(component)\n</code></pre> <p>Add a new branch to the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>component</code> <code>BaseComponent</code> <p>The branch component to be added.</p> required Source code in <code>kotaemon\\llms\\branching.py</code> <pre><code>def add_branch(self, component: BaseComponent):\n    \"\"\"\n    Add a new branch to the pipeline.\n\n    Args:\n        component (BaseComponent): The branch component to be added.\n    \"\"\"\n    self.branches.append(component)\n</code></pre>"},{"location":"reference/llms/#llms.SimpleBranchingPipeline.run","title":"run","text":"<pre><code>run(**prompt_kwargs)\n</code></pre> <p>Execute the pipeline by running each branch and return the outputs as a list.</p> <p>Parameters:</p> Name Type Description Default <code>**prompt_kwargs</code> <p>Keyword arguments for the branches.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>List</code> <p>The outputs of each branch as a list.</p> Source code in <code>kotaemon\\llms\\branching.py</code> <pre><code>def run(self, **prompt_kwargs):\n    \"\"\"\n    Execute the pipeline by running each branch and return the outputs as a list.\n\n    Args:\n        **prompt_kwargs: Keyword arguments for the branches.\n\n    Returns:\n        List: The outputs of each branch as a list.\n    \"\"\"\n    output = []\n    for i, branch in enumerate(self.branches):\n        self._prepare_child(branch, name=f\"branch-{i}\")\n        output.append(branch(**prompt_kwargs))\n\n    return output\n</code></pre>"},{"location":"reference/llms/#llms.AzureOpenAI","title":"AzureOpenAI","text":"<p>             Bases: <code>LCCompletionMixin</code>, <code>LLM</code></p> <p>Wrapper around Langchain's AzureOpenAI class, focusing on key parameters</p> Source code in <code>kotaemon\\llms\\completions\\langchain_based.py</code> <pre><code>class AzureOpenAI(LCCompletionMixin, LLM):\n    \"\"\"Wrapper around Langchain's AzureOpenAI class, focusing on key parameters\"\"\"\n\n    def __init__(\n        self,\n        azure_endpoint: Optional[str] = None,\n        deployment_name: Optional[str] = None,\n        openai_api_version: str = \"\",\n        openai_api_key: Optional[str] = None,\n        model_name: str = \"text-davinci-003\",\n        temperature: float = 0.7,\n        max_tokens: int = 256,\n        top_p: float = 1,\n        frequency_penalty: float = 0,\n        n: int = 1,\n        best_of: int = 1,\n        request_timeout: Optional[float] = None,\n        max_retries: int = 2,\n        streaming: bool = False,\n        **params,\n    ):\n        super().__init__(\n            azure_endpoint=azure_endpoint,\n            deployment_name=deployment_name,\n            openai_api_version=openai_api_version,\n            openai_api_key=openai_api_key,\n            model_name=model_name,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            top_p=top_p,\n            frequency_penalty=frequency_penalty,\n            n=n,\n            best_of=best_of,\n            request_timeout=request_timeout,\n            max_retries=max_retries,\n            streaming=streaming,\n            **params,\n        )\n\n    def _get_lc_class(self):\n        import langchain.llms as langchain_llms\n\n        return langchain_llms.AzureOpenAI\n</code></pre>"},{"location":"reference/llms/#llms.OpenAI","title":"OpenAI","text":"<p>             Bases: <code>LCCompletionMixin</code>, <code>LLM</code></p> <p>Wrapper around Langchain's OpenAI class, focusing on key parameters</p> Source code in <code>kotaemon\\llms\\completions\\langchain_based.py</code> <pre><code>class OpenAI(LCCompletionMixin, LLM):\n    \"\"\"Wrapper around Langchain's OpenAI class, focusing on key parameters\"\"\"\n\n    def __init__(\n        self,\n        openai_api_key: Optional[str] = None,\n        openai_api_base: Optional[str] = None,\n        model_name: str = \"text-davinci-003\",\n        temperature: float = 0.7,\n        max_tokens: int = 256,\n        top_p: float = 1,\n        frequency_penalty: float = 0,\n        n: int = 1,\n        best_of: int = 1,\n        request_timeout: Optional[float] = None,\n        max_retries: int = 2,\n        streaming: bool = False,\n        **params,\n    ):\n        super().__init__(\n            openai_api_key=openai_api_key,\n            openai_api_base=openai_api_base,\n            model_name=model_name,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            top_p=top_p,\n            frequency_penalty=frequency_penalty,\n            n=n,\n            best_of=best_of,\n            request_timeout=request_timeout,\n            max_retries=max_retries,\n            streaming=streaming,\n            **params,\n        )\n\n    def _get_lc_class(self):\n        import langchain.llms as langchain_llms\n\n        return langchain_llms.OpenAI\n</code></pre>"},{"location":"reference/llms/#llms.ManualSequentialChainOfThought","title":"ManualSequentialChainOfThought","text":"<p>             Bases: <code>BaseComponent</code></p> <p>Perform sequential chain-of-thought with manual pre-defined prompts</p> <p>This method supports variable number of steps. Each step corresponds to a <code>kotaemon.pipelines.cot.Thought</code>. Please refer that section for Thought's detail. This section is about chaining thought together.</p> <p>Usage:</p> <p>Create and run a chain of thought without \"+\" operator:</p> <pre><code>&gt;&gt;&gt; from kotaemon.pipelines.cot import Thought, ManualSequentialChainOfThought\n&gt;&gt;&gt; llm = AzureChatOpenAI(...)\n&gt;&gt;&gt; thought1 = Thought(\n&gt;&gt;&gt;    prompt=\"Word {word} in {language} is \",\n&gt;&gt;&gt;    post_process=lambda string: {\"translated\": string},\n&gt;&gt;&gt; )\n&gt;&gt;&gt; thought2 = Thought(\n&gt;&gt;&gt;     prompt=\"Translate {translated} to Japanese\",\n&gt;&gt;&gt;     post_process=lambda string: {\"output\": string},\n&gt;&gt;&gt; )\n&gt;&gt;&gt; thought = ManualSequentialChainOfThought(thoughts=[thought1, thought2], llm=llm)\n&gt;&gt;&gt; thought(word=\"hello\", language=\"French\")\n{'word': 'hello',\n 'language': 'French',\n 'translated': '\"Bonjour\"',\n 'output': '\u3053\u3093\u306b\u3061\u306f (Konnichiwa)'}\n</code></pre> <p>Create and run a chain of thought without \"+\" operator: Please refer the <code>kotaemon.pipelines.cot.Thought</code> section for examples.</p> <p>This chain-of-thought optionally takes a termination check callback function. This function will be called after each thought is executed. It takes in a dictionary of all thought outputs so far, and it returns True or False. If True, the chain-of-thought will terminate. If unset, the default callback always returns False.</p> Source code in <code>kotaemon\\llms\\cot.py</code> <pre><code>class ManualSequentialChainOfThought(BaseComponent):\n    \"\"\"Perform sequential chain-of-thought with manual pre-defined prompts\n\n    This method supports variable number of steps. Each step corresponds to a\n    `kotaemon.pipelines.cot.Thought`. Please refer that section for\n    Thought's detail. This section is about chaining thought together.\n\n    _**Usage:**_\n\n    **Create and run a chain of thought without \"+\" operator:**\n\n    ```pycon\n    &gt;&gt;&gt; from kotaemon.pipelines.cot import Thought, ManualSequentialChainOfThought\n    &gt;&gt;&gt; llm = AzureChatOpenAI(...)\n    &gt;&gt;&gt; thought1 = Thought(\n    &gt;&gt;&gt;    prompt=\"Word {word} in {language} is \",\n    &gt;&gt;&gt;    post_process=lambda string: {\"translated\": string},\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; thought2 = Thought(\n    &gt;&gt;&gt;     prompt=\"Translate {translated} to Japanese\",\n    &gt;&gt;&gt;     post_process=lambda string: {\"output\": string},\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; thought = ManualSequentialChainOfThought(thoughts=[thought1, thought2], llm=llm)\n    &gt;&gt;&gt; thought(word=\"hello\", language=\"French\")\n    {'word': 'hello',\n     'language': 'French',\n     'translated': '\"Bonjour\"',\n     'output': '\u3053\u3093\u306b\u3061\u306f (Konnichiwa)'}\n    ```\n\n    **Create and run a chain of thought without \"+\" operator:** Please refer the\n    `kotaemon.pipelines.cot.Thought` section for examples.\n\n    This chain-of-thought optionally takes a termination check callback function.\n    This function will be called after each thought is executed. It takes in a\n    dictionary of all thought outputs so far, and it returns True or False. If\n    True, the chain-of-thought will terminate. If unset, the default callback always\n    returns False.\n    \"\"\"\n\n    thoughts: List[Thought] = Param(\n        default_callback=lambda *_: [], help=\"List of Thought\"\n    )\n    llm: LLM = Param(help=\"The LLM model to use (base of kotaemon.llms.BaseLLM)\")\n    terminate: Callable = Param(\n        default=lambda _: False,\n        help=\"Callback on terminate condition. Default to always return False\",\n    )\n\n    def run(self, **kwargs) -&gt; Document:\n        \"\"\"Run the manual chain of thought\"\"\"\n\n        inputs = deepcopy(kwargs)\n        for idx, thought in enumerate(self.thoughts):\n            if self.llm:\n                thought.llm = self.llm\n            self._prepare_child(thought, f\"thought{idx}\")\n\n            output = thought(**inputs)\n            inputs.update(output.content)\n            if self.terminate(inputs):\n                break\n\n        return Document(inputs)\n\n    def __add__(self, next_thought: Thought) -&gt; \"ManualSequentialChainOfThought\":\n        return ManualSequentialChainOfThought(\n            thoughts=self.thoughts + [next_thought], llm=self.llm\n        )\n</code></pre>"},{"location":"reference/llms/#llms.ManualSequentialChainOfThought.run","title":"run","text":"<pre><code>run(**kwargs)\n</code></pre> <p>Run the manual chain of thought</p> Source code in <code>kotaemon\\llms\\cot.py</code> <pre><code>def run(self, **kwargs) -&gt; Document:\n    \"\"\"Run the manual chain of thought\"\"\"\n\n    inputs = deepcopy(kwargs)\n    for idx, thought in enumerate(self.thoughts):\n        if self.llm:\n            thought.llm = self.llm\n        self._prepare_child(thought, f\"thought{idx}\")\n\n        output = thought(**inputs)\n        inputs.update(output.content)\n        if self.terminate(inputs):\n            break\n\n    return Document(inputs)\n</code></pre>"},{"location":"reference/llms/#llms.Thought","title":"Thought","text":"<p>             Bases: <code>BaseComponent</code></p> <p>A thought in the chain of thought</p> <ul> <li>Input: <code>**kwargs</code> pairs, where key is the placeholder in the prompt, and value is the value.</li> <li>Output: an output dictionary</li> </ul> <p>Usage:</p> <p>Create and run a thought:</p> <pre><code>&gt;&gt; from kotaemon.pipelines.cot import Thought\n&gt;&gt; thought = Thought(\n     prompt=\"How to {action} {object}?\",\n     llm=AzureChatOpenAI(...),\n     post_process=lambda string: {\"tutorial\": string},\n   )\n&gt;&gt; output = thought(action=\"install\", object=\"python\")\n&gt;&gt; print(output)\n{'tutorial': 'As an AI language model,...'}\n</code></pre> <p>Basically, when a thought is run, it will:</p> <ol> <li>Populate the prompt template with the input <code>**kwargs</code>.</li> <li>Run the LLM model with the populated prompt.</li> <li>Post-process the LLM output with the post-processor.</li> </ol> <p>This <code>Thought</code> allows chaining sequentially with the + operator. For example:</p> <pre><code>&gt;&gt; llm = AzureChatOpenAI(...)\n&gt;&gt; thought1 = Thought(\n       prompt=\"Word {word} in {language} is \",\n       llm=llm,\n       post_process=lambda string: {\"translated\": string},\n   )\n&gt;&gt; thought2 = Thought(\n        prompt=\"Translate {translated} to Japanese\",\n        llm=llm,\n        post_process=lambda string: {\"output\": string},\n   )\n\n&gt;&gt; thought = thought1 + thought2\n&gt;&gt; thought(word=\"hello\", language=\"French\")\n{'word': 'hello',\n 'language': 'French',\n 'translated': '\"Bonjour\"',\n 'output': '\u3053\u3093\u306b\u3061\u306f (Konnichiwa)'}\n</code></pre> <p>Under the hood, when the <code>+</code> operator is used, a <code>ManualSequentialChainOfThought</code> is created.</p> Source code in <code>kotaemon\\llms\\cot.py</code> <pre><code>class Thought(BaseComponent):\n    \"\"\"A thought in the chain of thought\n\n    - Input: `**kwargs` pairs, where key is the placeholder in the prompt, and\n    value is the value.\n    - Output: an output dictionary\n\n    _**Usage:**_\n\n    Create and run a thought:\n\n    ```python\n    &gt;&gt; from kotaemon.pipelines.cot import Thought\n    &gt;&gt; thought = Thought(\n         prompt=\"How to {action} {object}?\",\n         llm=AzureChatOpenAI(...),\n         post_process=lambda string: {\"tutorial\": string},\n       )\n    &gt;&gt; output = thought(action=\"install\", object=\"python\")\n    &gt;&gt; print(output)\n    {'tutorial': 'As an AI language model,...'}\n    ```\n\n    Basically, when a thought is run, it will:\n\n    1. Populate the prompt template with the input `**kwargs`.\n    2. Run the LLM model with the populated prompt.\n    3. Post-process the LLM output with the post-processor.\n\n    This `Thought` allows chaining sequentially with the + operator. For example:\n\n    ```python\n    &gt;&gt; llm = AzureChatOpenAI(...)\n    &gt;&gt; thought1 = Thought(\n           prompt=\"Word {word} in {language} is \",\n           llm=llm,\n           post_process=lambda string: {\"translated\": string},\n       )\n    &gt;&gt; thought2 = Thought(\n            prompt=\"Translate {translated} to Japanese\",\n            llm=llm,\n            post_process=lambda string: {\"output\": string},\n       )\n\n    &gt;&gt; thought = thought1 + thought2\n    &gt;&gt; thought(word=\"hello\", language=\"French\")\n    {'word': 'hello',\n     'language': 'French',\n     'translated': '\"Bonjour\"',\n     'output': '\u3053\u3093\u306b\u3061\u306f (Konnichiwa)'}\n    ```\n\n    Under the hood, when the `+` operator is used, a `ManualSequentialChainOfThought`\n    is created.\n    \"\"\"\n\n    prompt: str = Param(\n        help=(\n            \"The prompt template string. This prompt template has Python-like \"\n            \"variable placeholders, that then will be subsituted with real values when \"\n            \"this component is executed\"\n        )\n    )\n    llm: LLM = Node(AzureChatOpenAI, help=\"The LLM model to execute the input prompt\")\n    post_process: Function = Node(\n        help=(\n            \"The function post-processor that post-processes LLM output prediction .\"\n            \"It should take a string as input (this is the LLM output text) and return \"\n            \"a dictionary, where the key should\"\n        )\n    )\n\n    @Node.auto(depends_on=\"prompt\")\n    def prompt_template(self):\n        \"\"\"Automatically wrap around param prompt. Can ignore\"\"\"\n        return BasePromptComponent(self.prompt)\n\n    def run(self, **kwargs) -&gt; Document:\n        \"\"\"Run the chain of thought\"\"\"\n        prompt = self.prompt_template(**kwargs).text\n        response = self.llm(prompt).text\n        response = self.post_process(response)\n\n        return Document(response)\n\n    def get_variables(self) -&gt; List[str]:\n        return []\n\n    def __add__(self, next_thought: \"Thought\") -&gt; \"ManualSequentialChainOfThought\":\n        return ManualSequentialChainOfThought(\n            thoughts=[self, next_thought], llm=self.llm\n        )\n</code></pre>"},{"location":"reference/llms/#llms.Thought.prompt_template","title":"prompt_template","text":"<pre><code>prompt_template()\n</code></pre> <p>Automatically wrap around param prompt. Can ignore</p> Source code in <code>kotaemon\\llms\\cot.py</code> <pre><code>@Node.auto(depends_on=\"prompt\")\ndef prompt_template(self):\n    \"\"\"Automatically wrap around param prompt. Can ignore\"\"\"\n    return BasePromptComponent(self.prompt)\n</code></pre>"},{"location":"reference/llms/#llms.Thought.run","title":"run","text":"<pre><code>run(**kwargs)\n</code></pre> <p>Run the chain of thought</p> Source code in <code>kotaemon\\llms\\cot.py</code> <pre><code>def run(self, **kwargs) -&gt; Document:\n    \"\"\"Run the chain of thought\"\"\"\n    prompt = self.prompt_template(**kwargs).text\n    response = self.llm(prompt).text\n    response = self.post_process(response)\n\n    return Document(response)\n</code></pre>"},{"location":"reference/llms/#llms.GatedLinearPipeline","title":"GatedLinearPipeline","text":"<p>             Bases: <code>SimpleLinearPipeline</code></p> <p>A pipeline that extends the SimpleLinearPipeline class and adds a condition     attribute.</p> <p>Attributes:</p> Name Type Description <code>condition</code> <code>Callable[[IO_Type], Any]</code> <p>A callable function that represents the condition.</p> Usage Example Usage<pre><code>from kotaemon.llms import AzureChatOpenAI, BasePromptComponent\nfrom kotaemon.parsers import RegexExtractor\n\ndef identity(x):\n    return x\n\nllm = AzureChatOpenAI(\n    openai_api_base=\"your openai api base\",\n    openai_api_key=\"your openai api key\",\n    openai_api_version=\"your openai api version\",\n    deployment_name=\"nlp-q2-gpt35\",\n    temperature=0,\n    request_timeout=600,\n)\n\npipeline = GatedLinearPipeline(\n    prompt=BasePromptComponent(template=\"what is {word} in Japanese ?\"),\n    condition=RegexExtractor(pattern=\"some pattern\"),\n    llm=llm,\n    post_processor=identity,\n)\nprint(pipeline(condition_text=\"some pattern\", word=\"lone\"))\nprint(pipeline(condition_text=\"other pattern\", word=\"lone\"))\n</code></pre> Source code in <code>kotaemon\\llms\\linear.py</code> <pre><code>class GatedLinearPipeline(SimpleLinearPipeline):\n    \"\"\"\n    A pipeline that extends the SimpleLinearPipeline class and adds a condition\n        attribute.\n\n    Attributes:\n        condition (Callable[[IO_Type], Any]): A callable function that represents the\n            condition.\n\n    Usage:\n        ```{.py3 title=\"Example Usage\"}\n        from kotaemon.llms import AzureChatOpenAI, BasePromptComponent\n        from kotaemon.parsers import RegexExtractor\n\n        def identity(x):\n            return x\n\n        llm = AzureChatOpenAI(\n            openai_api_base=\"your openai api base\",\n            openai_api_key=\"your openai api key\",\n            openai_api_version=\"your openai api version\",\n            deployment_name=\"nlp-q2-gpt35\",\n            temperature=0,\n            request_timeout=600,\n        )\n\n        pipeline = GatedLinearPipeline(\n            prompt=BasePromptComponent(template=\"what is {word} in Japanese ?\"),\n            condition=RegexExtractor(pattern=\"some pattern\"),\n            llm=llm,\n            post_processor=identity,\n        )\n        print(pipeline(condition_text=\"some pattern\", word=\"lone\"))\n        print(pipeline(condition_text=\"other pattern\", word=\"lone\"))\n        ```\n    \"\"\"\n\n    condition: Callable[[IO_Type], Any]\n\n    def run(\n        self,\n        *,\n        condition_text: Optional[str] = None,\n        llm_kwargs: Optional[dict] = {},\n        post_processor_kwargs: Optional[dict] = {},\n        **prompt_kwargs,\n    ) -&gt; Document:\n        \"\"\"\n        Run the pipeline with the given arguments and return the final output as a\n            Document object.\n\n        Args:\n            condition_text (str): The condition text to evaluate. Default to None.\n            llm_kwargs (dict): Additional keyword arguments for the language model call.\n            post_processor_kwargs (dict): Additional keyword arguments for the\n                post-processor.\n            **prompt_kwargs: Keyword arguments for populating the prompt.\n\n        Returns:\n            Document: The final output of the pipeline as a Document object.\n\n        Raises:\n            ValueError: If condition_text is None\n        \"\"\"\n        if condition_text is None:\n            raise ValueError(\"`condition_text` must be provided\")\n\n        if self.condition(condition_text)[0]:\n            return super().run(\n                llm_kwargs=llm_kwargs,\n                post_processor_kwargs=post_processor_kwargs,\n                **prompt_kwargs,\n            )\n\n        return Document(None)\n</code></pre>"},{"location":"reference/llms/#llms.GatedLinearPipeline.run","title":"run","text":"<pre><code>run(*, condition_text=None, llm_kwargs={}, post_processor_kwargs={}, **prompt_kwargs)\n</code></pre> <p>Run the pipeline with the given arguments and return the final output as a     Document object.</p> <p>Parameters:</p> Name Type Description Default <code>condition_text</code> <code>str</code> <p>The condition text to evaluate. Default to None.</p> <code>None</code> <code>llm_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model call.</p> <code>{}</code> <code>post_processor_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the post-processor.</p> <code>{}</code> <code>**prompt_kwargs</code> <p>Keyword arguments for populating the prompt.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Document</code> <code>Document</code> <p>The final output of the pipeline as a Document object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If condition_text is None</p> Source code in <code>kotaemon\\llms\\linear.py</code> <pre><code>def run(\n    self,\n    *,\n    condition_text: Optional[str] = None,\n    llm_kwargs: Optional[dict] = {},\n    post_processor_kwargs: Optional[dict] = {},\n    **prompt_kwargs,\n) -&gt; Document:\n    \"\"\"\n    Run the pipeline with the given arguments and return the final output as a\n        Document object.\n\n    Args:\n        condition_text (str): The condition text to evaluate. Default to None.\n        llm_kwargs (dict): Additional keyword arguments for the language model call.\n        post_processor_kwargs (dict): Additional keyword arguments for the\n            post-processor.\n        **prompt_kwargs: Keyword arguments for populating the prompt.\n\n    Returns:\n        Document: The final output of the pipeline as a Document object.\n\n    Raises:\n        ValueError: If condition_text is None\n    \"\"\"\n    if condition_text is None:\n        raise ValueError(\"`condition_text` must be provided\")\n\n    if self.condition(condition_text)[0]:\n        return super().run(\n            llm_kwargs=llm_kwargs,\n            post_processor_kwargs=post_processor_kwargs,\n            **prompt_kwargs,\n        )\n\n    return Document(None)\n</code></pre>"},{"location":"reference/llms/#llms.SimpleLinearPipeline","title":"SimpleLinearPipeline","text":"<p>             Bases: <code>BaseComponent</code></p> <p>A simple pipeline for running a function with a prompt, a language model, and an     optional post-processor.</p> <p>Attributes:</p> Name Type Description <code>prompt</code> <code>BasePromptComponent</code> <p>The prompt component used to generate the initial input.</p> <code>llm</code> <code>Union[ChatLLM, LLM]</code> <p>The language model component used to generate the output.</p> <code>post_processor</code> <code>Union[BaseComponent, Callable[[IO_Type], IO_Type]]</code> <p>An optional post-processor component or function.</p> Example Usage <pre><code>from kotaemon.llms import AzureChatOpenAI, BasePromptComponent\n\ndef identity(x):\n    return x\n\nllm = AzureChatOpenAI(\n    openai_api_base=\"your openai api base\",\n    openai_api_key=\"your openai api key\",\n    openai_api_version=\"your openai api version\",\n    deployment_name=\"nlp-q2-gpt35\",\n    temperature=0,\n    request_timeout=600,\n)\n\npipeline = SimpleLinearPipeline(\n    prompt=BasePromptComponent(template=\"what is {word} in Japanese ?\"),\n    llm=llm,\n    post_processor=identity,\n)\nprint(pipeline(word=\"lone\"))\n</code></pre> Source code in <code>kotaemon\\llms\\linear.py</code> <pre><code>class SimpleLinearPipeline(BaseComponent):\n    \"\"\"\n    A simple pipeline for running a function with a prompt, a language model, and an\n        optional post-processor.\n\n    Attributes:\n        prompt (BasePromptComponent): The prompt component used to generate the initial\n            input.\n        llm (Union[ChatLLM, LLM]): The language model component used to generate the\n            output.\n        post_processor (Union[BaseComponent, Callable[[IO_Type], IO_Type]]): An optional\n            post-processor component or function.\n\n    Example Usage:\n        ```python\n        from kotaemon.llms import AzureChatOpenAI, BasePromptComponent\n\n        def identity(x):\n            return x\n\n        llm = AzureChatOpenAI(\n            openai_api_base=\"your openai api base\",\n            openai_api_key=\"your openai api key\",\n            openai_api_version=\"your openai api version\",\n            deployment_name=\"nlp-q2-gpt35\",\n            temperature=0,\n            request_timeout=600,\n        )\n\n        pipeline = SimpleLinearPipeline(\n            prompt=BasePromptComponent(template=\"what is {word} in Japanese ?\"),\n            llm=llm,\n            post_processor=identity,\n        )\n        print(pipeline(word=\"lone\"))\n        ```\n    \"\"\"\n\n    prompt: BasePromptComponent\n    llm: Union[ChatLLM, LLM]\n    post_processor: Union[BaseComponent, Callable[[IO_Type], IO_Type]]\n\n    def run(\n        self,\n        *,\n        llm_kwargs: Optional[dict] = {},\n        post_processor_kwargs: Optional[dict] = {},\n        **prompt_kwargs,\n    ):\n        \"\"\"\n        Run the function with the given arguments and return the final output as a\n            Document object.\n\n        Args:\n            llm_kwargs (dict): Keyword arguments for the llm call.\n            post_processor_kwargs (dict): Keyword arguments for the post_processor.\n            **prompt_kwargs: Keyword arguments for populating the prompt.\n\n        Returns:\n            Document: The final output of the function as a Document object.\n        \"\"\"\n        prompt = self.prompt(**prompt_kwargs)\n        llm_output = self.llm(prompt.text, **llm_kwargs)\n        if self.post_processor is not None:\n            final_output = self.post_processor(llm_output, **post_processor_kwargs)[0]\n        else:\n            final_output = llm_output\n\n        return Document(final_output)\n</code></pre>"},{"location":"reference/llms/#llms.SimpleLinearPipeline.run","title":"run","text":"<pre><code>run(*, llm_kwargs={}, post_processor_kwargs={}, **prompt_kwargs)\n</code></pre> <p>Run the function with the given arguments and return the final output as a     Document object.</p> <p>Parameters:</p> Name Type Description Default <code>llm_kwargs</code> <code>dict</code> <p>Keyword arguments for the llm call.</p> <code>{}</code> <code>post_processor_kwargs</code> <code>dict</code> <p>Keyword arguments for the post_processor.</p> <code>{}</code> <code>**prompt_kwargs</code> <p>Keyword arguments for populating the prompt.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Document</code> <p>The final output of the function as a Document object.</p> Source code in <code>kotaemon\\llms\\linear.py</code> <pre><code>def run(\n    self,\n    *,\n    llm_kwargs: Optional[dict] = {},\n    post_processor_kwargs: Optional[dict] = {},\n    **prompt_kwargs,\n):\n    \"\"\"\n    Run the function with the given arguments and return the final output as a\n        Document object.\n\n    Args:\n        llm_kwargs (dict): Keyword arguments for the llm call.\n        post_processor_kwargs (dict): Keyword arguments for the post_processor.\n        **prompt_kwargs: Keyword arguments for populating the prompt.\n\n    Returns:\n        Document: The final output of the function as a Document object.\n    \"\"\"\n    prompt = self.prompt(**prompt_kwargs)\n    llm_output = self.llm(prompt.text, **llm_kwargs)\n    if self.post_processor is not None:\n        final_output = self.post_processor(llm_output, **post_processor_kwargs)[0]\n    else:\n        final_output = llm_output\n\n    return Document(final_output)\n</code></pre>"},{"location":"reference/llms/#llms.BasePromptComponent","title":"BasePromptComponent","text":"<p>             Bases: <code>BaseComponent</code></p> <p>Base class for prompt components.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>PromptTemplate</code> <p>The prompt template.</p> required <code>**kwargs</code> <p>Any additional keyword arguments that will be used to populate the given template.</p> <code>{}</code> Source code in <code>kotaemon\\llms\\prompts\\base.py</code> <pre><code>class BasePromptComponent(BaseComponent):\n    \"\"\"\n    Base class for prompt components.\n\n    Args:\n        template (PromptTemplate): The prompt template.\n        **kwargs: Any additional keyword arguments that will be used to populate the\n            given template.\n    \"\"\"\n\n    class Config:\n        middleware_switches = {\"theflow.middleware.CachingMiddleware\": False}\n        allow_extra = True\n\n    def __init__(self, template: Union[str, PromptTemplate], **kwargs):\n        super().__init__()\n        self.template = (\n            template\n            if isinstance(template, PromptTemplate)\n            else PromptTemplate(template)\n        )\n\n        self.__set(**kwargs)\n\n    def __check_redundant_kwargs(self, **kwargs):\n        \"\"\"\n        Check for redundant keyword arguments.\n\n        Parameters:\n            **kwargs (dict): A dictionary of keyword arguments.\n\n        Raises:\n            ValueError: If any keys provided are not in the template.\n\n        Returns:\n            None\n        \"\"\"\n        self.template.check_redundant_kwargs(**kwargs)\n\n    def __check_unset_placeholders(self):\n        \"\"\"\n        Check if all the placeholders in the template are set.\n\n        This function checks if all the expected placeholders in the template are set as\n            attributes of the object. If any placeholders are missing, a `ValueError`\n            is raised with the names of the missing keys.\n\n        Parameters:\n            None\n\n        Returns:\n            None\n        \"\"\"\n        self.template.check_missing_kwargs(**self.__dict__)\n\n    def __validate_value_type(self, **kwargs):\n        \"\"\"\n        Validates the value types of the given keyword arguments.\n\n        Parameters:\n            **kwargs (dict): A dictionary of keyword arguments to be validated.\n\n        Raises:\n            ValueError: If any of the values in the kwargs dictionary have an\n                unsupported type.\n\n        Returns:\n            None\n        \"\"\"\n        type_error = []\n        for k, v in kwargs.items():\n            if not isinstance(v, (str, int, Document, Callable)):  # type: ignore\n                type_error.append((k, type(v)))\n\n        if type_error:\n            raise ValueError(\n                \"Type of values must be either int, str, Document, Callable, \"\n                f\"found unsupported type for (key, type): {type_error}\"\n            )\n\n    def __set(self, **kwargs):\n        \"\"\"\n        Set the values of the attributes in the object based on the provided keyword\n            arguments.\n\n        Args:\n            kwargs (dict): A dictionary with the attribute names as keys and the new\n                values as values.\n\n        Returns:\n            None\n        \"\"\"\n        self.__check_redundant_kwargs(**kwargs)\n        self.__validate_value_type(**kwargs)\n\n        self.__dict__.update(kwargs)\n\n    def __prepare_value(self):\n        \"\"\"\n        Generate a dictionary of keyword arguments based on the template's placeholders\n            and the current instance's attributes.\n\n        Returns:\n            dict: A dictionary of keyword arguments.\n        \"\"\"\n\n        def __prepare(key, value):\n            if isinstance(value, str):\n                return value\n            if isinstance(value, (int, Document)):\n                return str(value)\n\n            raise ValueError(\n                f\"Unsupported type {type(value)} for template value of key {key}\"\n            )\n\n        kwargs = {}\n        for k in self.template.placeholders:\n            v = getattr(self, k)\n\n            # if get a callable, execute to get its output\n            if isinstance(v, Callable):  # type: ignore[arg-type]\n                v = v()\n\n            if isinstance(v, list):\n                v = str([__prepare(k, each) for each in v])\n            elif isinstance(v, (str, int, Document)):\n                v = __prepare(k, v)\n            else:\n                raise ValueError(\n                    f\"Unsupported type {type(v)} for template value of key `{k}`\"\n                )\n            kwargs[k] = v\n\n        return kwargs\n\n    def set(self, **kwargs):\n        \"\"\"\n        Similar to `__set` but for external use.\n\n        Set the values of the attributes in the object based on the provided keyword\n            arguments.\n\n        Args:\n            kwargs (dict): A dictionary with the attribute names as keys and the new\n                values as values.\n\n        Returns:\n            None\n        \"\"\"\n        self.__set(**kwargs)\n\n    def run(self, **kwargs):\n        \"\"\"\n        Run the function with the given keyword arguments.\n\n        Args:\n            **kwargs: The keyword arguments to pass to the function.\n\n        Returns:\n            The result of calling the `populate` method of the `template` object\n            with the given keyword arguments.\n        \"\"\"\n        self.__set(**kwargs)\n        self.__check_unset_placeholders()\n        prepared_kwargs = self.__prepare_value()\n\n        text = self.template.populate(**prepared_kwargs)\n        return Document(text=text, metadata={\"origin\": \"PromptComponent\"})\n\n    def flow(self):\n        return self.__call__()\n</code></pre>"},{"location":"reference/llms/#llms.BasePromptComponent.set","title":"set","text":"<pre><code>set(**kwargs)\n</code></pre> <p>Similar to <code>__set</code> but for external use.</p> <p>Set the values of the attributes in the object based on the provided keyword     arguments.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>dict</code> <p>A dictionary with the attribute names as keys and the new values as values.</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>kotaemon\\llms\\prompts\\base.py</code> <pre><code>def set(self, **kwargs):\n    \"\"\"\n    Similar to `__set` but for external use.\n\n    Set the values of the attributes in the object based on the provided keyword\n        arguments.\n\n    Args:\n        kwargs (dict): A dictionary with the attribute names as keys and the new\n            values as values.\n\n    Returns:\n        None\n    \"\"\"\n    self.__set(**kwargs)\n</code></pre>"},{"location":"reference/llms/#llms.BasePromptComponent.run","title":"run","text":"<pre><code>run(**kwargs)\n</code></pre> <p>Run the function with the given keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>The keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The result of calling the <code>populate</code> method of the <code>template</code> object</p> <p>with the given keyword arguments.</p> Source code in <code>kotaemon\\llms\\prompts\\base.py</code> <pre><code>def run(self, **kwargs):\n    \"\"\"\n    Run the function with the given keyword arguments.\n\n    Args:\n        **kwargs: The keyword arguments to pass to the function.\n\n    Returns:\n        The result of calling the `populate` method of the `template` object\n        with the given keyword arguments.\n    \"\"\"\n    self.__set(**kwargs)\n    self.__check_unset_placeholders()\n    prepared_kwargs = self.__prepare_value()\n\n    text = self.template.populate(**prepared_kwargs)\n    return Document(text=text, metadata={\"origin\": \"PromptComponent\"})\n</code></pre>"},{"location":"reference/llms/#llms.PromptTemplate","title":"PromptTemplate","text":"<p>Base class for prompt templates.</p> Source code in <code>kotaemon\\llms\\prompts\\template.py</code> <pre><code>class PromptTemplate:\n    \"\"\"\n    Base class for prompt templates.\n    \"\"\"\n\n    def __init__(self, template: str, ignore_invalid=True):\n        template = template\n        formatter = Formatter()\n        parsed_template = list(formatter.parse(template))\n\n        placeholders = set()\n        for _, key, _, _ in parsed_template:\n            if key is None:\n                continue\n            if not key.isidentifier():\n                if ignore_invalid:\n                    warnings.warn(f\"Ignore invalid placeholder: {key}.\", UserWarning)\n                else:\n                    raise ValueError(\n                        \"Placeholder name must be a valid Python identifier, found:\"\n                        f\" {key}.\"\n                    )\n            placeholders.add(key)\n\n        self.template = template\n        self.placeholders = placeholders\n        self.__formatter = formatter\n        self.__parsed_template = parsed_template\n\n    def check_missing_kwargs(self, **kwargs):\n        \"\"\"\n        Check if all the placeholders in the template are set.\n\n        This function checks if all the expected placeholders in the template are set as\n            attributes of the object. If any placeholders are missing, a `ValueError`\n            is raised with the names of the missing keys.\n\n        Parameters:\n            None\n\n        Returns:\n            None\n        \"\"\"\n        missing_keys = self.placeholders.difference(kwargs.keys())\n        if missing_keys:\n            raise ValueError(f\"Missing keys in template: {','.join(missing_keys)}\")\n\n    def check_redundant_kwargs(self, **kwargs):\n        \"\"\"\n        Check if all the placeholders in the template are set.\n\n        This function checks if all the expected placeholders in the template are set as\n            attributes of the object. If any placeholders are missing, a `ValueError`\n            is raised with the names of the missing keys.\n\n        Parameters:\n            None\n\n        Returns:\n            None\n        \"\"\"\n        provided_keys = set(kwargs.keys())\n        redundant_keys = provided_keys - self.placeholders\n\n        if redundant_keys:\n            warnings.warn(\n                f\"Keys provided but not in template: {','.join(redundant_keys)}\",\n                UserWarning,\n            )\n\n    def populate(self, **kwargs) -&gt; str:\n        \"\"\"\n        Strictly populate the template with the given keyword arguments.\n\n        Args:\n            **kwargs: The keyword arguments to populate the template.\n                      Each keyword corresponds to a placeholder in the template.\n\n        Returns:\n            The populated template.\n\n        Raises:\n            ValueError: If an unknown placeholder is provided.\n        \"\"\"\n        self.check_missing_kwargs(**kwargs)\n\n        return self.partial_populate(**kwargs)\n\n    def partial_populate(self, **kwargs):\n        \"\"\"\n        Partially populate the template with the given keyword arguments.\n\n        Args:\n            **kwargs: The keyword arguments to populate the template.\n                      Each keyword corresponds to a placeholder in the template.\n\n        Returns:\n            str: The populated template.\n        \"\"\"\n        self.check_redundant_kwargs(**kwargs)\n\n        prompt = []\n        for literal_text, field_name, format_spec, conversion in self.__parsed_template:\n            prompt.append(literal_text)\n\n            if field_name is None:\n                continue\n\n            if field_name not in kwargs:\n                if conversion:\n                    value = f\"{{{field_name}}}!{conversion}:{format_spec}\"\n                else:\n                    value = f\"{{{field_name}:{format_spec}}}\"\n            else:\n                value = kwargs[field_name]\n                if conversion is not None:\n                    value = self.__formatter.convert_field(value, conversion)\n                if format_spec is not None:\n                    value = self.__formatter.format_field(value, format_spec)\n\n            prompt.append(value)\n\n        return \"\".join(prompt)\n\n    def __add__(self, other):\n        \"\"\"\n        Create a new PromptTemplate object by concatenating the template of the current\n            object with the template of another PromptTemplate object.\n\n        Parameters:\n            other (PromptTemplate): Another PromptTemplate object.\n\n        Returns:\n            PromptTemplate: A new PromptTemplate object with the concatenated templates.\n        \"\"\"\n        return PromptTemplate(self.template + \"\\n\" + other.template)\n</code></pre>"},{"location":"reference/llms/#llms.PromptTemplate.check_missing_kwargs","title":"check_missing_kwargs","text":"<pre><code>check_missing_kwargs(**kwargs)\n</code></pre> <p>Check if all the placeholders in the template are set.</p> <p>This function checks if all the expected placeholders in the template are set as     attributes of the object. If any placeholders are missing, a <code>ValueError</code>     is raised with the names of the missing keys.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>kotaemon\\llms\\prompts\\template.py</code> <pre><code>def check_missing_kwargs(self, **kwargs):\n    \"\"\"\n    Check if all the placeholders in the template are set.\n\n    This function checks if all the expected placeholders in the template are set as\n        attributes of the object. If any placeholders are missing, a `ValueError`\n        is raised with the names of the missing keys.\n\n    Parameters:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    missing_keys = self.placeholders.difference(kwargs.keys())\n    if missing_keys:\n        raise ValueError(f\"Missing keys in template: {','.join(missing_keys)}\")\n</code></pre>"},{"location":"reference/llms/#llms.PromptTemplate.check_redundant_kwargs","title":"check_redundant_kwargs","text":"<pre><code>check_redundant_kwargs(**kwargs)\n</code></pre> <p>Check if all the placeholders in the template are set.</p> <p>This function checks if all the expected placeholders in the template are set as     attributes of the object. If any placeholders are missing, a <code>ValueError</code>     is raised with the names of the missing keys.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>kotaemon\\llms\\prompts\\template.py</code> <pre><code>def check_redundant_kwargs(self, **kwargs):\n    \"\"\"\n    Check if all the placeholders in the template are set.\n\n    This function checks if all the expected placeholders in the template are set as\n        attributes of the object. If any placeholders are missing, a `ValueError`\n        is raised with the names of the missing keys.\n\n    Parameters:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    provided_keys = set(kwargs.keys())\n    redundant_keys = provided_keys - self.placeholders\n\n    if redundant_keys:\n        warnings.warn(\n            f\"Keys provided but not in template: {','.join(redundant_keys)}\",\n            UserWarning,\n        )\n</code></pre>"},{"location":"reference/llms/#llms.PromptTemplate.populate","title":"populate","text":"<pre><code>populate(**kwargs)\n</code></pre> <p>Strictly populate the template with the given keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>The keyword arguments to populate the template.       Each keyword corresponds to a placeholder in the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The populated template.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unknown placeholder is provided.</p> Source code in <code>kotaemon\\llms\\prompts\\template.py</code> <pre><code>def populate(self, **kwargs) -&gt; str:\n    \"\"\"\n    Strictly populate the template with the given keyword arguments.\n\n    Args:\n        **kwargs: The keyword arguments to populate the template.\n                  Each keyword corresponds to a placeholder in the template.\n\n    Returns:\n        The populated template.\n\n    Raises:\n        ValueError: If an unknown placeholder is provided.\n    \"\"\"\n    self.check_missing_kwargs(**kwargs)\n\n    return self.partial_populate(**kwargs)\n</code></pre>"},{"location":"reference/llms/#llms.PromptTemplate.partial_populate","title":"partial_populate","text":"<pre><code>partial_populate(**kwargs)\n</code></pre> <p>Partially populate the template with the given keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>The keyword arguments to populate the template.       Each keyword corresponds to a placeholder in the template.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The populated template.</p> Source code in <code>kotaemon\\llms\\prompts\\template.py</code> <pre><code>def partial_populate(self, **kwargs):\n    \"\"\"\n    Partially populate the template with the given keyword arguments.\n\n    Args:\n        **kwargs: The keyword arguments to populate the template.\n                  Each keyword corresponds to a placeholder in the template.\n\n    Returns:\n        str: The populated template.\n    \"\"\"\n    self.check_redundant_kwargs(**kwargs)\n\n    prompt = []\n    for literal_text, field_name, format_spec, conversion in self.__parsed_template:\n        prompt.append(literal_text)\n\n        if field_name is None:\n            continue\n\n        if field_name not in kwargs:\n            if conversion:\n                value = f\"{{{field_name}}}!{conversion}:{format_spec}\"\n            else:\n                value = f\"{{{field_name}:{format_spec}}}\"\n        else:\n            value = kwargs[field_name]\n            if conversion is not None:\n                value = self.__formatter.convert_field(value, conversion)\n            if format_spec is not None:\n                value = self.__formatter.format_field(value, format_spec)\n\n        prompt.append(value)\n\n    return \"\".join(prompt)\n</code></pre>"},{"location":"reference/llms/branching/","title":"Branching","text":""},{"location":"reference/llms/branching/#llms.branching.SimpleBranchingPipeline","title":"SimpleBranchingPipeline","text":"<p>             Bases: <code>BaseComponent</code></p> <p>A simple branching pipeline for executing multiple branches.</p> <p>Attributes:</p> Name Type Description <code>branches</code> <code>List[BaseComponent]</code> <p>The list of branches to be executed.</p> Example Usage <pre><code>from kotaemon.llms import (\n    AzureChatOpenAI,\n    BasePromptComponent,\n    GatedLinearPipeline,\n)\nfrom kotaemon.parsers import RegexExtractor\n\ndef identity(x):\n    return x\n\npipeline = SimpleBranchingPipeline()\nllm = AzureChatOpenAI(\n    openai_api_base=\"your openai api base\",\n    openai_api_key=\"your openai api key\",\n    openai_api_version=\"your openai api version\",\n    deployment_name=\"nlp-q2-gpt35\",\n    temperature=0,\n    request_timeout=600,\n)\n\nfor i in range(3):\n    pipeline.add_branch(\n        GatedLinearPipeline(\n            prompt=BasePromptComponent(template=f\"what is {i} in Japanese ?\"),\n            condition=RegexExtractor(pattern=f\"{i}\"),\n            llm=llm,\n            post_processor=identity,\n        )\n    )\nprint(pipeline(condition_text=\"1\"))\nprint(pipeline(condition_text=\"2\"))\nprint(pipeline(condition_text=\"12\"))\n</code></pre> Source code in <code>kotaemon\\llms\\branching.py</code> <pre><code>class SimpleBranchingPipeline(BaseComponent):\n    \"\"\"\n    A simple branching pipeline for executing multiple branches.\n\n    Attributes:\n        branches (List[BaseComponent]): The list of branches to be executed.\n\n    Example Usage:\n        ```python\n        from kotaemon.llms import (\n            AzureChatOpenAI,\n            BasePromptComponent,\n            GatedLinearPipeline,\n        )\n        from kotaemon.parsers import RegexExtractor\n\n        def identity(x):\n            return x\n\n        pipeline = SimpleBranchingPipeline()\n        llm = AzureChatOpenAI(\n            openai_api_base=\"your openai api base\",\n            openai_api_key=\"your openai api key\",\n            openai_api_version=\"your openai api version\",\n            deployment_name=\"nlp-q2-gpt35\",\n            temperature=0,\n            request_timeout=600,\n        )\n\n        for i in range(3):\n            pipeline.add_branch(\n                GatedLinearPipeline(\n                    prompt=BasePromptComponent(template=f\"what is {i} in Japanese ?\"),\n                    condition=RegexExtractor(pattern=f\"{i}\"),\n                    llm=llm,\n                    post_processor=identity,\n                )\n            )\n        print(pipeline(condition_text=\"1\"))\n        print(pipeline(condition_text=\"2\"))\n        print(pipeline(condition_text=\"12\"))\n        ```\n    \"\"\"\n\n    branches: List[BaseComponent] = Param(default_callback=lambda *_: [])\n\n    def add_branch(self, component: BaseComponent):\n        \"\"\"\n        Add a new branch to the pipeline.\n\n        Args:\n            component (BaseComponent): The branch component to be added.\n        \"\"\"\n        self.branches.append(component)\n\n    def run(self, **prompt_kwargs):\n        \"\"\"\n        Execute the pipeline by running each branch and return the outputs as a list.\n\n        Args:\n            **prompt_kwargs: Keyword arguments for the branches.\n\n        Returns:\n            List: The outputs of each branch as a list.\n        \"\"\"\n        output = []\n        for i, branch in enumerate(self.branches):\n            self._prepare_child(branch, name=f\"branch-{i}\")\n            output.append(branch(**prompt_kwargs))\n\n        return output\n</code></pre>"},{"location":"reference/llms/branching/#llms.branching.SimpleBranchingPipeline.add_branch","title":"add_branch","text":"<pre><code>add_branch(component)\n</code></pre> <p>Add a new branch to the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>component</code> <code>BaseComponent</code> <p>The branch component to be added.</p> required Source code in <code>kotaemon\\llms\\branching.py</code> <pre><code>def add_branch(self, component: BaseComponent):\n    \"\"\"\n    Add a new branch to the pipeline.\n\n    Args:\n        component (BaseComponent): The branch component to be added.\n    \"\"\"\n    self.branches.append(component)\n</code></pre>"},{"location":"reference/llms/branching/#llms.branching.SimpleBranchingPipeline.run","title":"run","text":"<pre><code>run(**prompt_kwargs)\n</code></pre> <p>Execute the pipeline by running each branch and return the outputs as a list.</p> <p>Parameters:</p> Name Type Description Default <code>**prompt_kwargs</code> <p>Keyword arguments for the branches.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>List</code> <p>The outputs of each branch as a list.</p> Source code in <code>kotaemon\\llms\\branching.py</code> <pre><code>def run(self, **prompt_kwargs):\n    \"\"\"\n    Execute the pipeline by running each branch and return the outputs as a list.\n\n    Args:\n        **prompt_kwargs: Keyword arguments for the branches.\n\n    Returns:\n        List: The outputs of each branch as a list.\n    \"\"\"\n    output = []\n    for i, branch in enumerate(self.branches):\n        self._prepare_child(branch, name=f\"branch-{i}\")\n        output.append(branch(**prompt_kwargs))\n\n    return output\n</code></pre>"},{"location":"reference/llms/branching/#llms.branching.GatedBranchingPipeline","title":"GatedBranchingPipeline","text":"<p>             Bases: <code>SimpleBranchingPipeline</code></p> <p>A simple gated branching pipeline for executing multiple branches based on a     condition.</p> <p>This class extends the SimpleBranchingPipeline class and adds the ability to execute     the branches until a branch returns a non-empty output based on a condition.</p> <p>Attributes:</p> Name Type Description <code>branches</code> <code>List[BaseComponent]</code> <p>The list of branches to be executed.</p> Usage <pre><code>from kotaemon.llms import (\n    AzureChatOpenAI,\n    BasePromptComponent,\n    GatedLinearPipeline,\n)\nfrom kotaemon.parsers import RegexExtractor\n\ndef identity(x):\n    return x\n\npipeline = GatedBranchingPipeline()\nllm = AzureChatOpenAI(\n    openai_api_base=\"your openai api base\",\n    openai_api_key=\"your openai api key\",\n    openai_api_version=\"your openai api version\",\n    deployment_name=\"nlp-q2-gpt35\",\n    temperature=0,\n    request_timeout=600,\n)\n\nfor i in range(3):\n    pipeline.add_branch(\n        GatedLinearPipeline(\n            prompt=BasePromptComponent(template=f\"what is {i} in Japanese ?\"),\n            condition=RegexExtractor(pattern=f\"{i}\"),\n            llm=llm,\n            post_processor=identity,\n        )\n    )\nprint(pipeline(condition_text=\"1\"))\nprint(pipeline(condition_text=\"2\"))\n</code></pre> Source code in <code>kotaemon\\llms\\branching.py</code> <pre><code>class GatedBranchingPipeline(SimpleBranchingPipeline):\n    \"\"\"\n    A simple gated branching pipeline for executing multiple branches based on a\n        condition.\n\n    This class extends the SimpleBranchingPipeline class and adds the ability to execute\n        the branches until a branch returns a non-empty output based on a condition.\n\n    Attributes:\n        branches (List[BaseComponent]): The list of branches to be executed.\n\n    Usage:\n        ```python\n        from kotaemon.llms import (\n            AzureChatOpenAI,\n            BasePromptComponent,\n            GatedLinearPipeline,\n        )\n        from kotaemon.parsers import RegexExtractor\n\n        def identity(x):\n            return x\n\n        pipeline = GatedBranchingPipeline()\n        llm = AzureChatOpenAI(\n            openai_api_base=\"your openai api base\",\n            openai_api_key=\"your openai api key\",\n            openai_api_version=\"your openai api version\",\n            deployment_name=\"nlp-q2-gpt35\",\n            temperature=0,\n            request_timeout=600,\n        )\n\n        for i in range(3):\n            pipeline.add_branch(\n                GatedLinearPipeline(\n                    prompt=BasePromptComponent(template=f\"what is {i} in Japanese ?\"),\n                    condition=RegexExtractor(pattern=f\"{i}\"),\n                    llm=llm,\n                    post_processor=identity,\n                )\n            )\n        print(pipeline(condition_text=\"1\"))\n        print(pipeline(condition_text=\"2\"))\n        ```\n    \"\"\"\n\n    def run(self, *, condition_text: Optional[str] = None, **prompt_kwargs):\n        \"\"\"\n        Execute the pipeline by running each branch and return the output of the first\n            branch that returns a non-empty output based on the provided condition.\n\n        Args:\n            condition_text (str): The condition text to evaluate for each branch.\n                Default to None.\n            **prompt_kwargs: Keyword arguments for the branches.\n\n        Returns:\n            Union[OutputType, None]: The output of the first branch that satisfies the\n            condition, or None if no branch satisfies the condition.\n\n        Raises:\n            ValueError: If condition_text is None\n        \"\"\"\n        if condition_text is None:\n            raise ValueError(\"`condition_text` must be provided.\")\n\n        for i, branch in enumerate(self.branches):\n            self._prepare_child(branch, name=f\"branch-{i}\")\n            output = branch(condition_text=condition_text, **prompt_kwargs)\n            if output:\n                return output\n\n        return Document(None)\n</code></pre>"},{"location":"reference/llms/branching/#llms.branching.GatedBranchingPipeline.run","title":"run","text":"<pre><code>run(*, condition_text=None, **prompt_kwargs)\n</code></pre> <p>Execute the pipeline by running each branch and return the output of the first     branch that returns a non-empty output based on the provided condition.</p> <p>Parameters:</p> Name Type Description Default <code>condition_text</code> <code>str</code> <p>The condition text to evaluate for each branch. Default to None.</p> <code>None</code> <code>**prompt_kwargs</code> <p>Keyword arguments for the branches.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Union[OutputType, None]: The output of the first branch that satisfies the</p> <p>condition, or None if no branch satisfies the condition.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If condition_text is None</p> Source code in <code>kotaemon\\llms\\branching.py</code> <pre><code>def run(self, *, condition_text: Optional[str] = None, **prompt_kwargs):\n    \"\"\"\n    Execute the pipeline by running each branch and return the output of the first\n        branch that returns a non-empty output based on the provided condition.\n\n    Args:\n        condition_text (str): The condition text to evaluate for each branch.\n            Default to None.\n        **prompt_kwargs: Keyword arguments for the branches.\n\n    Returns:\n        Union[OutputType, None]: The output of the first branch that satisfies the\n        condition, or None if no branch satisfies the condition.\n\n    Raises:\n        ValueError: If condition_text is None\n    \"\"\"\n    if condition_text is None:\n        raise ValueError(\"`condition_text` must be provided.\")\n\n    for i, branch in enumerate(self.branches):\n        self._prepare_child(branch, name=f\"branch-{i}\")\n        output = branch(condition_text=condition_text, **prompt_kwargs)\n        if output:\n            return output\n\n    return Document(None)\n</code></pre>"},{"location":"reference/llms/cot/","title":"Cot","text":""},{"location":"reference/llms/cot/#llms.cot.Thought","title":"Thought","text":"<p>             Bases: <code>BaseComponent</code></p> <p>A thought in the chain of thought</p> <ul> <li>Input: <code>**kwargs</code> pairs, where key is the placeholder in the prompt, and value is the value.</li> <li>Output: an output dictionary</li> </ul> <p>Usage:</p> <p>Create and run a thought:</p> <pre><code>&gt;&gt; from kotaemon.pipelines.cot import Thought\n&gt;&gt; thought = Thought(\n     prompt=\"How to {action} {object}?\",\n     llm=AzureChatOpenAI(...),\n     post_process=lambda string: {\"tutorial\": string},\n   )\n&gt;&gt; output = thought(action=\"install\", object=\"python\")\n&gt;&gt; print(output)\n{'tutorial': 'As an AI language model,...'}\n</code></pre> <p>Basically, when a thought is run, it will:</p> <ol> <li>Populate the prompt template with the input <code>**kwargs</code>.</li> <li>Run the LLM model with the populated prompt.</li> <li>Post-process the LLM output with the post-processor.</li> </ol> <p>This <code>Thought</code> allows chaining sequentially with the + operator. For example:</p> <pre><code>&gt;&gt; llm = AzureChatOpenAI(...)\n&gt;&gt; thought1 = Thought(\n       prompt=\"Word {word} in {language} is \",\n       llm=llm,\n       post_process=lambda string: {\"translated\": string},\n   )\n&gt;&gt; thought2 = Thought(\n        prompt=\"Translate {translated} to Japanese\",\n        llm=llm,\n        post_process=lambda string: {\"output\": string},\n   )\n\n&gt;&gt; thought = thought1 + thought2\n&gt;&gt; thought(word=\"hello\", language=\"French\")\n{'word': 'hello',\n 'language': 'French',\n 'translated': '\"Bonjour\"',\n 'output': '\u3053\u3093\u306b\u3061\u306f (Konnichiwa)'}\n</code></pre> <p>Under the hood, when the <code>+</code> operator is used, a <code>ManualSequentialChainOfThought</code> is created.</p> Source code in <code>kotaemon\\llms\\cot.py</code> <pre><code>class Thought(BaseComponent):\n    \"\"\"A thought in the chain of thought\n\n    - Input: `**kwargs` pairs, where key is the placeholder in the prompt, and\n    value is the value.\n    - Output: an output dictionary\n\n    _**Usage:**_\n\n    Create and run a thought:\n\n    ```python\n    &gt;&gt; from kotaemon.pipelines.cot import Thought\n    &gt;&gt; thought = Thought(\n         prompt=\"How to {action} {object}?\",\n         llm=AzureChatOpenAI(...),\n         post_process=lambda string: {\"tutorial\": string},\n       )\n    &gt;&gt; output = thought(action=\"install\", object=\"python\")\n    &gt;&gt; print(output)\n    {'tutorial': 'As an AI language model,...'}\n    ```\n\n    Basically, when a thought is run, it will:\n\n    1. Populate the prompt template with the input `**kwargs`.\n    2. Run the LLM model with the populated prompt.\n    3. Post-process the LLM output with the post-processor.\n\n    This `Thought` allows chaining sequentially with the + operator. For example:\n\n    ```python\n    &gt;&gt; llm = AzureChatOpenAI(...)\n    &gt;&gt; thought1 = Thought(\n           prompt=\"Word {word} in {language} is \",\n           llm=llm,\n           post_process=lambda string: {\"translated\": string},\n       )\n    &gt;&gt; thought2 = Thought(\n            prompt=\"Translate {translated} to Japanese\",\n            llm=llm,\n            post_process=lambda string: {\"output\": string},\n       )\n\n    &gt;&gt; thought = thought1 + thought2\n    &gt;&gt; thought(word=\"hello\", language=\"French\")\n    {'word': 'hello',\n     'language': 'French',\n     'translated': '\"Bonjour\"',\n     'output': '\u3053\u3093\u306b\u3061\u306f (Konnichiwa)'}\n    ```\n\n    Under the hood, when the `+` operator is used, a `ManualSequentialChainOfThought`\n    is created.\n    \"\"\"\n\n    prompt: str = Param(\n        help=(\n            \"The prompt template string. This prompt template has Python-like \"\n            \"variable placeholders, that then will be subsituted with real values when \"\n            \"this component is executed\"\n        )\n    )\n    llm: LLM = Node(AzureChatOpenAI, help=\"The LLM model to execute the input prompt\")\n    post_process: Function = Node(\n        help=(\n            \"The function post-processor that post-processes LLM output prediction .\"\n            \"It should take a string as input (this is the LLM output text) and return \"\n            \"a dictionary, where the key should\"\n        )\n    )\n\n    @Node.auto(depends_on=\"prompt\")\n    def prompt_template(self):\n        \"\"\"Automatically wrap around param prompt. Can ignore\"\"\"\n        return BasePromptComponent(self.prompt)\n\n    def run(self, **kwargs) -&gt; Document:\n        \"\"\"Run the chain of thought\"\"\"\n        prompt = self.prompt_template(**kwargs).text\n        response = self.llm(prompt).text\n        response = self.post_process(response)\n\n        return Document(response)\n\n    def get_variables(self) -&gt; List[str]:\n        return []\n\n    def __add__(self, next_thought: \"Thought\") -&gt; \"ManualSequentialChainOfThought\":\n        return ManualSequentialChainOfThought(\n            thoughts=[self, next_thought], llm=self.llm\n        )\n</code></pre>"},{"location":"reference/llms/cot/#llms.cot.Thought.prompt_template","title":"prompt_template","text":"<pre><code>prompt_template()\n</code></pre> <p>Automatically wrap around param prompt. Can ignore</p> Source code in <code>kotaemon\\llms\\cot.py</code> <pre><code>@Node.auto(depends_on=\"prompt\")\ndef prompt_template(self):\n    \"\"\"Automatically wrap around param prompt. Can ignore\"\"\"\n    return BasePromptComponent(self.prompt)\n</code></pre>"},{"location":"reference/llms/cot/#llms.cot.Thought.run","title":"run","text":"<pre><code>run(**kwargs)\n</code></pre> <p>Run the chain of thought</p> Source code in <code>kotaemon\\llms\\cot.py</code> <pre><code>def run(self, **kwargs) -&gt; Document:\n    \"\"\"Run the chain of thought\"\"\"\n    prompt = self.prompt_template(**kwargs).text\n    response = self.llm(prompt).text\n    response = self.post_process(response)\n\n    return Document(response)\n</code></pre>"},{"location":"reference/llms/cot/#llms.cot.ManualSequentialChainOfThought","title":"ManualSequentialChainOfThought","text":"<p>             Bases: <code>BaseComponent</code></p> <p>Perform sequential chain-of-thought with manual pre-defined prompts</p> <p>This method supports variable number of steps. Each step corresponds to a <code>kotaemon.pipelines.cot.Thought</code>. Please refer that section for Thought's detail. This section is about chaining thought together.</p> <p>Usage:</p> <p>Create and run a chain of thought without \"+\" operator:</p> <pre><code>&gt;&gt;&gt; from kotaemon.pipelines.cot import Thought, ManualSequentialChainOfThought\n&gt;&gt;&gt; llm = AzureChatOpenAI(...)\n&gt;&gt;&gt; thought1 = Thought(\n&gt;&gt;&gt;    prompt=\"Word {word} in {language} is \",\n&gt;&gt;&gt;    post_process=lambda string: {\"translated\": string},\n&gt;&gt;&gt; )\n&gt;&gt;&gt; thought2 = Thought(\n&gt;&gt;&gt;     prompt=\"Translate {translated} to Japanese\",\n&gt;&gt;&gt;     post_process=lambda string: {\"output\": string},\n&gt;&gt;&gt; )\n&gt;&gt;&gt; thought = ManualSequentialChainOfThought(thoughts=[thought1, thought2], llm=llm)\n&gt;&gt;&gt; thought(word=\"hello\", language=\"French\")\n{'word': 'hello',\n 'language': 'French',\n 'translated': '\"Bonjour\"',\n 'output': '\u3053\u3093\u306b\u3061\u306f (Konnichiwa)'}\n</code></pre> <p>Create and run a chain of thought without \"+\" operator: Please refer the <code>kotaemon.pipelines.cot.Thought</code> section for examples.</p> <p>This chain-of-thought optionally takes a termination check callback function. This function will be called after each thought is executed. It takes in a dictionary of all thought outputs so far, and it returns True or False. If True, the chain-of-thought will terminate. If unset, the default callback always returns False.</p> Source code in <code>kotaemon\\llms\\cot.py</code> <pre><code>class ManualSequentialChainOfThought(BaseComponent):\n    \"\"\"Perform sequential chain-of-thought with manual pre-defined prompts\n\n    This method supports variable number of steps. Each step corresponds to a\n    `kotaemon.pipelines.cot.Thought`. Please refer that section for\n    Thought's detail. This section is about chaining thought together.\n\n    _**Usage:**_\n\n    **Create and run a chain of thought without \"+\" operator:**\n\n    ```pycon\n    &gt;&gt;&gt; from kotaemon.pipelines.cot import Thought, ManualSequentialChainOfThought\n    &gt;&gt;&gt; llm = AzureChatOpenAI(...)\n    &gt;&gt;&gt; thought1 = Thought(\n    &gt;&gt;&gt;    prompt=\"Word {word} in {language} is \",\n    &gt;&gt;&gt;    post_process=lambda string: {\"translated\": string},\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; thought2 = Thought(\n    &gt;&gt;&gt;     prompt=\"Translate {translated} to Japanese\",\n    &gt;&gt;&gt;     post_process=lambda string: {\"output\": string},\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; thought = ManualSequentialChainOfThought(thoughts=[thought1, thought2], llm=llm)\n    &gt;&gt;&gt; thought(word=\"hello\", language=\"French\")\n    {'word': 'hello',\n     'language': 'French',\n     'translated': '\"Bonjour\"',\n     'output': '\u3053\u3093\u306b\u3061\u306f (Konnichiwa)'}\n    ```\n\n    **Create and run a chain of thought without \"+\" operator:** Please refer the\n    `kotaemon.pipelines.cot.Thought` section for examples.\n\n    This chain-of-thought optionally takes a termination check callback function.\n    This function will be called after each thought is executed. It takes in a\n    dictionary of all thought outputs so far, and it returns True or False. If\n    True, the chain-of-thought will terminate. If unset, the default callback always\n    returns False.\n    \"\"\"\n\n    thoughts: List[Thought] = Param(\n        default_callback=lambda *_: [], help=\"List of Thought\"\n    )\n    llm: LLM = Param(help=\"The LLM model to use (base of kotaemon.llms.BaseLLM)\")\n    terminate: Callable = Param(\n        default=lambda _: False,\n        help=\"Callback on terminate condition. Default to always return False\",\n    )\n\n    def run(self, **kwargs) -&gt; Document:\n        \"\"\"Run the manual chain of thought\"\"\"\n\n        inputs = deepcopy(kwargs)\n        for idx, thought in enumerate(self.thoughts):\n            if self.llm:\n                thought.llm = self.llm\n            self._prepare_child(thought, f\"thought{idx}\")\n\n            output = thought(**inputs)\n            inputs.update(output.content)\n            if self.terminate(inputs):\n                break\n\n        return Document(inputs)\n\n    def __add__(self, next_thought: Thought) -&gt; \"ManualSequentialChainOfThought\":\n        return ManualSequentialChainOfThought(\n            thoughts=self.thoughts + [next_thought], llm=self.llm\n        )\n</code></pre>"},{"location":"reference/llms/cot/#llms.cot.ManualSequentialChainOfThought.run","title":"run","text":"<pre><code>run(**kwargs)\n</code></pre> <p>Run the manual chain of thought</p> Source code in <code>kotaemon\\llms\\cot.py</code> <pre><code>def run(self, **kwargs) -&gt; Document:\n    \"\"\"Run the manual chain of thought\"\"\"\n\n    inputs = deepcopy(kwargs)\n    for idx, thought in enumerate(self.thoughts):\n        if self.llm:\n            thought.llm = self.llm\n        self._prepare_child(thought, f\"thought{idx}\")\n\n        output = thought(**inputs)\n        inputs.update(output.content)\n        if self.terminate(inputs):\n            break\n\n    return Document(inputs)\n</code></pre>"},{"location":"reference/llms/linear/","title":"Linear","text":""},{"location":"reference/llms/linear/#llms.linear.SimpleLinearPipeline","title":"SimpleLinearPipeline","text":"<p>             Bases: <code>BaseComponent</code></p> <p>A simple pipeline for running a function with a prompt, a language model, and an     optional post-processor.</p> <p>Attributes:</p> Name Type Description <code>prompt</code> <code>BasePromptComponent</code> <p>The prompt component used to generate the initial input.</p> <code>llm</code> <code>Union[ChatLLM, LLM]</code> <p>The language model component used to generate the output.</p> <code>post_processor</code> <code>Union[BaseComponent, Callable[[IO_Type], IO_Type]]</code> <p>An optional post-processor component or function.</p> Example Usage <pre><code>from kotaemon.llms import AzureChatOpenAI, BasePromptComponent\n\ndef identity(x):\n    return x\n\nllm = AzureChatOpenAI(\n    openai_api_base=\"your openai api base\",\n    openai_api_key=\"your openai api key\",\n    openai_api_version=\"your openai api version\",\n    deployment_name=\"nlp-q2-gpt35\",\n    temperature=0,\n    request_timeout=600,\n)\n\npipeline = SimpleLinearPipeline(\n    prompt=BasePromptComponent(template=\"what is {word} in Japanese ?\"),\n    llm=llm,\n    post_processor=identity,\n)\nprint(pipeline(word=\"lone\"))\n</code></pre> Source code in <code>kotaemon\\llms\\linear.py</code> <pre><code>class SimpleLinearPipeline(BaseComponent):\n    \"\"\"\n    A simple pipeline for running a function with a prompt, a language model, and an\n        optional post-processor.\n\n    Attributes:\n        prompt (BasePromptComponent): The prompt component used to generate the initial\n            input.\n        llm (Union[ChatLLM, LLM]): The language model component used to generate the\n            output.\n        post_processor (Union[BaseComponent, Callable[[IO_Type], IO_Type]]): An optional\n            post-processor component or function.\n\n    Example Usage:\n        ```python\n        from kotaemon.llms import AzureChatOpenAI, BasePromptComponent\n\n        def identity(x):\n            return x\n\n        llm = AzureChatOpenAI(\n            openai_api_base=\"your openai api base\",\n            openai_api_key=\"your openai api key\",\n            openai_api_version=\"your openai api version\",\n            deployment_name=\"nlp-q2-gpt35\",\n            temperature=0,\n            request_timeout=600,\n        )\n\n        pipeline = SimpleLinearPipeline(\n            prompt=BasePromptComponent(template=\"what is {word} in Japanese ?\"),\n            llm=llm,\n            post_processor=identity,\n        )\n        print(pipeline(word=\"lone\"))\n        ```\n    \"\"\"\n\n    prompt: BasePromptComponent\n    llm: Union[ChatLLM, LLM]\n    post_processor: Union[BaseComponent, Callable[[IO_Type], IO_Type]]\n\n    def run(\n        self,\n        *,\n        llm_kwargs: Optional[dict] = {},\n        post_processor_kwargs: Optional[dict] = {},\n        **prompt_kwargs,\n    ):\n        \"\"\"\n        Run the function with the given arguments and return the final output as a\n            Document object.\n\n        Args:\n            llm_kwargs (dict): Keyword arguments for the llm call.\n            post_processor_kwargs (dict): Keyword arguments for the post_processor.\n            **prompt_kwargs: Keyword arguments for populating the prompt.\n\n        Returns:\n            Document: The final output of the function as a Document object.\n        \"\"\"\n        prompt = self.prompt(**prompt_kwargs)\n        llm_output = self.llm(prompt.text, **llm_kwargs)\n        if self.post_processor is not None:\n            final_output = self.post_processor(llm_output, **post_processor_kwargs)[0]\n        else:\n            final_output = llm_output\n\n        return Document(final_output)\n</code></pre>"},{"location":"reference/llms/linear/#llms.linear.SimpleLinearPipeline.run","title":"run","text":"<pre><code>run(*, llm_kwargs={}, post_processor_kwargs={}, **prompt_kwargs)\n</code></pre> <p>Run the function with the given arguments and return the final output as a     Document object.</p> <p>Parameters:</p> Name Type Description Default <code>llm_kwargs</code> <code>dict</code> <p>Keyword arguments for the llm call.</p> <code>{}</code> <code>post_processor_kwargs</code> <code>dict</code> <p>Keyword arguments for the post_processor.</p> <code>{}</code> <code>**prompt_kwargs</code> <p>Keyword arguments for populating the prompt.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Document</code> <p>The final output of the function as a Document object.</p> Source code in <code>kotaemon\\llms\\linear.py</code> <pre><code>def run(\n    self,\n    *,\n    llm_kwargs: Optional[dict] = {},\n    post_processor_kwargs: Optional[dict] = {},\n    **prompt_kwargs,\n):\n    \"\"\"\n    Run the function with the given arguments and return the final output as a\n        Document object.\n\n    Args:\n        llm_kwargs (dict): Keyword arguments for the llm call.\n        post_processor_kwargs (dict): Keyword arguments for the post_processor.\n        **prompt_kwargs: Keyword arguments for populating the prompt.\n\n    Returns:\n        Document: The final output of the function as a Document object.\n    \"\"\"\n    prompt = self.prompt(**prompt_kwargs)\n    llm_output = self.llm(prompt.text, **llm_kwargs)\n    if self.post_processor is not None:\n        final_output = self.post_processor(llm_output, **post_processor_kwargs)[0]\n    else:\n        final_output = llm_output\n\n    return Document(final_output)\n</code></pre>"},{"location":"reference/llms/linear/#llms.linear.GatedLinearPipeline","title":"GatedLinearPipeline","text":"<p>             Bases: <code>SimpleLinearPipeline</code></p> <p>A pipeline that extends the SimpleLinearPipeline class and adds a condition     attribute.</p> <p>Attributes:</p> Name Type Description <code>condition</code> <code>Callable[[IO_Type], Any]</code> <p>A callable function that represents the condition.</p> Usage Example Usage<pre><code>from kotaemon.llms import AzureChatOpenAI, BasePromptComponent\nfrom kotaemon.parsers import RegexExtractor\n\ndef identity(x):\n    return x\n\nllm = AzureChatOpenAI(\n    openai_api_base=\"your openai api base\",\n    openai_api_key=\"your openai api key\",\n    openai_api_version=\"your openai api version\",\n    deployment_name=\"nlp-q2-gpt35\",\n    temperature=0,\n    request_timeout=600,\n)\n\npipeline = GatedLinearPipeline(\n    prompt=BasePromptComponent(template=\"what is {word} in Japanese ?\"),\n    condition=RegexExtractor(pattern=\"some pattern\"),\n    llm=llm,\n    post_processor=identity,\n)\nprint(pipeline(condition_text=\"some pattern\", word=\"lone\"))\nprint(pipeline(condition_text=\"other pattern\", word=\"lone\"))\n</code></pre> Source code in <code>kotaemon\\llms\\linear.py</code> <pre><code>class GatedLinearPipeline(SimpleLinearPipeline):\n    \"\"\"\n    A pipeline that extends the SimpleLinearPipeline class and adds a condition\n        attribute.\n\n    Attributes:\n        condition (Callable[[IO_Type], Any]): A callable function that represents the\n            condition.\n\n    Usage:\n        ```{.py3 title=\"Example Usage\"}\n        from kotaemon.llms import AzureChatOpenAI, BasePromptComponent\n        from kotaemon.parsers import RegexExtractor\n\n        def identity(x):\n            return x\n\n        llm = AzureChatOpenAI(\n            openai_api_base=\"your openai api base\",\n            openai_api_key=\"your openai api key\",\n            openai_api_version=\"your openai api version\",\n            deployment_name=\"nlp-q2-gpt35\",\n            temperature=0,\n            request_timeout=600,\n        )\n\n        pipeline = GatedLinearPipeline(\n            prompt=BasePromptComponent(template=\"what is {word} in Japanese ?\"),\n            condition=RegexExtractor(pattern=\"some pattern\"),\n            llm=llm,\n            post_processor=identity,\n        )\n        print(pipeline(condition_text=\"some pattern\", word=\"lone\"))\n        print(pipeline(condition_text=\"other pattern\", word=\"lone\"))\n        ```\n    \"\"\"\n\n    condition: Callable[[IO_Type], Any]\n\n    def run(\n        self,\n        *,\n        condition_text: Optional[str] = None,\n        llm_kwargs: Optional[dict] = {},\n        post_processor_kwargs: Optional[dict] = {},\n        **prompt_kwargs,\n    ) -&gt; Document:\n        \"\"\"\n        Run the pipeline with the given arguments and return the final output as a\n            Document object.\n\n        Args:\n            condition_text (str): The condition text to evaluate. Default to None.\n            llm_kwargs (dict): Additional keyword arguments for the language model call.\n            post_processor_kwargs (dict): Additional keyword arguments for the\n                post-processor.\n            **prompt_kwargs: Keyword arguments for populating the prompt.\n\n        Returns:\n            Document: The final output of the pipeline as a Document object.\n\n        Raises:\n            ValueError: If condition_text is None\n        \"\"\"\n        if condition_text is None:\n            raise ValueError(\"`condition_text` must be provided\")\n\n        if self.condition(condition_text)[0]:\n            return super().run(\n                llm_kwargs=llm_kwargs,\n                post_processor_kwargs=post_processor_kwargs,\n                **prompt_kwargs,\n            )\n\n        return Document(None)\n</code></pre>"},{"location":"reference/llms/linear/#llms.linear.GatedLinearPipeline.run","title":"run","text":"<pre><code>run(*, condition_text=None, llm_kwargs={}, post_processor_kwargs={}, **prompt_kwargs)\n</code></pre> <p>Run the pipeline with the given arguments and return the final output as a     Document object.</p> <p>Parameters:</p> Name Type Description Default <code>condition_text</code> <code>str</code> <p>The condition text to evaluate. Default to None.</p> <code>None</code> <code>llm_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model call.</p> <code>{}</code> <code>post_processor_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the post-processor.</p> <code>{}</code> <code>**prompt_kwargs</code> <p>Keyword arguments for populating the prompt.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Document</code> <code>Document</code> <p>The final output of the pipeline as a Document object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If condition_text is None</p> Source code in <code>kotaemon\\llms\\linear.py</code> <pre><code>def run(\n    self,\n    *,\n    condition_text: Optional[str] = None,\n    llm_kwargs: Optional[dict] = {},\n    post_processor_kwargs: Optional[dict] = {},\n    **prompt_kwargs,\n) -&gt; Document:\n    \"\"\"\n    Run the pipeline with the given arguments and return the final output as a\n        Document object.\n\n    Args:\n        condition_text (str): The condition text to evaluate. Default to None.\n        llm_kwargs (dict): Additional keyword arguments for the language model call.\n        post_processor_kwargs (dict): Additional keyword arguments for the\n            post-processor.\n        **prompt_kwargs: Keyword arguments for populating the prompt.\n\n    Returns:\n        Document: The final output of the pipeline as a Document object.\n\n    Raises:\n        ValueError: If condition_text is None\n    \"\"\"\n    if condition_text is None:\n        raise ValueError(\"`condition_text` must be provided\")\n\n    if self.condition(condition_text)[0]:\n        return super().run(\n            llm_kwargs=llm_kwargs,\n            post_processor_kwargs=post_processor_kwargs,\n            **prompt_kwargs,\n        )\n\n    return Document(None)\n</code></pre>"},{"location":"reference/llms/chats/","title":"Chats","text":""},{"location":"reference/llms/chats/#llms.chats.LCChatMixin","title":"LCChatMixin","text":"Source code in <code>kotaemon\\llms\\chats\\langchain_based.py</code> <pre><code>class LCChatMixin:\n    def _get_lc_class(self):\n        raise NotImplementedError(\n            \"Please return the relevant Langchain class in in _get_lc_class\"\n        )\n\n    def __init__(self, **params):\n        self._lc_class = self._get_lc_class()\n        self._obj = self._lc_class(**params)\n        self._kwargs: dict = params\n\n        super().__init__()\n\n    def run(\n        self, messages: str | BaseMessage | list[BaseMessage], **kwargs\n    ) -&gt; LLMInterface:\n        \"\"\"Generate response from messages\n\n        Args:\n            messages: history of messages to generate response from\n            **kwargs: additional arguments to pass to the langchain chat model\n\n        Returns:\n            LLMInterface: generated response\n        \"\"\"\n        input_: list[BaseMessage] = []\n\n        if isinstance(messages, str):\n            input_ = [HumanMessage(content=messages)]\n        elif isinstance(messages, BaseMessage):\n            input_ = [messages]\n        else:\n            input_ = messages\n\n        pred = self._obj.generate(messages=[input_], **kwargs)\n        all_text = [each.text for each in pred.generations[0]]\n        all_messages = [each.message for each in pred.generations[0]]\n\n        completion_tokens, total_tokens, prompt_tokens = 0, 0, 0\n        try:\n            if pred.llm_output is not None:\n                completion_tokens = pred.llm_output[\"token_usage\"][\"completion_tokens\"]\n                total_tokens = pred.llm_output[\"token_usage\"][\"total_tokens\"]\n                prompt_tokens = pred.llm_output[\"token_usage\"][\"prompt_tokens\"]\n        except Exception:\n            logger.warning(\n                f\"Cannot get token usage from LLM output for {self._lc_class.__name__}\"\n            )\n\n        return LLMInterface(\n            text=all_text[0] if len(all_text) &gt; 0 else \"\",\n            candidates=all_text,\n            completion_tokens=completion_tokens,\n            total_tokens=total_tokens,\n            prompt_tokens=prompt_tokens,\n            messages=all_messages,\n            logits=[],\n        )\n\n    def __repr__(self):\n        kwargs = []\n        for key, value_obj in self._kwargs.items():\n            value = repr(value_obj)\n            kwargs.append(f\"{key}={value}\")\n        kwargs_repr = \", \".join(kwargs)\n        return f\"{self.__class__.__name__}({kwargs_repr})\"\n\n    def __str__(self):\n        kwargs = []\n        for key, value_obj in self._kwargs.items():\n            value = str(value_obj)\n            if len(value) &gt; 20:\n                value = f\"{value[:15]}...\"\n            kwargs.append(f\"{key}={value}\")\n        kwargs_repr = \", \".join(kwargs)\n        return f\"{self.__class__.__name__}({kwargs_repr})\"\n\n    def __setattr__(self, name, value):\n        if name == \"_lc_class\":\n            return super().__setattr__(name, value)\n\n        if name in self._lc_class.__fields__:\n            self._kwargs[name] = value\n            self._obj = self._lc_class(**self._kwargs)\n        else:\n            super().__setattr__(name, value)\n\n    def __getattr__(self, name):\n        if name in self._kwargs:\n            return self._kwargs[name]\n        return getattr(self._obj, name)\n\n    def dump(self, *args, **kwargs):\n        from theflow.utils.modules import serialize\n\n        params = {key: serialize(value) for key, value in self._kwargs.items()}\n        return {\n            \"__type__\": f\"{self.__module__}.{self.__class__.__qualname__}\",\n            **params,\n        }\n\n    def specs(self, path: str):\n        path = path.strip(\".\")\n        if \".\" in path:\n            raise ValueError(\"path should not contain '.'\")\n\n        if path in self._lc_class.__fields__:\n            return {\n                \"__type__\": \"theflow.base.ParamAttr\",\n                \"refresh_on_set\": True,\n                \"strict_type\": True,\n            }\n\n        raise ValueError(f\"Invalid param {path}\")\n</code></pre>"},{"location":"reference/llms/chats/#llms.chats.LCChatMixin.run","title":"run","text":"<pre><code>run(messages, **kwargs)\n</code></pre> <p>Generate response from messages</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>str | BaseMessage | list[BaseMessage]</code> <p>history of messages to generate response from</p> required <code>**kwargs</code> <p>additional arguments to pass to the langchain chat model</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>LLMInterface</code> <code>LLMInterface</code> <p>generated response</p> Source code in <code>kotaemon\\llms\\chats\\langchain_based.py</code> <pre><code>def run(\n    self, messages: str | BaseMessage | list[BaseMessage], **kwargs\n) -&gt; LLMInterface:\n    \"\"\"Generate response from messages\n\n    Args:\n        messages: history of messages to generate response from\n        **kwargs: additional arguments to pass to the langchain chat model\n\n    Returns:\n        LLMInterface: generated response\n    \"\"\"\n    input_: list[BaseMessage] = []\n\n    if isinstance(messages, str):\n        input_ = [HumanMessage(content=messages)]\n    elif isinstance(messages, BaseMessage):\n        input_ = [messages]\n    else:\n        input_ = messages\n\n    pred = self._obj.generate(messages=[input_], **kwargs)\n    all_text = [each.text for each in pred.generations[0]]\n    all_messages = [each.message for each in pred.generations[0]]\n\n    completion_tokens, total_tokens, prompt_tokens = 0, 0, 0\n    try:\n        if pred.llm_output is not None:\n            completion_tokens = pred.llm_output[\"token_usage\"][\"completion_tokens\"]\n            total_tokens = pred.llm_output[\"token_usage\"][\"total_tokens\"]\n            prompt_tokens = pred.llm_output[\"token_usage\"][\"prompt_tokens\"]\n    except Exception:\n        logger.warning(\n            f\"Cannot get token usage from LLM output for {self._lc_class.__name__}\"\n        )\n\n    return LLMInterface(\n        text=all_text[0] if len(all_text) &gt; 0 else \"\",\n        candidates=all_text,\n        completion_tokens=completion_tokens,\n        total_tokens=total_tokens,\n        prompt_tokens=prompt_tokens,\n        messages=all_messages,\n        logits=[],\n    )\n</code></pre>"},{"location":"reference/llms/chats/base/","title":"Base","text":""},{"location":"reference/llms/chats/langchain_based/","title":"Langchain Based","text":""},{"location":"reference/llms/chats/langchain_based/#llms.chats.langchain_based.LCChatMixin","title":"LCChatMixin","text":"Source code in <code>kotaemon\\llms\\chats\\langchain_based.py</code> <pre><code>class LCChatMixin:\n    def _get_lc_class(self):\n        raise NotImplementedError(\n            \"Please return the relevant Langchain class in in _get_lc_class\"\n        )\n\n    def __init__(self, **params):\n        self._lc_class = self._get_lc_class()\n        self._obj = self._lc_class(**params)\n        self._kwargs: dict = params\n\n        super().__init__()\n\n    def run(\n        self, messages: str | BaseMessage | list[BaseMessage], **kwargs\n    ) -&gt; LLMInterface:\n        \"\"\"Generate response from messages\n\n        Args:\n            messages: history of messages to generate response from\n            **kwargs: additional arguments to pass to the langchain chat model\n\n        Returns:\n            LLMInterface: generated response\n        \"\"\"\n        input_: list[BaseMessage] = []\n\n        if isinstance(messages, str):\n            input_ = [HumanMessage(content=messages)]\n        elif isinstance(messages, BaseMessage):\n            input_ = [messages]\n        else:\n            input_ = messages\n\n        pred = self._obj.generate(messages=[input_], **kwargs)\n        all_text = [each.text for each in pred.generations[0]]\n        all_messages = [each.message for each in pred.generations[0]]\n\n        completion_tokens, total_tokens, prompt_tokens = 0, 0, 0\n        try:\n            if pred.llm_output is not None:\n                completion_tokens = pred.llm_output[\"token_usage\"][\"completion_tokens\"]\n                total_tokens = pred.llm_output[\"token_usage\"][\"total_tokens\"]\n                prompt_tokens = pred.llm_output[\"token_usage\"][\"prompt_tokens\"]\n        except Exception:\n            logger.warning(\n                f\"Cannot get token usage from LLM output for {self._lc_class.__name__}\"\n            )\n\n        return LLMInterface(\n            text=all_text[0] if len(all_text) &gt; 0 else \"\",\n            candidates=all_text,\n            completion_tokens=completion_tokens,\n            total_tokens=total_tokens,\n            prompt_tokens=prompt_tokens,\n            messages=all_messages,\n            logits=[],\n        )\n\n    def __repr__(self):\n        kwargs = []\n        for key, value_obj in self._kwargs.items():\n            value = repr(value_obj)\n            kwargs.append(f\"{key}={value}\")\n        kwargs_repr = \", \".join(kwargs)\n        return f\"{self.__class__.__name__}({kwargs_repr})\"\n\n    def __str__(self):\n        kwargs = []\n        for key, value_obj in self._kwargs.items():\n            value = str(value_obj)\n            if len(value) &gt; 20:\n                value = f\"{value[:15]}...\"\n            kwargs.append(f\"{key}={value}\")\n        kwargs_repr = \", \".join(kwargs)\n        return f\"{self.__class__.__name__}({kwargs_repr})\"\n\n    def __setattr__(self, name, value):\n        if name == \"_lc_class\":\n            return super().__setattr__(name, value)\n\n        if name in self._lc_class.__fields__:\n            self._kwargs[name] = value\n            self._obj = self._lc_class(**self._kwargs)\n        else:\n            super().__setattr__(name, value)\n\n    def __getattr__(self, name):\n        if name in self._kwargs:\n            return self._kwargs[name]\n        return getattr(self._obj, name)\n\n    def dump(self, *args, **kwargs):\n        from theflow.utils.modules import serialize\n\n        params = {key: serialize(value) for key, value in self._kwargs.items()}\n        return {\n            \"__type__\": f\"{self.__module__}.{self.__class__.__qualname__}\",\n            **params,\n        }\n\n    def specs(self, path: str):\n        path = path.strip(\".\")\n        if \".\" in path:\n            raise ValueError(\"path should not contain '.'\")\n\n        if path in self._lc_class.__fields__:\n            return {\n                \"__type__\": \"theflow.base.ParamAttr\",\n                \"refresh_on_set\": True,\n                \"strict_type\": True,\n            }\n\n        raise ValueError(f\"Invalid param {path}\")\n</code></pre>"},{"location":"reference/llms/chats/langchain_based/#llms.chats.langchain_based.LCChatMixin.run","title":"run","text":"<pre><code>run(messages, **kwargs)\n</code></pre> <p>Generate response from messages</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>str | BaseMessage | list[BaseMessage]</code> <p>history of messages to generate response from</p> required <code>**kwargs</code> <p>additional arguments to pass to the langchain chat model</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>LLMInterface</code> <code>LLMInterface</code> <p>generated response</p> Source code in <code>kotaemon\\llms\\chats\\langchain_based.py</code> <pre><code>def run(\n    self, messages: str | BaseMessage | list[BaseMessage], **kwargs\n) -&gt; LLMInterface:\n    \"\"\"Generate response from messages\n\n    Args:\n        messages: history of messages to generate response from\n        **kwargs: additional arguments to pass to the langchain chat model\n\n    Returns:\n        LLMInterface: generated response\n    \"\"\"\n    input_: list[BaseMessage] = []\n\n    if isinstance(messages, str):\n        input_ = [HumanMessage(content=messages)]\n    elif isinstance(messages, BaseMessage):\n        input_ = [messages]\n    else:\n        input_ = messages\n\n    pred = self._obj.generate(messages=[input_], **kwargs)\n    all_text = [each.text for each in pred.generations[0]]\n    all_messages = [each.message for each in pred.generations[0]]\n\n    completion_tokens, total_tokens, prompt_tokens = 0, 0, 0\n    try:\n        if pred.llm_output is not None:\n            completion_tokens = pred.llm_output[\"token_usage\"][\"completion_tokens\"]\n            total_tokens = pred.llm_output[\"token_usage\"][\"total_tokens\"]\n            prompt_tokens = pred.llm_output[\"token_usage\"][\"prompt_tokens\"]\n    except Exception:\n        logger.warning(\n            f\"Cannot get token usage from LLM output for {self._lc_class.__name__}\"\n        )\n\n    return LLMInterface(\n        text=all_text[0] if len(all_text) &gt; 0 else \"\",\n        candidates=all_text,\n        completion_tokens=completion_tokens,\n        total_tokens=total_tokens,\n        prompt_tokens=prompt_tokens,\n        messages=all_messages,\n        logits=[],\n    )\n</code></pre>"},{"location":"reference/llms/completions/","title":"Completions","text":""},{"location":"reference/llms/completions/#llms.completions.AzureOpenAI","title":"AzureOpenAI","text":"<p>             Bases: <code>LCCompletionMixin</code>, <code>LLM</code></p> <p>Wrapper around Langchain's AzureOpenAI class, focusing on key parameters</p> Source code in <code>kotaemon\\llms\\completions\\langchain_based.py</code> <pre><code>class AzureOpenAI(LCCompletionMixin, LLM):\n    \"\"\"Wrapper around Langchain's AzureOpenAI class, focusing on key parameters\"\"\"\n\n    def __init__(\n        self,\n        azure_endpoint: Optional[str] = None,\n        deployment_name: Optional[str] = None,\n        openai_api_version: str = \"\",\n        openai_api_key: Optional[str] = None,\n        model_name: str = \"text-davinci-003\",\n        temperature: float = 0.7,\n        max_tokens: int = 256,\n        top_p: float = 1,\n        frequency_penalty: float = 0,\n        n: int = 1,\n        best_of: int = 1,\n        request_timeout: Optional[float] = None,\n        max_retries: int = 2,\n        streaming: bool = False,\n        **params,\n    ):\n        super().__init__(\n            azure_endpoint=azure_endpoint,\n            deployment_name=deployment_name,\n            openai_api_version=openai_api_version,\n            openai_api_key=openai_api_key,\n            model_name=model_name,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            top_p=top_p,\n            frequency_penalty=frequency_penalty,\n            n=n,\n            best_of=best_of,\n            request_timeout=request_timeout,\n            max_retries=max_retries,\n            streaming=streaming,\n            **params,\n        )\n\n    def _get_lc_class(self):\n        import langchain.llms as langchain_llms\n\n        return langchain_llms.AzureOpenAI\n</code></pre>"},{"location":"reference/llms/completions/#llms.completions.OpenAI","title":"OpenAI","text":"<p>             Bases: <code>LCCompletionMixin</code>, <code>LLM</code></p> <p>Wrapper around Langchain's OpenAI class, focusing on key parameters</p> Source code in <code>kotaemon\\llms\\completions\\langchain_based.py</code> <pre><code>class OpenAI(LCCompletionMixin, LLM):\n    \"\"\"Wrapper around Langchain's OpenAI class, focusing on key parameters\"\"\"\n\n    def __init__(\n        self,\n        openai_api_key: Optional[str] = None,\n        openai_api_base: Optional[str] = None,\n        model_name: str = \"text-davinci-003\",\n        temperature: float = 0.7,\n        max_tokens: int = 256,\n        top_p: float = 1,\n        frequency_penalty: float = 0,\n        n: int = 1,\n        best_of: int = 1,\n        request_timeout: Optional[float] = None,\n        max_retries: int = 2,\n        streaming: bool = False,\n        **params,\n    ):\n        super().__init__(\n            openai_api_key=openai_api_key,\n            openai_api_base=openai_api_base,\n            model_name=model_name,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            top_p=top_p,\n            frequency_penalty=frequency_penalty,\n            n=n,\n            best_of=best_of,\n            request_timeout=request_timeout,\n            max_retries=max_retries,\n            streaming=streaming,\n            **params,\n        )\n\n    def _get_lc_class(self):\n        import langchain.llms as langchain_llms\n\n        return langchain_llms.OpenAI\n</code></pre>"},{"location":"reference/llms/completions/base/","title":"Base","text":""},{"location":"reference/llms/completions/langchain_based/","title":"Langchain Based","text":""},{"location":"reference/llms/completions/langchain_based/#llms.completions.langchain_based.OpenAI","title":"OpenAI","text":"<p>             Bases: <code>LCCompletionMixin</code>, <code>LLM</code></p> <p>Wrapper around Langchain's OpenAI class, focusing on key parameters</p> Source code in <code>kotaemon\\llms\\completions\\langchain_based.py</code> <pre><code>class OpenAI(LCCompletionMixin, LLM):\n    \"\"\"Wrapper around Langchain's OpenAI class, focusing on key parameters\"\"\"\n\n    def __init__(\n        self,\n        openai_api_key: Optional[str] = None,\n        openai_api_base: Optional[str] = None,\n        model_name: str = \"text-davinci-003\",\n        temperature: float = 0.7,\n        max_tokens: int = 256,\n        top_p: float = 1,\n        frequency_penalty: float = 0,\n        n: int = 1,\n        best_of: int = 1,\n        request_timeout: Optional[float] = None,\n        max_retries: int = 2,\n        streaming: bool = False,\n        **params,\n    ):\n        super().__init__(\n            openai_api_key=openai_api_key,\n            openai_api_base=openai_api_base,\n            model_name=model_name,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            top_p=top_p,\n            frequency_penalty=frequency_penalty,\n            n=n,\n            best_of=best_of,\n            request_timeout=request_timeout,\n            max_retries=max_retries,\n            streaming=streaming,\n            **params,\n        )\n\n    def _get_lc_class(self):\n        import langchain.llms as langchain_llms\n\n        return langchain_llms.OpenAI\n</code></pre>"},{"location":"reference/llms/completions/langchain_based/#llms.completions.langchain_based.AzureOpenAI","title":"AzureOpenAI","text":"<p>             Bases: <code>LCCompletionMixin</code>, <code>LLM</code></p> <p>Wrapper around Langchain's AzureOpenAI class, focusing on key parameters</p> Source code in <code>kotaemon\\llms\\completions\\langchain_based.py</code> <pre><code>class AzureOpenAI(LCCompletionMixin, LLM):\n    \"\"\"Wrapper around Langchain's AzureOpenAI class, focusing on key parameters\"\"\"\n\n    def __init__(\n        self,\n        azure_endpoint: Optional[str] = None,\n        deployment_name: Optional[str] = None,\n        openai_api_version: str = \"\",\n        openai_api_key: Optional[str] = None,\n        model_name: str = \"text-davinci-003\",\n        temperature: float = 0.7,\n        max_tokens: int = 256,\n        top_p: float = 1,\n        frequency_penalty: float = 0,\n        n: int = 1,\n        best_of: int = 1,\n        request_timeout: Optional[float] = None,\n        max_retries: int = 2,\n        streaming: bool = False,\n        **params,\n    ):\n        super().__init__(\n            azure_endpoint=azure_endpoint,\n            deployment_name=deployment_name,\n            openai_api_version=openai_api_version,\n            openai_api_key=openai_api_key,\n            model_name=model_name,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            top_p=top_p,\n            frequency_penalty=frequency_penalty,\n            n=n,\n            best_of=best_of,\n            request_timeout=request_timeout,\n            max_retries=max_retries,\n            streaming=streaming,\n            **params,\n        )\n\n    def _get_lc_class(self):\n        import langchain.llms as langchain_llms\n\n        return langchain_llms.AzureOpenAI\n</code></pre>"},{"location":"reference/llms/prompts/","title":"Prompts","text":""},{"location":"reference/llms/prompts/#llms.prompts.BasePromptComponent","title":"BasePromptComponent","text":"<p>             Bases: <code>BaseComponent</code></p> <p>Base class for prompt components.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>PromptTemplate</code> <p>The prompt template.</p> required <code>**kwargs</code> <p>Any additional keyword arguments that will be used to populate the given template.</p> <code>{}</code> Source code in <code>kotaemon\\llms\\prompts\\base.py</code> <pre><code>class BasePromptComponent(BaseComponent):\n    \"\"\"\n    Base class for prompt components.\n\n    Args:\n        template (PromptTemplate): The prompt template.\n        **kwargs: Any additional keyword arguments that will be used to populate the\n            given template.\n    \"\"\"\n\n    class Config:\n        middleware_switches = {\"theflow.middleware.CachingMiddleware\": False}\n        allow_extra = True\n\n    def __init__(self, template: Union[str, PromptTemplate], **kwargs):\n        super().__init__()\n        self.template = (\n            template\n            if isinstance(template, PromptTemplate)\n            else PromptTemplate(template)\n        )\n\n        self.__set(**kwargs)\n\n    def __check_redundant_kwargs(self, **kwargs):\n        \"\"\"\n        Check for redundant keyword arguments.\n\n        Parameters:\n            **kwargs (dict): A dictionary of keyword arguments.\n\n        Raises:\n            ValueError: If any keys provided are not in the template.\n\n        Returns:\n            None\n        \"\"\"\n        self.template.check_redundant_kwargs(**kwargs)\n\n    def __check_unset_placeholders(self):\n        \"\"\"\n        Check if all the placeholders in the template are set.\n\n        This function checks if all the expected placeholders in the template are set as\n            attributes of the object. If any placeholders are missing, a `ValueError`\n            is raised with the names of the missing keys.\n\n        Parameters:\n            None\n\n        Returns:\n            None\n        \"\"\"\n        self.template.check_missing_kwargs(**self.__dict__)\n\n    def __validate_value_type(self, **kwargs):\n        \"\"\"\n        Validates the value types of the given keyword arguments.\n\n        Parameters:\n            **kwargs (dict): A dictionary of keyword arguments to be validated.\n\n        Raises:\n            ValueError: If any of the values in the kwargs dictionary have an\n                unsupported type.\n\n        Returns:\n            None\n        \"\"\"\n        type_error = []\n        for k, v in kwargs.items():\n            if not isinstance(v, (str, int, Document, Callable)):  # type: ignore\n                type_error.append((k, type(v)))\n\n        if type_error:\n            raise ValueError(\n                \"Type of values must be either int, str, Document, Callable, \"\n                f\"found unsupported type for (key, type): {type_error}\"\n            )\n\n    def __set(self, **kwargs):\n        \"\"\"\n        Set the values of the attributes in the object based on the provided keyword\n            arguments.\n\n        Args:\n            kwargs (dict): A dictionary with the attribute names as keys and the new\n                values as values.\n\n        Returns:\n            None\n        \"\"\"\n        self.__check_redundant_kwargs(**kwargs)\n        self.__validate_value_type(**kwargs)\n\n        self.__dict__.update(kwargs)\n\n    def __prepare_value(self):\n        \"\"\"\n        Generate a dictionary of keyword arguments based on the template's placeholders\n            and the current instance's attributes.\n\n        Returns:\n            dict: A dictionary of keyword arguments.\n        \"\"\"\n\n        def __prepare(key, value):\n            if isinstance(value, str):\n                return value\n            if isinstance(value, (int, Document)):\n                return str(value)\n\n            raise ValueError(\n                f\"Unsupported type {type(value)} for template value of key {key}\"\n            )\n\n        kwargs = {}\n        for k in self.template.placeholders:\n            v = getattr(self, k)\n\n            # if get a callable, execute to get its output\n            if isinstance(v, Callable):  # type: ignore[arg-type]\n                v = v()\n\n            if isinstance(v, list):\n                v = str([__prepare(k, each) for each in v])\n            elif isinstance(v, (str, int, Document)):\n                v = __prepare(k, v)\n            else:\n                raise ValueError(\n                    f\"Unsupported type {type(v)} for template value of key `{k}`\"\n                )\n            kwargs[k] = v\n\n        return kwargs\n\n    def set(self, **kwargs):\n        \"\"\"\n        Similar to `__set` but for external use.\n\n        Set the values of the attributes in the object based on the provided keyword\n            arguments.\n\n        Args:\n            kwargs (dict): A dictionary with the attribute names as keys and the new\n                values as values.\n\n        Returns:\n            None\n        \"\"\"\n        self.__set(**kwargs)\n\n    def run(self, **kwargs):\n        \"\"\"\n        Run the function with the given keyword arguments.\n\n        Args:\n            **kwargs: The keyword arguments to pass to the function.\n\n        Returns:\n            The result of calling the `populate` method of the `template` object\n            with the given keyword arguments.\n        \"\"\"\n        self.__set(**kwargs)\n        self.__check_unset_placeholders()\n        prepared_kwargs = self.__prepare_value()\n\n        text = self.template.populate(**prepared_kwargs)\n        return Document(text=text, metadata={\"origin\": \"PromptComponent\"})\n\n    def flow(self):\n        return self.__call__()\n</code></pre>"},{"location":"reference/llms/prompts/#llms.prompts.BasePromptComponent.set","title":"set","text":"<pre><code>set(**kwargs)\n</code></pre> <p>Similar to <code>__set</code> but for external use.</p> <p>Set the values of the attributes in the object based on the provided keyword     arguments.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>dict</code> <p>A dictionary with the attribute names as keys and the new values as values.</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>kotaemon\\llms\\prompts\\base.py</code> <pre><code>def set(self, **kwargs):\n    \"\"\"\n    Similar to `__set` but for external use.\n\n    Set the values of the attributes in the object based on the provided keyword\n        arguments.\n\n    Args:\n        kwargs (dict): A dictionary with the attribute names as keys and the new\n            values as values.\n\n    Returns:\n        None\n    \"\"\"\n    self.__set(**kwargs)\n</code></pre>"},{"location":"reference/llms/prompts/#llms.prompts.BasePromptComponent.run","title":"run","text":"<pre><code>run(**kwargs)\n</code></pre> <p>Run the function with the given keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>The keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The result of calling the <code>populate</code> method of the <code>template</code> object</p> <p>with the given keyword arguments.</p> Source code in <code>kotaemon\\llms\\prompts\\base.py</code> <pre><code>def run(self, **kwargs):\n    \"\"\"\n    Run the function with the given keyword arguments.\n\n    Args:\n        **kwargs: The keyword arguments to pass to the function.\n\n    Returns:\n        The result of calling the `populate` method of the `template` object\n        with the given keyword arguments.\n    \"\"\"\n    self.__set(**kwargs)\n    self.__check_unset_placeholders()\n    prepared_kwargs = self.__prepare_value()\n\n    text = self.template.populate(**prepared_kwargs)\n    return Document(text=text, metadata={\"origin\": \"PromptComponent\"})\n</code></pre>"},{"location":"reference/llms/prompts/#llms.prompts.PromptTemplate","title":"PromptTemplate","text":"<p>Base class for prompt templates.</p> Source code in <code>kotaemon\\llms\\prompts\\template.py</code> <pre><code>class PromptTemplate:\n    \"\"\"\n    Base class for prompt templates.\n    \"\"\"\n\n    def __init__(self, template: str, ignore_invalid=True):\n        template = template\n        formatter = Formatter()\n        parsed_template = list(formatter.parse(template))\n\n        placeholders = set()\n        for _, key, _, _ in parsed_template:\n            if key is None:\n                continue\n            if not key.isidentifier():\n                if ignore_invalid:\n                    warnings.warn(f\"Ignore invalid placeholder: {key}.\", UserWarning)\n                else:\n                    raise ValueError(\n                        \"Placeholder name must be a valid Python identifier, found:\"\n                        f\" {key}.\"\n                    )\n            placeholders.add(key)\n\n        self.template = template\n        self.placeholders = placeholders\n        self.__formatter = formatter\n        self.__parsed_template = parsed_template\n\n    def check_missing_kwargs(self, **kwargs):\n        \"\"\"\n        Check if all the placeholders in the template are set.\n\n        This function checks if all the expected placeholders in the template are set as\n            attributes of the object. If any placeholders are missing, a `ValueError`\n            is raised with the names of the missing keys.\n\n        Parameters:\n            None\n\n        Returns:\n            None\n        \"\"\"\n        missing_keys = self.placeholders.difference(kwargs.keys())\n        if missing_keys:\n            raise ValueError(f\"Missing keys in template: {','.join(missing_keys)}\")\n\n    def check_redundant_kwargs(self, **kwargs):\n        \"\"\"\n        Check if all the placeholders in the template are set.\n\n        This function checks if all the expected placeholders in the template are set as\n            attributes of the object. If any placeholders are missing, a `ValueError`\n            is raised with the names of the missing keys.\n\n        Parameters:\n            None\n\n        Returns:\n            None\n        \"\"\"\n        provided_keys = set(kwargs.keys())\n        redundant_keys = provided_keys - self.placeholders\n\n        if redundant_keys:\n            warnings.warn(\n                f\"Keys provided but not in template: {','.join(redundant_keys)}\",\n                UserWarning,\n            )\n\n    def populate(self, **kwargs) -&gt; str:\n        \"\"\"\n        Strictly populate the template with the given keyword arguments.\n\n        Args:\n            **kwargs: The keyword arguments to populate the template.\n                      Each keyword corresponds to a placeholder in the template.\n\n        Returns:\n            The populated template.\n\n        Raises:\n            ValueError: If an unknown placeholder is provided.\n        \"\"\"\n        self.check_missing_kwargs(**kwargs)\n\n        return self.partial_populate(**kwargs)\n\n    def partial_populate(self, **kwargs):\n        \"\"\"\n        Partially populate the template with the given keyword arguments.\n\n        Args:\n            **kwargs: The keyword arguments to populate the template.\n                      Each keyword corresponds to a placeholder in the template.\n\n        Returns:\n            str: The populated template.\n        \"\"\"\n        self.check_redundant_kwargs(**kwargs)\n\n        prompt = []\n        for literal_text, field_name, format_spec, conversion in self.__parsed_template:\n            prompt.append(literal_text)\n\n            if field_name is None:\n                continue\n\n            if field_name not in kwargs:\n                if conversion:\n                    value = f\"{{{field_name}}}!{conversion}:{format_spec}\"\n                else:\n                    value = f\"{{{field_name}:{format_spec}}}\"\n            else:\n                value = kwargs[field_name]\n                if conversion is not None:\n                    value = self.__formatter.convert_field(value, conversion)\n                if format_spec is not None:\n                    value = self.__formatter.format_field(value, format_spec)\n\n            prompt.append(value)\n\n        return \"\".join(prompt)\n\n    def __add__(self, other):\n        \"\"\"\n        Create a new PromptTemplate object by concatenating the template of the current\n            object with the template of another PromptTemplate object.\n\n        Parameters:\n            other (PromptTemplate): Another PromptTemplate object.\n\n        Returns:\n            PromptTemplate: A new PromptTemplate object with the concatenated templates.\n        \"\"\"\n        return PromptTemplate(self.template + \"\\n\" + other.template)\n</code></pre>"},{"location":"reference/llms/prompts/#llms.prompts.PromptTemplate.check_missing_kwargs","title":"check_missing_kwargs","text":"<pre><code>check_missing_kwargs(**kwargs)\n</code></pre> <p>Check if all the placeholders in the template are set.</p> <p>This function checks if all the expected placeholders in the template are set as     attributes of the object. If any placeholders are missing, a <code>ValueError</code>     is raised with the names of the missing keys.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>kotaemon\\llms\\prompts\\template.py</code> <pre><code>def check_missing_kwargs(self, **kwargs):\n    \"\"\"\n    Check if all the placeholders in the template are set.\n\n    This function checks if all the expected placeholders in the template are set as\n        attributes of the object. If any placeholders are missing, a `ValueError`\n        is raised with the names of the missing keys.\n\n    Parameters:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    missing_keys = self.placeholders.difference(kwargs.keys())\n    if missing_keys:\n        raise ValueError(f\"Missing keys in template: {','.join(missing_keys)}\")\n</code></pre>"},{"location":"reference/llms/prompts/#llms.prompts.PromptTemplate.check_redundant_kwargs","title":"check_redundant_kwargs","text":"<pre><code>check_redundant_kwargs(**kwargs)\n</code></pre> <p>Check if all the placeholders in the template are set.</p> <p>This function checks if all the expected placeholders in the template are set as     attributes of the object. If any placeholders are missing, a <code>ValueError</code>     is raised with the names of the missing keys.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>kotaemon\\llms\\prompts\\template.py</code> <pre><code>def check_redundant_kwargs(self, **kwargs):\n    \"\"\"\n    Check if all the placeholders in the template are set.\n\n    This function checks if all the expected placeholders in the template are set as\n        attributes of the object. If any placeholders are missing, a `ValueError`\n        is raised with the names of the missing keys.\n\n    Parameters:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    provided_keys = set(kwargs.keys())\n    redundant_keys = provided_keys - self.placeholders\n\n    if redundant_keys:\n        warnings.warn(\n            f\"Keys provided but not in template: {','.join(redundant_keys)}\",\n            UserWarning,\n        )\n</code></pre>"},{"location":"reference/llms/prompts/#llms.prompts.PromptTemplate.populate","title":"populate","text":"<pre><code>populate(**kwargs)\n</code></pre> <p>Strictly populate the template with the given keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>The keyword arguments to populate the template.       Each keyword corresponds to a placeholder in the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The populated template.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unknown placeholder is provided.</p> Source code in <code>kotaemon\\llms\\prompts\\template.py</code> <pre><code>def populate(self, **kwargs) -&gt; str:\n    \"\"\"\n    Strictly populate the template with the given keyword arguments.\n\n    Args:\n        **kwargs: The keyword arguments to populate the template.\n                  Each keyword corresponds to a placeholder in the template.\n\n    Returns:\n        The populated template.\n\n    Raises:\n        ValueError: If an unknown placeholder is provided.\n    \"\"\"\n    self.check_missing_kwargs(**kwargs)\n\n    return self.partial_populate(**kwargs)\n</code></pre>"},{"location":"reference/llms/prompts/#llms.prompts.PromptTemplate.partial_populate","title":"partial_populate","text":"<pre><code>partial_populate(**kwargs)\n</code></pre> <p>Partially populate the template with the given keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>The keyword arguments to populate the template.       Each keyword corresponds to a placeholder in the template.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The populated template.</p> Source code in <code>kotaemon\\llms\\prompts\\template.py</code> <pre><code>def partial_populate(self, **kwargs):\n    \"\"\"\n    Partially populate the template with the given keyword arguments.\n\n    Args:\n        **kwargs: The keyword arguments to populate the template.\n                  Each keyword corresponds to a placeholder in the template.\n\n    Returns:\n        str: The populated template.\n    \"\"\"\n    self.check_redundant_kwargs(**kwargs)\n\n    prompt = []\n    for literal_text, field_name, format_spec, conversion in self.__parsed_template:\n        prompt.append(literal_text)\n\n        if field_name is None:\n            continue\n\n        if field_name not in kwargs:\n            if conversion:\n                value = f\"{{{field_name}}}!{conversion}:{format_spec}\"\n            else:\n                value = f\"{{{field_name}:{format_spec}}}\"\n        else:\n            value = kwargs[field_name]\n            if conversion is not None:\n                value = self.__formatter.convert_field(value, conversion)\n            if format_spec is not None:\n                value = self.__formatter.format_field(value, format_spec)\n\n        prompt.append(value)\n\n    return \"\".join(prompt)\n</code></pre>"},{"location":"reference/llms/prompts/base/","title":"Base","text":""},{"location":"reference/llms/prompts/base/#llms.prompts.base.BasePromptComponent","title":"BasePromptComponent","text":"<p>             Bases: <code>BaseComponent</code></p> <p>Base class for prompt components.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>PromptTemplate</code> <p>The prompt template.</p> required <code>**kwargs</code> <p>Any additional keyword arguments that will be used to populate the given template.</p> <code>{}</code> Source code in <code>kotaemon\\llms\\prompts\\base.py</code> <pre><code>class BasePromptComponent(BaseComponent):\n    \"\"\"\n    Base class for prompt components.\n\n    Args:\n        template (PromptTemplate): The prompt template.\n        **kwargs: Any additional keyword arguments that will be used to populate the\n            given template.\n    \"\"\"\n\n    class Config:\n        middleware_switches = {\"theflow.middleware.CachingMiddleware\": False}\n        allow_extra = True\n\n    def __init__(self, template: Union[str, PromptTemplate], **kwargs):\n        super().__init__()\n        self.template = (\n            template\n            if isinstance(template, PromptTemplate)\n            else PromptTemplate(template)\n        )\n\n        self.__set(**kwargs)\n\n    def __check_redundant_kwargs(self, **kwargs):\n        \"\"\"\n        Check for redundant keyword arguments.\n\n        Parameters:\n            **kwargs (dict): A dictionary of keyword arguments.\n\n        Raises:\n            ValueError: If any keys provided are not in the template.\n\n        Returns:\n            None\n        \"\"\"\n        self.template.check_redundant_kwargs(**kwargs)\n\n    def __check_unset_placeholders(self):\n        \"\"\"\n        Check if all the placeholders in the template are set.\n\n        This function checks if all the expected placeholders in the template are set as\n            attributes of the object. If any placeholders are missing, a `ValueError`\n            is raised with the names of the missing keys.\n\n        Parameters:\n            None\n\n        Returns:\n            None\n        \"\"\"\n        self.template.check_missing_kwargs(**self.__dict__)\n\n    def __validate_value_type(self, **kwargs):\n        \"\"\"\n        Validates the value types of the given keyword arguments.\n\n        Parameters:\n            **kwargs (dict): A dictionary of keyword arguments to be validated.\n\n        Raises:\n            ValueError: If any of the values in the kwargs dictionary have an\n                unsupported type.\n\n        Returns:\n            None\n        \"\"\"\n        type_error = []\n        for k, v in kwargs.items():\n            if not isinstance(v, (str, int, Document, Callable)):  # type: ignore\n                type_error.append((k, type(v)))\n\n        if type_error:\n            raise ValueError(\n                \"Type of values must be either int, str, Document, Callable, \"\n                f\"found unsupported type for (key, type): {type_error}\"\n            )\n\n    def __set(self, **kwargs):\n        \"\"\"\n        Set the values of the attributes in the object based on the provided keyword\n            arguments.\n\n        Args:\n            kwargs (dict): A dictionary with the attribute names as keys and the new\n                values as values.\n\n        Returns:\n            None\n        \"\"\"\n        self.__check_redundant_kwargs(**kwargs)\n        self.__validate_value_type(**kwargs)\n\n        self.__dict__.update(kwargs)\n\n    def __prepare_value(self):\n        \"\"\"\n        Generate a dictionary of keyword arguments based on the template's placeholders\n            and the current instance's attributes.\n\n        Returns:\n            dict: A dictionary of keyword arguments.\n        \"\"\"\n\n        def __prepare(key, value):\n            if isinstance(value, str):\n                return value\n            if isinstance(value, (int, Document)):\n                return str(value)\n\n            raise ValueError(\n                f\"Unsupported type {type(value)} for template value of key {key}\"\n            )\n\n        kwargs = {}\n        for k in self.template.placeholders:\n            v = getattr(self, k)\n\n            # if get a callable, execute to get its output\n            if isinstance(v, Callable):  # type: ignore[arg-type]\n                v = v()\n\n            if isinstance(v, list):\n                v = str([__prepare(k, each) for each in v])\n            elif isinstance(v, (str, int, Document)):\n                v = __prepare(k, v)\n            else:\n                raise ValueError(\n                    f\"Unsupported type {type(v)} for template value of key `{k}`\"\n                )\n            kwargs[k] = v\n\n        return kwargs\n\n    def set(self, **kwargs):\n        \"\"\"\n        Similar to `__set` but for external use.\n\n        Set the values of the attributes in the object based on the provided keyword\n            arguments.\n\n        Args:\n            kwargs (dict): A dictionary with the attribute names as keys and the new\n                values as values.\n\n        Returns:\n            None\n        \"\"\"\n        self.__set(**kwargs)\n\n    def run(self, **kwargs):\n        \"\"\"\n        Run the function with the given keyword arguments.\n\n        Args:\n            **kwargs: The keyword arguments to pass to the function.\n\n        Returns:\n            The result of calling the `populate` method of the `template` object\n            with the given keyword arguments.\n        \"\"\"\n        self.__set(**kwargs)\n        self.__check_unset_placeholders()\n        prepared_kwargs = self.__prepare_value()\n\n        text = self.template.populate(**prepared_kwargs)\n        return Document(text=text, metadata={\"origin\": \"PromptComponent\"})\n\n    def flow(self):\n        return self.__call__()\n</code></pre>"},{"location":"reference/llms/prompts/base/#llms.prompts.base.BasePromptComponent.set","title":"set","text":"<pre><code>set(**kwargs)\n</code></pre> <p>Similar to <code>__set</code> but for external use.</p> <p>Set the values of the attributes in the object based on the provided keyword     arguments.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>dict</code> <p>A dictionary with the attribute names as keys and the new values as values.</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>kotaemon\\llms\\prompts\\base.py</code> <pre><code>def set(self, **kwargs):\n    \"\"\"\n    Similar to `__set` but for external use.\n\n    Set the values of the attributes in the object based on the provided keyword\n        arguments.\n\n    Args:\n        kwargs (dict): A dictionary with the attribute names as keys and the new\n            values as values.\n\n    Returns:\n        None\n    \"\"\"\n    self.__set(**kwargs)\n</code></pre>"},{"location":"reference/llms/prompts/base/#llms.prompts.base.BasePromptComponent.run","title":"run","text":"<pre><code>run(**kwargs)\n</code></pre> <p>Run the function with the given keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>The keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The result of calling the <code>populate</code> method of the <code>template</code> object</p> <p>with the given keyword arguments.</p> Source code in <code>kotaemon\\llms\\prompts\\base.py</code> <pre><code>def run(self, **kwargs):\n    \"\"\"\n    Run the function with the given keyword arguments.\n\n    Args:\n        **kwargs: The keyword arguments to pass to the function.\n\n    Returns:\n        The result of calling the `populate` method of the `template` object\n        with the given keyword arguments.\n    \"\"\"\n    self.__set(**kwargs)\n    self.__check_unset_placeholders()\n    prepared_kwargs = self.__prepare_value()\n\n    text = self.template.populate(**prepared_kwargs)\n    return Document(text=text, metadata={\"origin\": \"PromptComponent\"})\n</code></pre>"},{"location":"reference/llms/prompts/template/","title":"Template","text":""},{"location":"reference/llms/prompts/template/#llms.prompts.template.PromptTemplate","title":"PromptTemplate","text":"<p>Base class for prompt templates.</p> Source code in <code>kotaemon\\llms\\prompts\\template.py</code> <pre><code>class PromptTemplate:\n    \"\"\"\n    Base class for prompt templates.\n    \"\"\"\n\n    def __init__(self, template: str, ignore_invalid=True):\n        template = template\n        formatter = Formatter()\n        parsed_template = list(formatter.parse(template))\n\n        placeholders = set()\n        for _, key, _, _ in parsed_template:\n            if key is None:\n                continue\n            if not key.isidentifier():\n                if ignore_invalid:\n                    warnings.warn(f\"Ignore invalid placeholder: {key}.\", UserWarning)\n                else:\n                    raise ValueError(\n                        \"Placeholder name must be a valid Python identifier, found:\"\n                        f\" {key}.\"\n                    )\n            placeholders.add(key)\n\n        self.template = template\n        self.placeholders = placeholders\n        self.__formatter = formatter\n        self.__parsed_template = parsed_template\n\n    def check_missing_kwargs(self, **kwargs):\n        \"\"\"\n        Check if all the placeholders in the template are set.\n\n        This function checks if all the expected placeholders in the template are set as\n            attributes of the object. If any placeholders are missing, a `ValueError`\n            is raised with the names of the missing keys.\n\n        Parameters:\n            None\n\n        Returns:\n            None\n        \"\"\"\n        missing_keys = self.placeholders.difference(kwargs.keys())\n        if missing_keys:\n            raise ValueError(f\"Missing keys in template: {','.join(missing_keys)}\")\n\n    def check_redundant_kwargs(self, **kwargs):\n        \"\"\"\n        Check if all the placeholders in the template are set.\n\n        This function checks if all the expected placeholders in the template are set as\n            attributes of the object. If any placeholders are missing, a `ValueError`\n            is raised with the names of the missing keys.\n\n        Parameters:\n            None\n\n        Returns:\n            None\n        \"\"\"\n        provided_keys = set(kwargs.keys())\n        redundant_keys = provided_keys - self.placeholders\n\n        if redundant_keys:\n            warnings.warn(\n                f\"Keys provided but not in template: {','.join(redundant_keys)}\",\n                UserWarning,\n            )\n\n    def populate(self, **kwargs) -&gt; str:\n        \"\"\"\n        Strictly populate the template with the given keyword arguments.\n\n        Args:\n            **kwargs: The keyword arguments to populate the template.\n                      Each keyword corresponds to a placeholder in the template.\n\n        Returns:\n            The populated template.\n\n        Raises:\n            ValueError: If an unknown placeholder is provided.\n        \"\"\"\n        self.check_missing_kwargs(**kwargs)\n\n        return self.partial_populate(**kwargs)\n\n    def partial_populate(self, **kwargs):\n        \"\"\"\n        Partially populate the template with the given keyword arguments.\n\n        Args:\n            **kwargs: The keyword arguments to populate the template.\n                      Each keyword corresponds to a placeholder in the template.\n\n        Returns:\n            str: The populated template.\n        \"\"\"\n        self.check_redundant_kwargs(**kwargs)\n\n        prompt = []\n        for literal_text, field_name, format_spec, conversion in self.__parsed_template:\n            prompt.append(literal_text)\n\n            if field_name is None:\n                continue\n\n            if field_name not in kwargs:\n                if conversion:\n                    value = f\"{{{field_name}}}!{conversion}:{format_spec}\"\n                else:\n                    value = f\"{{{field_name}:{format_spec}}}\"\n            else:\n                value = kwargs[field_name]\n                if conversion is not None:\n                    value = self.__formatter.convert_field(value, conversion)\n                if format_spec is not None:\n                    value = self.__formatter.format_field(value, format_spec)\n\n            prompt.append(value)\n\n        return \"\".join(prompt)\n\n    def __add__(self, other):\n        \"\"\"\n        Create a new PromptTemplate object by concatenating the template of the current\n            object with the template of another PromptTemplate object.\n\n        Parameters:\n            other (PromptTemplate): Another PromptTemplate object.\n\n        Returns:\n            PromptTemplate: A new PromptTemplate object with the concatenated templates.\n        \"\"\"\n        return PromptTemplate(self.template + \"\\n\" + other.template)\n</code></pre>"},{"location":"reference/llms/prompts/template/#llms.prompts.template.PromptTemplate.check_missing_kwargs","title":"check_missing_kwargs","text":"<pre><code>check_missing_kwargs(**kwargs)\n</code></pre> <p>Check if all the placeholders in the template are set.</p> <p>This function checks if all the expected placeholders in the template are set as     attributes of the object. If any placeholders are missing, a <code>ValueError</code>     is raised with the names of the missing keys.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>kotaemon\\llms\\prompts\\template.py</code> <pre><code>def check_missing_kwargs(self, **kwargs):\n    \"\"\"\n    Check if all the placeholders in the template are set.\n\n    This function checks if all the expected placeholders in the template are set as\n        attributes of the object. If any placeholders are missing, a `ValueError`\n        is raised with the names of the missing keys.\n\n    Parameters:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    missing_keys = self.placeholders.difference(kwargs.keys())\n    if missing_keys:\n        raise ValueError(f\"Missing keys in template: {','.join(missing_keys)}\")\n</code></pre>"},{"location":"reference/llms/prompts/template/#llms.prompts.template.PromptTemplate.check_redundant_kwargs","title":"check_redundant_kwargs","text":"<pre><code>check_redundant_kwargs(**kwargs)\n</code></pre> <p>Check if all the placeholders in the template are set.</p> <p>This function checks if all the expected placeholders in the template are set as     attributes of the object. If any placeholders are missing, a <code>ValueError</code>     is raised with the names of the missing keys.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>kotaemon\\llms\\prompts\\template.py</code> <pre><code>def check_redundant_kwargs(self, **kwargs):\n    \"\"\"\n    Check if all the placeholders in the template are set.\n\n    This function checks if all the expected placeholders in the template are set as\n        attributes of the object. If any placeholders are missing, a `ValueError`\n        is raised with the names of the missing keys.\n\n    Parameters:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    provided_keys = set(kwargs.keys())\n    redundant_keys = provided_keys - self.placeholders\n\n    if redundant_keys:\n        warnings.warn(\n            f\"Keys provided but not in template: {','.join(redundant_keys)}\",\n            UserWarning,\n        )\n</code></pre>"},{"location":"reference/llms/prompts/template/#llms.prompts.template.PromptTemplate.populate","title":"populate","text":"<pre><code>populate(**kwargs)\n</code></pre> <p>Strictly populate the template with the given keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>The keyword arguments to populate the template.       Each keyword corresponds to a placeholder in the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The populated template.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unknown placeholder is provided.</p> Source code in <code>kotaemon\\llms\\prompts\\template.py</code> <pre><code>def populate(self, **kwargs) -&gt; str:\n    \"\"\"\n    Strictly populate the template with the given keyword arguments.\n\n    Args:\n        **kwargs: The keyword arguments to populate the template.\n                  Each keyword corresponds to a placeholder in the template.\n\n    Returns:\n        The populated template.\n\n    Raises:\n        ValueError: If an unknown placeholder is provided.\n    \"\"\"\n    self.check_missing_kwargs(**kwargs)\n\n    return self.partial_populate(**kwargs)\n</code></pre>"},{"location":"reference/llms/prompts/template/#llms.prompts.template.PromptTemplate.partial_populate","title":"partial_populate","text":"<pre><code>partial_populate(**kwargs)\n</code></pre> <p>Partially populate the template with the given keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>The keyword arguments to populate the template.       Each keyword corresponds to a placeholder in the template.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The populated template.</p> Source code in <code>kotaemon\\llms\\prompts\\template.py</code> <pre><code>def partial_populate(self, **kwargs):\n    \"\"\"\n    Partially populate the template with the given keyword arguments.\n\n    Args:\n        **kwargs: The keyword arguments to populate the template.\n                  Each keyword corresponds to a placeholder in the template.\n\n    Returns:\n        str: The populated template.\n    \"\"\"\n    self.check_redundant_kwargs(**kwargs)\n\n    prompt = []\n    for literal_text, field_name, format_spec, conversion in self.__parsed_template:\n        prompt.append(literal_text)\n\n        if field_name is None:\n            continue\n\n        if field_name not in kwargs:\n            if conversion:\n                value = f\"{{{field_name}}}!{conversion}:{format_spec}\"\n            else:\n                value = f\"{{{field_name}:{format_spec}}}\"\n        else:\n            value = kwargs[field_name]\n            if conversion is not None:\n                value = self.__formatter.convert_field(value, conversion)\n            if format_spec is not None:\n                value = self.__formatter.format_field(value, format_spec)\n\n        prompt.append(value)\n\n    return \"\".join(prompt)\n</code></pre>"},{"location":"reference/loaders/","title":"Loaders","text":""},{"location":"reference/loaders/#loaders.AutoReader","title":"AutoReader","text":"<p>             Bases: <code>BaseComponent</code></p> <p>General auto reader for a variety of files. (based on llama-hub)</p> Source code in <code>kotaemon\\loaders\\base.py</code> <pre><code>class AutoReader(BaseComponent):\n    \"\"\"General auto reader for a variety of files. (based on llama-hub)\"\"\"\n\n    def __init__(self, reader_type: Union[str, Type[BaseReader]]) -&gt; None:\n        \"\"\"Init reader using string identifier or class name from llama-hub\"\"\"\n\n        if isinstance(reader_type, str):\n            self._reader = download_loader(reader_type)()\n        else:\n            self._reader = reader_type()\n        super().__init__()\n\n    def load_data(self, file: Union[Path, str], **kwargs: Any) -&gt; List[Document]:\n        documents = self._reader.load_data(file=file, **kwargs)\n\n        # convert Document to new base class from kotaemon\n        converted_documents = [Document.from_dict(doc.to_dict()) for doc in documents]\n        return converted_documents\n\n    def run(self, file: Union[Path, str], **kwargs: Any) -&gt; List[Document]:\n        return self.load_data(file=file, **kwargs)\n</code></pre>"},{"location":"reference/loaders/#loaders.PandasExcelReader","title":"PandasExcelReader","text":"<p>             Bases: <code>BaseReader</code></p> <p>Pandas-based CSV parser.</p> <p>Parses CSVs using the separator detection from Pandas <code>read_csv</code> function. If special parameters are required, use the <code>pandas_config</code> dict.</p> <p>Args:</p> <pre><code>pandas_config (dict): Options for the `pandas.read_excel` function call.\n    Refer to https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html\n    for more information. Set to empty dict by default,\n    this means defaults will be used.\n</code></pre> Source code in <code>kotaemon\\loaders\\excel_loader.py</code> <pre><code>class PandasExcelReader(BaseReader):\n    r\"\"\"Pandas-based CSV parser.\n\n    Parses CSVs using the separator detection from Pandas `read_csv` function.\n    If special parameters are required, use the `pandas_config` dict.\n\n    Args:\n\n        pandas_config (dict): Options for the `pandas.read_excel` function call.\n            Refer to https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html\n            for more information. Set to empty dict by default,\n            this means defaults will be used.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *args: Any,\n        pandas_config: Optional[dict] = None,\n        row_joiner: str = \"\\n\",\n        col_joiner: str = \" \",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._pandas_config = pandas_config or {}\n        self._row_joiner = row_joiner if row_joiner else \"\\n\"\n        self._col_joiner = col_joiner if col_joiner else \" \"\n\n    def load_data(\n        self,\n        file: Path,\n        include_sheetname: bool = False,\n        sheet_name: Optional[Union[str, int, list]] = None,\n        **kwargs,\n    ) -&gt; List[Document]:\n        \"\"\"Parse file and extract values from a specific column.\n\n        Args:\n            file (Path): The path to the Excel file to read.\n            include_sheetname (bool): Whether to include the sheet name in the output.\n            sheet_name (Union[str, int, None]): The specific sheet to read from,\n                default is None which reads all sheets.\n\n        Returns:\n            List[Document]: A list of`Document objects containing the\n                values from the specified column in the Excel file.\n        \"\"\"\n        import itertools\n\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"install pandas using `pip3 install pandas` to use this loader\"\n            )\n\n        if sheet_name is not None:\n            sheet_name = (\n                [sheet_name] if not isinstance(sheet_name, list) else sheet_name\n            )\n\n        dfs = pd.read_excel(file, sheet_name=sheet_name, **self._pandas_config)\n        sheet_names = dfs.keys()\n        df_sheets = []\n\n        for key in sheet_names:\n            sheet = []\n            if include_sheetname:\n                sheet.append([key])\n            sheet.extend(dfs[key].values.astype(str).tolist())\n            df_sheets.append(sheet)\n\n        text_list = list(\n            itertools.chain.from_iterable(df_sheets)\n        )  # flatten list of lists\n\n        output = [\n            Document(\n                text=self._row_joiner.join(\n                    self._col_joiner.join(sublist) for sublist in text_list\n                ),\n                metadata={\"source\": file.stem},\n            )\n        ]\n\n        return output\n</code></pre>"},{"location":"reference/loaders/#loaders.PandasExcelReader.load_data","title":"load_data","text":"<pre><code>load_data(file, include_sheetname=False, sheet_name=None, **kwargs)\n</code></pre> <p>Parse file and extract values from a specific column.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Path</code> <p>The path to the Excel file to read.</p> required <code>include_sheetname</code> <code>bool</code> <p>Whether to include the sheet name in the output.</p> <code>False</code> <code>sheet_name</code> <code>Union[str, int, None]</code> <p>The specific sheet to read from, default is None which reads all sheets.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: A list of`Document objects containing the values from the specified column in the Excel file.</p> Source code in <code>kotaemon\\loaders\\excel_loader.py</code> <pre><code>def load_data(\n    self,\n    file: Path,\n    include_sheetname: bool = False,\n    sheet_name: Optional[Union[str, int, list]] = None,\n    **kwargs,\n) -&gt; List[Document]:\n    \"\"\"Parse file and extract values from a specific column.\n\n    Args:\n        file (Path): The path to the Excel file to read.\n        include_sheetname (bool): Whether to include the sheet name in the output.\n        sheet_name (Union[str, int, None]): The specific sheet to read from,\n            default is None which reads all sheets.\n\n    Returns:\n        List[Document]: A list of`Document objects containing the\n            values from the specified column in the Excel file.\n    \"\"\"\n    import itertools\n\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\n            \"install pandas using `pip3 install pandas` to use this loader\"\n        )\n\n    if sheet_name is not None:\n        sheet_name = (\n            [sheet_name] if not isinstance(sheet_name, list) else sheet_name\n        )\n\n    dfs = pd.read_excel(file, sheet_name=sheet_name, **self._pandas_config)\n    sheet_names = dfs.keys()\n    df_sheets = []\n\n    for key in sheet_names:\n        sheet = []\n        if include_sheetname:\n            sheet.append([key])\n        sheet.extend(dfs[key].values.astype(str).tolist())\n        df_sheets.append(sheet)\n\n    text_list = list(\n        itertools.chain.from_iterable(df_sheets)\n    )  # flatten list of lists\n\n    output = [\n        Document(\n            text=self._row_joiner.join(\n                self._col_joiner.join(sublist) for sublist in text_list\n            ),\n            metadata={\"source\": file.stem},\n        )\n    ]\n\n    return output\n</code></pre>"},{"location":"reference/loaders/#loaders.MathpixPDFReader","title":"MathpixPDFReader","text":"<p>             Bases: <code>BaseReader</code></p> <p>Load <code>PDF</code> files using <code>Mathpix</code> service.</p> Source code in <code>kotaemon\\loaders\\mathpix_loader.py</code> <pre><code>class MathpixPDFReader(BaseReader):\n    \"\"\"Load `PDF` files using `Mathpix` service.\"\"\"\n\n    def __init__(\n        self,\n        processed_file_format: str = \"md\",\n        max_wait_time_seconds: int = 500,\n        should_clean_pdf: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize with a file path.\n\n        Args:\n            processed_file_format: a format of the processed file. Default is   \"mmd\".\n            max_wait_time_seconds: a maximum time to wait for the response from\n                the server. Default is 500.\n            should_clean_pdf: a flag to clean the PDF file. Default is False.\n            **kwargs: additional keyword arguments.\n        \"\"\"\n        self.mathpix_api_key = get_from_dict_or_env(\n            kwargs, \"mathpix_api_key\", \"MATHPIX_API_KEY\", default=\"empty\"\n        )\n        self.mathpix_api_id = get_from_dict_or_env(\n            kwargs, \"mathpix_api_id\", \"MATHPIX_API_ID\", default=\"empty\"\n        )\n        self.processed_file_format = processed_file_format\n        self.max_wait_time_seconds = max_wait_time_seconds\n        self.should_clean_pdf = should_clean_pdf\n        super().__init__()\n\n    @property\n    def _mathpix_headers(self) -&gt; Dict[str, str]:\n        return {\"app_id\": self.mathpix_api_id, \"app_key\": self.mathpix_api_key}\n\n    @property\n    def url(self) -&gt; str:\n        return \"https://api.mathpix.com/v3/pdf\"\n\n    @property\n    def data(self) -&gt; dict:\n        options = {\n            \"conversion_formats\": {self.processed_file_format: True},\n            \"enable_tables_fallback\": True,\n        }\n        return {\"options_json\": json.dumps(options)}\n\n    def send_pdf(self, file_path) -&gt; str:\n        with open(file_path, \"rb\") as f:\n            files = {\"file\": f}\n            response = requests.post(\n                self.url, headers=self._mathpix_headers, files=files, data=self.data\n            )\n        response_data = response.json()\n        if \"pdf_id\" in response_data:\n            pdf_id = response_data[\"pdf_id\"]\n            return pdf_id\n        else:\n            raise ValueError(\"Unable to send PDF to Mathpix.\")\n\n    def wait_for_processing(self, pdf_id: str) -&gt; None:\n        \"\"\"Wait for processing to complete.\n\n        Args:\n            pdf_id: a PDF id.\n\n        Returns: None\n        \"\"\"\n        url = self.url + \"/\" + pdf_id\n        for _ in range(0, self.max_wait_time_seconds, 5):\n            response = requests.get(url, headers=self._mathpix_headers)\n            response_data = response.json()\n            status = response_data.get(\"status\", None)\n\n            if status == \"completed\":\n                return\n            elif status == \"error\":\n                raise ValueError(\"Unable to retrieve PDF from Mathpix\")\n            else:\n                print(response_data)\n                print(url)\n                time.sleep(5)\n        raise TimeoutError\n\n    def get_processed_pdf(self, pdf_id: str) -&gt; str:\n        self.wait_for_processing(pdf_id)\n        url = f\"{self.url}/{pdf_id}.{self.processed_file_format}\"\n        response = requests.get(url, headers=self._mathpix_headers)\n        return response.content.decode(\"utf-8\")\n\n    def clean_pdf(self, contents: str) -&gt; str:\n        \"\"\"Clean the PDF file.\n\n        Args:\n            contents: a PDF file contents.\n\n        Returns:\n\n        \"\"\"\n        contents = \"\\n\".join(\n            [line for line in contents.split(\"\\n\") if not line.startswith(\"![]\")]\n        )\n        # replace \\section{Title} with # Title\n        contents = contents.replace(\"\\\\section{\", \"# \")\n        # replace the \"\\\" slash that Mathpix adds to escape $, %, (, etc.\n\n        # http:// or https:// followed by anything but a closing paren\n        url_regex = \"http[s]?://[^)]+\"\n        markup_regex = r\"\\[]\\(\\s*({0})\\s*\\)\".format(url_regex)\n        contents = (\n            contents.replace(r\"\\$\", \"$\")\n            .replace(r\"\\%\", \"%\")\n            .replace(r\"\\(\", \"(\")\n            .replace(r\"\\)\", \")\")\n            .replace(\"$\\\\begin{array}\", \"\")\n            .replace(\"\\\\end{array}$\", \"\")\n            .replace(\"\\\\\\\\\", \"\")\n            .replace(\"\\\\text\", \"\")\n            .replace(\"}\", \"\")\n            .replace(\"{\", \"\")\n            .replace(\"\\\\mathrm\", \"\")\n        )\n        contents = re.sub(markup_regex, \"\", contents)\n        return contents\n\n    def load_data(self, file_path: Path, **kwargs) -&gt; List[Document]:\n        if \"response_content\" in kwargs:\n            # overriding response content if specified\n            content = kwargs[\"response_content\"]\n        else:\n            # call original API\n            pdf_id = self.send_pdf(file_path)\n            content = self.get_processed_pdf(pdf_id)\n\n        if self.should_clean_pdf:\n            content = self.clean_pdf(content)\n        tables, texts = parse_markdown_text_to_tables(content)\n        documents = []\n        for table in tables:\n            text = strip_special_chars_markdown(table)\n            metadata = {\n                \"source\": file_path.name,\n                \"table_origin\": table,\n                \"type\": \"table\",\n            }\n            documents.append(\n                Document(\n                    text=text,\n                    metadata=metadata,\n                    metadata_template=\"\",\n                    metadata_seperator=\"\",\n                )\n            )\n\n        for text in texts:\n            metadata = {\"source\": file_path.name, \"type\": \"text\"}\n            documents.append(Document(text=text, metadata=metadata))\n\n        return documents\n</code></pre>"},{"location":"reference/loaders/#loaders.MathpixPDFReader.wait_for_processing","title":"wait_for_processing","text":"<pre><code>wait_for_processing(pdf_id)\n</code></pre> <p>Wait for processing to complete.</p> <p>Parameters:</p> Name Type Description Default <code>pdf_id</code> <code>str</code> <p>a PDF id.</p> required <p>Returns: None</p> Source code in <code>kotaemon\\loaders\\mathpix_loader.py</code> <pre><code>def wait_for_processing(self, pdf_id: str) -&gt; None:\n    \"\"\"Wait for processing to complete.\n\n    Args:\n        pdf_id: a PDF id.\n\n    Returns: None\n    \"\"\"\n    url = self.url + \"/\" + pdf_id\n    for _ in range(0, self.max_wait_time_seconds, 5):\n        response = requests.get(url, headers=self._mathpix_headers)\n        response_data = response.json()\n        status = response_data.get(\"status\", None)\n\n        if status == \"completed\":\n            return\n        elif status == \"error\":\n            raise ValueError(\"Unable to retrieve PDF from Mathpix\")\n        else:\n            print(response_data)\n            print(url)\n            time.sleep(5)\n    raise TimeoutError\n</code></pre>"},{"location":"reference/loaders/#loaders.MathpixPDFReader.clean_pdf","title":"clean_pdf","text":"<pre><code>clean_pdf(contents)\n</code></pre> <p>Clean the PDF file.</p> <p>Parameters:</p> Name Type Description Default <code>contents</code> <code>str</code> <p>a PDF file contents.</p> required <p>Returns:</p> Source code in <code>kotaemon\\loaders\\mathpix_loader.py</code> <pre><code>def clean_pdf(self, contents: str) -&gt; str:\n    \"\"\"Clean the PDF file.\n\n    Args:\n        contents: a PDF file contents.\n\n    Returns:\n\n    \"\"\"\n    contents = \"\\n\".join(\n        [line for line in contents.split(\"\\n\") if not line.startswith(\"![]\")]\n    )\n    # replace \\section{Title} with # Title\n    contents = contents.replace(\"\\\\section{\", \"# \")\n    # replace the \"\\\" slash that Mathpix adds to escape $, %, (, etc.\n\n    # http:// or https:// followed by anything but a closing paren\n    url_regex = \"http[s]?://[^)]+\"\n    markup_regex = r\"\\[]\\(\\s*({0})\\s*\\)\".format(url_regex)\n    contents = (\n        contents.replace(r\"\\$\", \"$\")\n        .replace(r\"\\%\", \"%\")\n        .replace(r\"\\(\", \"(\")\n        .replace(r\"\\)\", \")\")\n        .replace(\"$\\\\begin{array}\", \"\")\n        .replace(\"\\\\end{array}$\", \"\")\n        .replace(\"\\\\\\\\\", \"\")\n        .replace(\"\\\\text\", \"\")\n        .replace(\"}\", \"\")\n        .replace(\"{\", \"\")\n        .replace(\"\\\\mathrm\", \"\")\n    )\n    contents = re.sub(markup_regex, \"\", contents)\n    return contents\n</code></pre>"},{"location":"reference/loaders/#loaders.OCRReader","title":"OCRReader","text":"<p>             Bases: <code>BaseReader</code></p> Source code in <code>kotaemon\\loaders\\ocr_loader.py</code> <pre><code>class OCRReader(BaseReader):\n    def __init__(self, endpoint: str = DEFAULT_OCR_ENDPOINT, use_ocr=True):\n        \"\"\"Init the OCR reader with OCR endpoint (FullOCR pipeline)\n\n        Args:\n            endpoint: URL to FullOCR endpoint. Defaults to OCR_ENDPOINT.\n            use_ocr: whether to use OCR to read text\n                (e.g: from images, tables) in the PDF\n        \"\"\"\n        super().__init__()\n        self.ocr_endpoint = endpoint\n        self.use_ocr = use_ocr\n\n    def load_data(\n        self,\n        file_path: Path,\n        **kwargs,\n    ) -&gt; List[Document]:\n        \"\"\"Load data using OCR reader\n\n        Args:\n            file_path (Path): Path to PDF file\n            debug_path (Path): Path to store debug image output\n            artifact_path (Path): Path to OCR endpoints artifacts directory\n\n        Returns:\n            List[Document]: list of documents extracted from the PDF file\n        \"\"\"\n        # create input params for the requests\n        content = open(file_path, \"rb\")\n        files = {\"input\": content}\n        data = {\"job_id\": uuid4(), \"table_only\": not self.use_ocr}\n\n        debug_path = kwargs.pop(\"debug_path\", None)\n        artifact_path = kwargs.pop(\"artifact_path\", None)\n\n        # call the API from FullOCR endpoint\n        if \"response_content\" in kwargs:\n            # overriding response content if specified\n            ocr_results = kwargs[\"response_content\"]\n        else:\n            # call original API\n            resp = requests.post(url=self.ocr_endpoint, files=files, data=data)\n            ocr_results = resp.json()[\"result\"]\n\n        # read PDF through normal reader (unstructured)\n        pdf_page_items = read_pdf_unstructured(file_path)\n        # merge PDF text output with OCR output\n        tables, texts = parse_ocr_output(\n            ocr_results,\n            pdf_page_items,\n            debug_path=debug_path,\n            artifact_path=artifact_path,\n        )\n\n        # create output Document with metadata from table\n        documents = [\n            Document(\n                text=strip_special_chars_markdown(table_text),\n                metadata={\n                    \"table_origin\": table_text,\n                    \"type\": \"table\",\n                    \"page_label\": page_id + 1,\n                    \"source\": file_path.name,\n                },\n                metadata_template=\"\",\n                metadata_seperator=\"\",\n            )\n            for page_id, table_text in tables\n        ]\n        # create Document from non-table text\n        documents.extend(\n            [\n                Document(\n                    text=non_table_text,\n                    metadata={\n                        \"page_label\": page_id + 1,\n                        \"source\": file_path.name,\n                    },\n                )\n                for page_id, non_table_text in texts\n            ]\n        )\n\n        return documents\n</code></pre>"},{"location":"reference/loaders/#loaders.OCRReader.load_data","title":"load_data","text":"<pre><code>load_data(file_path, **kwargs)\n</code></pre> <p>Load data using OCR reader</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to PDF file</p> required <code>debug_path</code> <code>Path</code> <p>Path to store debug image output</p> required <code>artifact_path</code> <code>Path</code> <p>Path to OCR endpoints artifacts directory</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: list of documents extracted from the PDF file</p> Source code in <code>kotaemon\\loaders\\ocr_loader.py</code> <pre><code>def load_data(\n    self,\n    file_path: Path,\n    **kwargs,\n) -&gt; List[Document]:\n    \"\"\"Load data using OCR reader\n\n    Args:\n        file_path (Path): Path to PDF file\n        debug_path (Path): Path to store debug image output\n        artifact_path (Path): Path to OCR endpoints artifacts directory\n\n    Returns:\n        List[Document]: list of documents extracted from the PDF file\n    \"\"\"\n    # create input params for the requests\n    content = open(file_path, \"rb\")\n    files = {\"input\": content}\n    data = {\"job_id\": uuid4(), \"table_only\": not self.use_ocr}\n\n    debug_path = kwargs.pop(\"debug_path\", None)\n    artifact_path = kwargs.pop(\"artifact_path\", None)\n\n    # call the API from FullOCR endpoint\n    if \"response_content\" in kwargs:\n        # overriding response content if specified\n        ocr_results = kwargs[\"response_content\"]\n    else:\n        # call original API\n        resp = requests.post(url=self.ocr_endpoint, files=files, data=data)\n        ocr_results = resp.json()[\"result\"]\n\n    # read PDF through normal reader (unstructured)\n    pdf_page_items = read_pdf_unstructured(file_path)\n    # merge PDF text output with OCR output\n    tables, texts = parse_ocr_output(\n        ocr_results,\n        pdf_page_items,\n        debug_path=debug_path,\n        artifact_path=artifact_path,\n    )\n\n    # create output Document with metadata from table\n    documents = [\n        Document(\n            text=strip_special_chars_markdown(table_text),\n            metadata={\n                \"table_origin\": table_text,\n                \"type\": \"table\",\n                \"page_label\": page_id + 1,\n                \"source\": file_path.name,\n            },\n            metadata_template=\"\",\n            metadata_seperator=\"\",\n        )\n        for page_id, table_text in tables\n    ]\n    # create Document from non-table text\n    documents.extend(\n        [\n            Document(\n                text=non_table_text,\n                metadata={\n                    \"page_label\": page_id + 1,\n                    \"source\": file_path.name,\n                },\n            )\n            for page_id, non_table_text in texts\n        ]\n    )\n\n    return documents\n</code></pre>"},{"location":"reference/loaders/#loaders.UnstructuredReader","title":"UnstructuredReader","text":"<p>             Bases: <code>BaseReader</code></p> <p>General unstructured text reader for a variety of files.</p> Source code in <code>kotaemon\\loaders\\unstructured_loader.py</code> <pre><code>class UnstructuredReader(BaseReader):\n    \"\"\"General unstructured text reader for a variety of files.\"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args)  # not passing kwargs to parent bc it cannot accept it\n\n        self.api = False  # we default to local\n        if \"url\" in kwargs:\n            self.server_url = str(kwargs[\"url\"])\n            self.api = True  # is url was set, switch to api\n        else:\n            self.server_url = \"http://localhost:8000\"\n\n        if \"api\" in kwargs:\n            self.api = kwargs[\"api\"]\n\n        self.api_key = \"\"\n        if \"api_key\" in kwargs:\n            self.api_key = kwargs[\"api_key\"]\n\n    \"\"\" Loads data using Unstructured.io\n\n        Depending on the construction if url is set or api = True\n        it'll parse file using API call, else parse it locally\n        additional_metadata is extended by the returned metadata if\n        split_documents is True\n\n        Returns list of documents\n    \"\"\"\n\n    def load_data(\n        self,\n        file: Path,\n        additional_metadata: Optional[Dict] = None,\n        split_documents: Optional[bool] = False,\n        **kwargs,\n    ) -&gt; List[Document]:\n        \"\"\"If api is set, parse through api\"\"\"\n        file_path_str = str(file)\n        if self.api:\n            from unstructured.partition.api import partition_via_api\n\n            elements = partition_via_api(\n                filename=file_path_str,\n                api_key=self.api_key,\n                api_url=self.server_url + \"/general/v0/general\",\n            )\n        else:\n            \"\"\"Parse file locally\"\"\"\n            from unstructured.partition.auto import partition\n\n            elements = partition(filename=file_path_str)\n\n        \"\"\" Process elements \"\"\"\n        docs = []\n        file_name = Path(file).name\n        if split_documents:\n            for node in elements:\n                metadata = {\"file_name\": file_name}\n                if hasattr(node, \"metadata\"):\n                    \"\"\"Load metadata fields\"\"\"\n                    for field, val in vars(node.metadata).items():\n                        if field == \"_known_field_names\":\n                            continue\n                        # removing coordinates because it does not serialize\n                        # and dont want to bother with it\n                        if field == \"coordinates\":\n                            continue\n                        # removing bc it might cause interference\n                        if field == \"parent_id\":\n                            continue\n                        metadata[field] = val\n\n                if additional_metadata is not None:\n                    metadata.update(additional_metadata)\n\n                metadata[\"file_name\"] = file_name\n                docs.append(Document(text=node.text, metadata=metadata))\n\n        else:\n            text_chunks = [\" \".join(str(el).split()) for el in elements]\n            metadata = {\"file_name\": file_name}\n\n            if additional_metadata is not None:\n                metadata.update(additional_metadata)\n\n            # Create a single document by joining all the texts\n            docs.append(Document(text=\"\\n\\n\".join(text_chunks), metadata=metadata))\n\n        return docs\n</code></pre>"},{"location":"reference/loaders/#loaders.UnstructuredReader.load_data","title":"load_data","text":"<pre><code>load_data(file, additional_metadata=None, split_documents=False, **kwargs)\n</code></pre> <p>If api is set, parse through api</p> Source code in <code>kotaemon\\loaders\\unstructured_loader.py</code> <pre><code>def load_data(\n    self,\n    file: Path,\n    additional_metadata: Optional[Dict] = None,\n    split_documents: Optional[bool] = False,\n    **kwargs,\n) -&gt; List[Document]:\n    \"\"\"If api is set, parse through api\"\"\"\n    file_path_str = str(file)\n    if self.api:\n        from unstructured.partition.api import partition_via_api\n\n        elements = partition_via_api(\n            filename=file_path_str,\n            api_key=self.api_key,\n            api_url=self.server_url + \"/general/v0/general\",\n        )\n    else:\n        \"\"\"Parse file locally\"\"\"\n        from unstructured.partition.auto import partition\n\n        elements = partition(filename=file_path_str)\n\n    \"\"\" Process elements \"\"\"\n    docs = []\n    file_name = Path(file).name\n    if split_documents:\n        for node in elements:\n            metadata = {\"file_name\": file_name}\n            if hasattr(node, \"metadata\"):\n                \"\"\"Load metadata fields\"\"\"\n                for field, val in vars(node.metadata).items():\n                    if field == \"_known_field_names\":\n                        continue\n                    # removing coordinates because it does not serialize\n                    # and dont want to bother with it\n                    if field == \"coordinates\":\n                        continue\n                    # removing bc it might cause interference\n                    if field == \"parent_id\":\n                        continue\n                    metadata[field] = val\n\n            if additional_metadata is not None:\n                metadata.update(additional_metadata)\n\n            metadata[\"file_name\"] = file_name\n            docs.append(Document(text=node.text, metadata=metadata))\n\n    else:\n        text_chunks = [\" \".join(str(el).split()) for el in elements]\n        metadata = {\"file_name\": file_name}\n\n        if additional_metadata is not None:\n            metadata.update(additional_metadata)\n\n        # Create a single document by joining all the texts\n        docs.append(Document(text=\"\\n\\n\".join(text_chunks), metadata=metadata))\n\n    return docs\n</code></pre>"},{"location":"reference/loaders/base/","title":"Base","text":""},{"location":"reference/loaders/base/#loaders.base.AutoReader","title":"AutoReader","text":"<p>             Bases: <code>BaseComponent</code></p> <p>General auto reader for a variety of files. (based on llama-hub)</p> Source code in <code>kotaemon\\loaders\\base.py</code> <pre><code>class AutoReader(BaseComponent):\n    \"\"\"General auto reader for a variety of files. (based on llama-hub)\"\"\"\n\n    def __init__(self, reader_type: Union[str, Type[BaseReader]]) -&gt; None:\n        \"\"\"Init reader using string identifier or class name from llama-hub\"\"\"\n\n        if isinstance(reader_type, str):\n            self._reader = download_loader(reader_type)()\n        else:\n            self._reader = reader_type()\n        super().__init__()\n\n    def load_data(self, file: Union[Path, str], **kwargs: Any) -&gt; List[Document]:\n        documents = self._reader.load_data(file=file, **kwargs)\n\n        # convert Document to new base class from kotaemon\n        converted_documents = [Document.from_dict(doc.to_dict()) for doc in documents]\n        return converted_documents\n\n    def run(self, file: Union[Path, str], **kwargs: Any) -&gt; List[Document]:\n        return self.load_data(file=file, **kwargs)\n</code></pre>"},{"location":"reference/loaders/excel_loader/","title":"Excel Loader","text":"<p>Pandas Excel reader.</p> <p>Pandas parser for .xlsx files.</p>"},{"location":"reference/loaders/excel_loader/#loaders.excel_loader.PandasExcelReader","title":"PandasExcelReader","text":"<p>             Bases: <code>BaseReader</code></p> <p>Pandas-based CSV parser.</p> <p>Parses CSVs using the separator detection from Pandas <code>read_csv</code> function. If special parameters are required, use the <code>pandas_config</code> dict.</p> <p>Args:</p> <pre><code>pandas_config (dict): Options for the `pandas.read_excel` function call.\n    Refer to https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html\n    for more information. Set to empty dict by default,\n    this means defaults will be used.\n</code></pre> Source code in <code>kotaemon\\loaders\\excel_loader.py</code> <pre><code>class PandasExcelReader(BaseReader):\n    r\"\"\"Pandas-based CSV parser.\n\n    Parses CSVs using the separator detection from Pandas `read_csv` function.\n    If special parameters are required, use the `pandas_config` dict.\n\n    Args:\n\n        pandas_config (dict): Options for the `pandas.read_excel` function call.\n            Refer to https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html\n            for more information. Set to empty dict by default,\n            this means defaults will be used.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *args: Any,\n        pandas_config: Optional[dict] = None,\n        row_joiner: str = \"\\n\",\n        col_joiner: str = \" \",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._pandas_config = pandas_config or {}\n        self._row_joiner = row_joiner if row_joiner else \"\\n\"\n        self._col_joiner = col_joiner if col_joiner else \" \"\n\n    def load_data(\n        self,\n        file: Path,\n        include_sheetname: bool = False,\n        sheet_name: Optional[Union[str, int, list]] = None,\n        **kwargs,\n    ) -&gt; List[Document]:\n        \"\"\"Parse file and extract values from a specific column.\n\n        Args:\n            file (Path): The path to the Excel file to read.\n            include_sheetname (bool): Whether to include the sheet name in the output.\n            sheet_name (Union[str, int, None]): The specific sheet to read from,\n                default is None which reads all sheets.\n\n        Returns:\n            List[Document]: A list of`Document objects containing the\n                values from the specified column in the Excel file.\n        \"\"\"\n        import itertools\n\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"install pandas using `pip3 install pandas` to use this loader\"\n            )\n\n        if sheet_name is not None:\n            sheet_name = (\n                [sheet_name] if not isinstance(sheet_name, list) else sheet_name\n            )\n\n        dfs = pd.read_excel(file, sheet_name=sheet_name, **self._pandas_config)\n        sheet_names = dfs.keys()\n        df_sheets = []\n\n        for key in sheet_names:\n            sheet = []\n            if include_sheetname:\n                sheet.append([key])\n            sheet.extend(dfs[key].values.astype(str).tolist())\n            df_sheets.append(sheet)\n\n        text_list = list(\n            itertools.chain.from_iterable(df_sheets)\n        )  # flatten list of lists\n\n        output = [\n            Document(\n                text=self._row_joiner.join(\n                    self._col_joiner.join(sublist) for sublist in text_list\n                ),\n                metadata={\"source\": file.stem},\n            )\n        ]\n\n        return output\n</code></pre>"},{"location":"reference/loaders/excel_loader/#loaders.excel_loader.PandasExcelReader.load_data","title":"load_data","text":"<pre><code>load_data(file, include_sheetname=False, sheet_name=None, **kwargs)\n</code></pre> <p>Parse file and extract values from a specific column.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Path</code> <p>The path to the Excel file to read.</p> required <code>include_sheetname</code> <code>bool</code> <p>Whether to include the sheet name in the output.</p> <code>False</code> <code>sheet_name</code> <code>Union[str, int, None]</code> <p>The specific sheet to read from, default is None which reads all sheets.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: A list of`Document objects containing the values from the specified column in the Excel file.</p> Source code in <code>kotaemon\\loaders\\excel_loader.py</code> <pre><code>def load_data(\n    self,\n    file: Path,\n    include_sheetname: bool = False,\n    sheet_name: Optional[Union[str, int, list]] = None,\n    **kwargs,\n) -&gt; List[Document]:\n    \"\"\"Parse file and extract values from a specific column.\n\n    Args:\n        file (Path): The path to the Excel file to read.\n        include_sheetname (bool): Whether to include the sheet name in the output.\n        sheet_name (Union[str, int, None]): The specific sheet to read from,\n            default is None which reads all sheets.\n\n    Returns:\n        List[Document]: A list of`Document objects containing the\n            values from the specified column in the Excel file.\n    \"\"\"\n    import itertools\n\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\n            \"install pandas using `pip3 install pandas` to use this loader\"\n        )\n\n    if sheet_name is not None:\n        sheet_name = (\n            [sheet_name] if not isinstance(sheet_name, list) else sheet_name\n        )\n\n    dfs = pd.read_excel(file, sheet_name=sheet_name, **self._pandas_config)\n    sheet_names = dfs.keys()\n    df_sheets = []\n\n    for key in sheet_names:\n        sheet = []\n        if include_sheetname:\n            sheet.append([key])\n        sheet.extend(dfs[key].values.astype(str).tolist())\n        df_sheets.append(sheet)\n\n    text_list = list(\n        itertools.chain.from_iterable(df_sheets)\n    )  # flatten list of lists\n\n    output = [\n        Document(\n            text=self._row_joiner.join(\n                self._col_joiner.join(sublist) for sublist in text_list\n            ),\n            metadata={\"source\": file.stem},\n        )\n    ]\n\n    return output\n</code></pre>"},{"location":"reference/loaders/mathpix_loader/","title":"Mathpix Loader","text":""},{"location":"reference/loaders/mathpix_loader/#loaders.mathpix_loader.MathpixPDFReader","title":"MathpixPDFReader","text":"<p>             Bases: <code>BaseReader</code></p> <p>Load <code>PDF</code> files using <code>Mathpix</code> service.</p> Source code in <code>kotaemon\\loaders\\mathpix_loader.py</code> <pre><code>class MathpixPDFReader(BaseReader):\n    \"\"\"Load `PDF` files using `Mathpix` service.\"\"\"\n\n    def __init__(\n        self,\n        processed_file_format: str = \"md\",\n        max_wait_time_seconds: int = 500,\n        should_clean_pdf: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize with a file path.\n\n        Args:\n            processed_file_format: a format of the processed file. Default is   \"mmd\".\n            max_wait_time_seconds: a maximum time to wait for the response from\n                the server. Default is 500.\n            should_clean_pdf: a flag to clean the PDF file. Default is False.\n            **kwargs: additional keyword arguments.\n        \"\"\"\n        self.mathpix_api_key = get_from_dict_or_env(\n            kwargs, \"mathpix_api_key\", \"MATHPIX_API_KEY\", default=\"empty\"\n        )\n        self.mathpix_api_id = get_from_dict_or_env(\n            kwargs, \"mathpix_api_id\", \"MATHPIX_API_ID\", default=\"empty\"\n        )\n        self.processed_file_format = processed_file_format\n        self.max_wait_time_seconds = max_wait_time_seconds\n        self.should_clean_pdf = should_clean_pdf\n        super().__init__()\n\n    @property\n    def _mathpix_headers(self) -&gt; Dict[str, str]:\n        return {\"app_id\": self.mathpix_api_id, \"app_key\": self.mathpix_api_key}\n\n    @property\n    def url(self) -&gt; str:\n        return \"https://api.mathpix.com/v3/pdf\"\n\n    @property\n    def data(self) -&gt; dict:\n        options = {\n            \"conversion_formats\": {self.processed_file_format: True},\n            \"enable_tables_fallback\": True,\n        }\n        return {\"options_json\": json.dumps(options)}\n\n    def send_pdf(self, file_path) -&gt; str:\n        with open(file_path, \"rb\") as f:\n            files = {\"file\": f}\n            response = requests.post(\n                self.url, headers=self._mathpix_headers, files=files, data=self.data\n            )\n        response_data = response.json()\n        if \"pdf_id\" in response_data:\n            pdf_id = response_data[\"pdf_id\"]\n            return pdf_id\n        else:\n            raise ValueError(\"Unable to send PDF to Mathpix.\")\n\n    def wait_for_processing(self, pdf_id: str) -&gt; None:\n        \"\"\"Wait for processing to complete.\n\n        Args:\n            pdf_id: a PDF id.\n\n        Returns: None\n        \"\"\"\n        url = self.url + \"/\" + pdf_id\n        for _ in range(0, self.max_wait_time_seconds, 5):\n            response = requests.get(url, headers=self._mathpix_headers)\n            response_data = response.json()\n            status = response_data.get(\"status\", None)\n\n            if status == \"completed\":\n                return\n            elif status == \"error\":\n                raise ValueError(\"Unable to retrieve PDF from Mathpix\")\n            else:\n                print(response_data)\n                print(url)\n                time.sleep(5)\n        raise TimeoutError\n\n    def get_processed_pdf(self, pdf_id: str) -&gt; str:\n        self.wait_for_processing(pdf_id)\n        url = f\"{self.url}/{pdf_id}.{self.processed_file_format}\"\n        response = requests.get(url, headers=self._mathpix_headers)\n        return response.content.decode(\"utf-8\")\n\n    def clean_pdf(self, contents: str) -&gt; str:\n        \"\"\"Clean the PDF file.\n\n        Args:\n            contents: a PDF file contents.\n\n        Returns:\n\n        \"\"\"\n        contents = \"\\n\".join(\n            [line for line in contents.split(\"\\n\") if not line.startswith(\"![]\")]\n        )\n        # replace \\section{Title} with # Title\n        contents = contents.replace(\"\\\\section{\", \"# \")\n        # replace the \"\\\" slash that Mathpix adds to escape $, %, (, etc.\n\n        # http:// or https:// followed by anything but a closing paren\n        url_regex = \"http[s]?://[^)]+\"\n        markup_regex = r\"\\[]\\(\\s*({0})\\s*\\)\".format(url_regex)\n        contents = (\n            contents.replace(r\"\\$\", \"$\")\n            .replace(r\"\\%\", \"%\")\n            .replace(r\"\\(\", \"(\")\n            .replace(r\"\\)\", \")\")\n            .replace(\"$\\\\begin{array}\", \"\")\n            .replace(\"\\\\end{array}$\", \"\")\n            .replace(\"\\\\\\\\\", \"\")\n            .replace(\"\\\\text\", \"\")\n            .replace(\"}\", \"\")\n            .replace(\"{\", \"\")\n            .replace(\"\\\\mathrm\", \"\")\n        )\n        contents = re.sub(markup_regex, \"\", contents)\n        return contents\n\n    def load_data(self, file_path: Path, **kwargs) -&gt; List[Document]:\n        if \"response_content\" in kwargs:\n            # overriding response content if specified\n            content = kwargs[\"response_content\"]\n        else:\n            # call original API\n            pdf_id = self.send_pdf(file_path)\n            content = self.get_processed_pdf(pdf_id)\n\n        if self.should_clean_pdf:\n            content = self.clean_pdf(content)\n        tables, texts = parse_markdown_text_to_tables(content)\n        documents = []\n        for table in tables:\n            text = strip_special_chars_markdown(table)\n            metadata = {\n                \"source\": file_path.name,\n                \"table_origin\": table,\n                \"type\": \"table\",\n            }\n            documents.append(\n                Document(\n                    text=text,\n                    metadata=metadata,\n                    metadata_template=\"\",\n                    metadata_seperator=\"\",\n                )\n            )\n\n        for text in texts:\n            metadata = {\"source\": file_path.name, \"type\": \"text\"}\n            documents.append(Document(text=text, metadata=metadata))\n\n        return documents\n</code></pre>"},{"location":"reference/loaders/mathpix_loader/#loaders.mathpix_loader.MathpixPDFReader.wait_for_processing","title":"wait_for_processing","text":"<pre><code>wait_for_processing(pdf_id)\n</code></pre> <p>Wait for processing to complete.</p> <p>Parameters:</p> Name Type Description Default <code>pdf_id</code> <code>str</code> <p>a PDF id.</p> required <p>Returns: None</p> Source code in <code>kotaemon\\loaders\\mathpix_loader.py</code> <pre><code>def wait_for_processing(self, pdf_id: str) -&gt; None:\n    \"\"\"Wait for processing to complete.\n\n    Args:\n        pdf_id: a PDF id.\n\n    Returns: None\n    \"\"\"\n    url = self.url + \"/\" + pdf_id\n    for _ in range(0, self.max_wait_time_seconds, 5):\n        response = requests.get(url, headers=self._mathpix_headers)\n        response_data = response.json()\n        status = response_data.get(\"status\", None)\n\n        if status == \"completed\":\n            return\n        elif status == \"error\":\n            raise ValueError(\"Unable to retrieve PDF from Mathpix\")\n        else:\n            print(response_data)\n            print(url)\n            time.sleep(5)\n    raise TimeoutError\n</code></pre>"},{"location":"reference/loaders/mathpix_loader/#loaders.mathpix_loader.MathpixPDFReader.clean_pdf","title":"clean_pdf","text":"<pre><code>clean_pdf(contents)\n</code></pre> <p>Clean the PDF file.</p> <p>Parameters:</p> Name Type Description Default <code>contents</code> <code>str</code> <p>a PDF file contents.</p> required <p>Returns:</p> Source code in <code>kotaemon\\loaders\\mathpix_loader.py</code> <pre><code>def clean_pdf(self, contents: str) -&gt; str:\n    \"\"\"Clean the PDF file.\n\n    Args:\n        contents: a PDF file contents.\n\n    Returns:\n\n    \"\"\"\n    contents = \"\\n\".join(\n        [line for line in contents.split(\"\\n\") if not line.startswith(\"![]\")]\n    )\n    # replace \\section{Title} with # Title\n    contents = contents.replace(\"\\\\section{\", \"# \")\n    # replace the \"\\\" slash that Mathpix adds to escape $, %, (, etc.\n\n    # http:// or https:// followed by anything but a closing paren\n    url_regex = \"http[s]?://[^)]+\"\n    markup_regex = r\"\\[]\\(\\s*({0})\\s*\\)\".format(url_regex)\n    contents = (\n        contents.replace(r\"\\$\", \"$\")\n        .replace(r\"\\%\", \"%\")\n        .replace(r\"\\(\", \"(\")\n        .replace(r\"\\)\", \")\")\n        .replace(\"$\\\\begin{array}\", \"\")\n        .replace(\"\\\\end{array}$\", \"\")\n        .replace(\"\\\\\\\\\", \"\")\n        .replace(\"\\\\text\", \"\")\n        .replace(\"}\", \"\")\n        .replace(\"{\", \"\")\n        .replace(\"\\\\mathrm\", \"\")\n    )\n    contents = re.sub(markup_regex, \"\", contents)\n    return contents\n</code></pre>"},{"location":"reference/loaders/ocr_loader/","title":"Ocr Loader","text":""},{"location":"reference/loaders/ocr_loader/#loaders.ocr_loader.OCRReader","title":"OCRReader","text":"<p>             Bases: <code>BaseReader</code></p> Source code in <code>kotaemon\\loaders\\ocr_loader.py</code> <pre><code>class OCRReader(BaseReader):\n    def __init__(self, endpoint: str = DEFAULT_OCR_ENDPOINT, use_ocr=True):\n        \"\"\"Init the OCR reader with OCR endpoint (FullOCR pipeline)\n\n        Args:\n            endpoint: URL to FullOCR endpoint. Defaults to OCR_ENDPOINT.\n            use_ocr: whether to use OCR to read text\n                (e.g: from images, tables) in the PDF\n        \"\"\"\n        super().__init__()\n        self.ocr_endpoint = endpoint\n        self.use_ocr = use_ocr\n\n    def load_data(\n        self,\n        file_path: Path,\n        **kwargs,\n    ) -&gt; List[Document]:\n        \"\"\"Load data using OCR reader\n\n        Args:\n            file_path (Path): Path to PDF file\n            debug_path (Path): Path to store debug image output\n            artifact_path (Path): Path to OCR endpoints artifacts directory\n\n        Returns:\n            List[Document]: list of documents extracted from the PDF file\n        \"\"\"\n        # create input params for the requests\n        content = open(file_path, \"rb\")\n        files = {\"input\": content}\n        data = {\"job_id\": uuid4(), \"table_only\": not self.use_ocr}\n\n        debug_path = kwargs.pop(\"debug_path\", None)\n        artifact_path = kwargs.pop(\"artifact_path\", None)\n\n        # call the API from FullOCR endpoint\n        if \"response_content\" in kwargs:\n            # overriding response content if specified\n            ocr_results = kwargs[\"response_content\"]\n        else:\n            # call original API\n            resp = requests.post(url=self.ocr_endpoint, files=files, data=data)\n            ocr_results = resp.json()[\"result\"]\n\n        # read PDF through normal reader (unstructured)\n        pdf_page_items = read_pdf_unstructured(file_path)\n        # merge PDF text output with OCR output\n        tables, texts = parse_ocr_output(\n            ocr_results,\n            pdf_page_items,\n            debug_path=debug_path,\n            artifact_path=artifact_path,\n        )\n\n        # create output Document with metadata from table\n        documents = [\n            Document(\n                text=strip_special_chars_markdown(table_text),\n                metadata={\n                    \"table_origin\": table_text,\n                    \"type\": \"table\",\n                    \"page_label\": page_id + 1,\n                    \"source\": file_path.name,\n                },\n                metadata_template=\"\",\n                metadata_seperator=\"\",\n            )\n            for page_id, table_text in tables\n        ]\n        # create Document from non-table text\n        documents.extend(\n            [\n                Document(\n                    text=non_table_text,\n                    metadata={\n                        \"page_label\": page_id + 1,\n                        \"source\": file_path.name,\n                    },\n                )\n                for page_id, non_table_text in texts\n            ]\n        )\n\n        return documents\n</code></pre>"},{"location":"reference/loaders/ocr_loader/#loaders.ocr_loader.OCRReader.load_data","title":"load_data","text":"<pre><code>load_data(file_path, **kwargs)\n</code></pre> <p>Load data using OCR reader</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to PDF file</p> required <code>debug_path</code> <code>Path</code> <p>Path to store debug image output</p> required <code>artifact_path</code> <code>Path</code> <p>Path to OCR endpoints artifacts directory</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: list of documents extracted from the PDF file</p> Source code in <code>kotaemon\\loaders\\ocr_loader.py</code> <pre><code>def load_data(\n    self,\n    file_path: Path,\n    **kwargs,\n) -&gt; List[Document]:\n    \"\"\"Load data using OCR reader\n\n    Args:\n        file_path (Path): Path to PDF file\n        debug_path (Path): Path to store debug image output\n        artifact_path (Path): Path to OCR endpoints artifacts directory\n\n    Returns:\n        List[Document]: list of documents extracted from the PDF file\n    \"\"\"\n    # create input params for the requests\n    content = open(file_path, \"rb\")\n    files = {\"input\": content}\n    data = {\"job_id\": uuid4(), \"table_only\": not self.use_ocr}\n\n    debug_path = kwargs.pop(\"debug_path\", None)\n    artifact_path = kwargs.pop(\"artifact_path\", None)\n\n    # call the API from FullOCR endpoint\n    if \"response_content\" in kwargs:\n        # overriding response content if specified\n        ocr_results = kwargs[\"response_content\"]\n    else:\n        # call original API\n        resp = requests.post(url=self.ocr_endpoint, files=files, data=data)\n        ocr_results = resp.json()[\"result\"]\n\n    # read PDF through normal reader (unstructured)\n    pdf_page_items = read_pdf_unstructured(file_path)\n    # merge PDF text output with OCR output\n    tables, texts = parse_ocr_output(\n        ocr_results,\n        pdf_page_items,\n        debug_path=debug_path,\n        artifact_path=artifact_path,\n    )\n\n    # create output Document with metadata from table\n    documents = [\n        Document(\n            text=strip_special_chars_markdown(table_text),\n            metadata={\n                \"table_origin\": table_text,\n                \"type\": \"table\",\n                \"page_label\": page_id + 1,\n                \"source\": file_path.name,\n            },\n            metadata_template=\"\",\n            metadata_seperator=\"\",\n        )\n        for page_id, table_text in tables\n    ]\n    # create Document from non-table text\n    documents.extend(\n        [\n            Document(\n                text=non_table_text,\n                metadata={\n                    \"page_label\": page_id + 1,\n                    \"source\": file_path.name,\n                },\n            )\n            for page_id, non_table_text in texts\n        ]\n    )\n\n    return documents\n</code></pre>"},{"location":"reference/loaders/unstructured_loader/","title":"Unstructured Loader","text":"<p>Unstructured file reader.</p> <p>A parser for unstructured text files using Unstructured.io. Supports .txt, .docx, .pptx, .jpg, .png, .eml, .html, and .pdf documents.</p> <p>To use .doc and .xls parser, install</p> <p>sudo apt-get install -y libmagic-dev poppler-utils libreoffice pip install xlrd</p>"},{"location":"reference/loaders/unstructured_loader/#loaders.unstructured_loader.UnstructuredReader","title":"UnstructuredReader","text":"<p>             Bases: <code>BaseReader</code></p> <p>General unstructured text reader for a variety of files.</p> Source code in <code>kotaemon\\loaders\\unstructured_loader.py</code> <pre><code>class UnstructuredReader(BaseReader):\n    \"\"\"General unstructured text reader for a variety of files.\"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(*args)  # not passing kwargs to parent bc it cannot accept it\n\n        self.api = False  # we default to local\n        if \"url\" in kwargs:\n            self.server_url = str(kwargs[\"url\"])\n            self.api = True  # is url was set, switch to api\n        else:\n            self.server_url = \"http://localhost:8000\"\n\n        if \"api\" in kwargs:\n            self.api = kwargs[\"api\"]\n\n        self.api_key = \"\"\n        if \"api_key\" in kwargs:\n            self.api_key = kwargs[\"api_key\"]\n\n    \"\"\" Loads data using Unstructured.io\n\n        Depending on the construction if url is set or api = True\n        it'll parse file using API call, else parse it locally\n        additional_metadata is extended by the returned metadata if\n        split_documents is True\n\n        Returns list of documents\n    \"\"\"\n\n    def load_data(\n        self,\n        file: Path,\n        additional_metadata: Optional[Dict] = None,\n        split_documents: Optional[bool] = False,\n        **kwargs,\n    ) -&gt; List[Document]:\n        \"\"\"If api is set, parse through api\"\"\"\n        file_path_str = str(file)\n        if self.api:\n            from unstructured.partition.api import partition_via_api\n\n            elements = partition_via_api(\n                filename=file_path_str,\n                api_key=self.api_key,\n                api_url=self.server_url + \"/general/v0/general\",\n            )\n        else:\n            \"\"\"Parse file locally\"\"\"\n            from unstructured.partition.auto import partition\n\n            elements = partition(filename=file_path_str)\n\n        \"\"\" Process elements \"\"\"\n        docs = []\n        file_name = Path(file).name\n        if split_documents:\n            for node in elements:\n                metadata = {\"file_name\": file_name}\n                if hasattr(node, \"metadata\"):\n                    \"\"\"Load metadata fields\"\"\"\n                    for field, val in vars(node.metadata).items():\n                        if field == \"_known_field_names\":\n                            continue\n                        # removing coordinates because it does not serialize\n                        # and dont want to bother with it\n                        if field == \"coordinates\":\n                            continue\n                        # removing bc it might cause interference\n                        if field == \"parent_id\":\n                            continue\n                        metadata[field] = val\n\n                if additional_metadata is not None:\n                    metadata.update(additional_metadata)\n\n                metadata[\"file_name\"] = file_name\n                docs.append(Document(text=node.text, metadata=metadata))\n\n        else:\n            text_chunks = [\" \".join(str(el).split()) for el in elements]\n            metadata = {\"file_name\": file_name}\n\n            if additional_metadata is not None:\n                metadata.update(additional_metadata)\n\n            # Create a single document by joining all the texts\n            docs.append(Document(text=\"\\n\\n\".join(text_chunks), metadata=metadata))\n\n        return docs\n</code></pre>"},{"location":"reference/loaders/unstructured_loader/#loaders.unstructured_loader.UnstructuredReader.load_data","title":"load_data","text":"<pre><code>load_data(file, additional_metadata=None, split_documents=False, **kwargs)\n</code></pre> <p>If api is set, parse through api</p> Source code in <code>kotaemon\\loaders\\unstructured_loader.py</code> <pre><code>def load_data(\n    self,\n    file: Path,\n    additional_metadata: Optional[Dict] = None,\n    split_documents: Optional[bool] = False,\n    **kwargs,\n) -&gt; List[Document]:\n    \"\"\"If api is set, parse through api\"\"\"\n    file_path_str = str(file)\n    if self.api:\n        from unstructured.partition.api import partition_via_api\n\n        elements = partition_via_api(\n            filename=file_path_str,\n            api_key=self.api_key,\n            api_url=self.server_url + \"/general/v0/general\",\n        )\n    else:\n        \"\"\"Parse file locally\"\"\"\n        from unstructured.partition.auto import partition\n\n        elements = partition(filename=file_path_str)\n\n    \"\"\" Process elements \"\"\"\n    docs = []\n    file_name = Path(file).name\n    if split_documents:\n        for node in elements:\n            metadata = {\"file_name\": file_name}\n            if hasattr(node, \"metadata\"):\n                \"\"\"Load metadata fields\"\"\"\n                for field, val in vars(node.metadata).items():\n                    if field == \"_known_field_names\":\n                        continue\n                    # removing coordinates because it does not serialize\n                    # and dont want to bother with it\n                    if field == \"coordinates\":\n                        continue\n                    # removing bc it might cause interference\n                    if field == \"parent_id\":\n                        continue\n                    metadata[field] = val\n\n            if additional_metadata is not None:\n                metadata.update(additional_metadata)\n\n            metadata[\"file_name\"] = file_name\n            docs.append(Document(text=node.text, metadata=metadata))\n\n    else:\n        text_chunks = [\" \".join(str(el).split()) for el in elements]\n        metadata = {\"file_name\": file_name}\n\n        if additional_metadata is not None:\n            metadata.update(additional_metadata)\n\n        # Create a single document by joining all the texts\n        docs.append(Document(text=\"\\n\\n\".join(text_chunks), metadata=metadata))\n\n    return docs\n</code></pre>"},{"location":"reference/loaders/utils/","title":"Utils","text":""},{"location":"reference/loaders/utils/box/","title":"Box","text":""},{"location":"reference/loaders/utils/box/#loaders.utils.box.bbox_to_points","title":"bbox_to_points","text":"<pre><code>bbox_to_points(box)\n</code></pre> <p>Convert bounding box to list of points</p> Source code in <code>kotaemon\\loaders\\utils\\box.py</code> <pre><code>def bbox_to_points(box: List[int]):\n    \"\"\"Convert bounding box to list of points\"\"\"\n    x1, y1, x2, y2 = box\n    return [(x1, y1), (x2, y1), (x2, y2), (x1, y2)]\n</code></pre>"},{"location":"reference/loaders/utils/box/#loaders.utils.box.points_to_bbox","title":"points_to_bbox","text":"<pre><code>points_to_bbox(points)\n</code></pre> <p>Convert list of points to bounding box</p> Source code in <code>kotaemon\\loaders\\utils\\box.py</code> <pre><code>def points_to_bbox(points: List[Tuple[int, int]]):\n    \"\"\"Convert list of points to bounding box\"\"\"\n    all_x = [p[0] for p in points]\n    all_y = [p[1] for p in points]\n    return [min(all_x), min(all_y), max(all_x), max(all_y)]\n</code></pre>"},{"location":"reference/loaders/utils/box/#loaders.utils.box.scale_points","title":"scale_points","text":"<pre><code>scale_points(points, scale_factor=1.0)\n</code></pre> <p>Scale points by a scale factor</p> Source code in <code>kotaemon\\loaders\\utils\\box.py</code> <pre><code>def scale_points(points: List[Tuple[int, int]], scale_factor: float = 1.0):\n    \"\"\"Scale points by a scale factor\"\"\"\n    return [(int(pos[0] * scale_factor), int(pos[1] * scale_factor)) for pos in points]\n</code></pre>"},{"location":"reference/loaders/utils/box/#loaders.utils.box.union_points","title":"union_points","text":"<pre><code>union_points(points)\n</code></pre> <p>Return union bounding box of list of points</p> Source code in <code>kotaemon\\loaders\\utils\\box.py</code> <pre><code>def union_points(points: List[Tuple[int, int]]):\n    \"\"\"Return union bounding box of list of points\"\"\"\n    all_x = [p[0] for p in points]\n    all_y = [p[1] for p in points]\n    bbox = (min(all_x), min(all_y), max(all_x), max(all_y))\n    return bbox\n</code></pre>"},{"location":"reference/loaders/utils/box/#loaders.utils.box.scale_box","title":"scale_box","text":"<pre><code>scale_box(box, scale_factor=1.0)\n</code></pre> <p>Scale box by a scale factor</p> Source code in <code>kotaemon\\loaders\\utils\\box.py</code> <pre><code>def scale_box(box: List[int], scale_factor: float = 1.0):\n    \"\"\"Scale box by a scale factor\"\"\"\n    return [int(pos * scale_factor) for pos in box]\n</code></pre>"},{"location":"reference/loaders/utils/box/#loaders.utils.box.box_h","title":"box_h","text":"<pre><code>box_h(box)\n</code></pre> <p>Return box height</p> Source code in <code>kotaemon\\loaders\\utils\\box.py</code> <pre><code>def box_h(box: List[int]):\n    \"Return box height\"\n    return box[3] - box[1]\n</code></pre>"},{"location":"reference/loaders/utils/box/#loaders.utils.box.box_w","title":"box_w","text":"<pre><code>box_w(box)\n</code></pre> <p>Return box width</p> Source code in <code>kotaemon\\loaders\\utils\\box.py</code> <pre><code>def box_w(box: List[int]):\n    \"Return box width\"\n    return box[2] - box[0]\n</code></pre>"},{"location":"reference/loaders/utils/box/#loaders.utils.box.box_area","title":"box_area","text":"<pre><code>box_area(box)\n</code></pre> <p>Return box area</p> Source code in <code>kotaemon\\loaders\\utils\\box.py</code> <pre><code>def box_area(box: List[int]):\n    \"Return box area\"\n    x1, y1, x2, y2 = box\n    return (x2 - x1) * (y2 - y1)\n</code></pre>"},{"location":"reference/loaders/utils/box/#loaders.utils.box.get_rect_iou","title":"get_rect_iou","text":"<pre><code>get_rect_iou(gt_box, pd_box, iou_type=0)\n</code></pre> <p>Intersection over union on layout rectangle</p> <p>Parameters:</p> Name Type Description Default <code>gt_box</code> <code>List[tuple]</code> <p>List[tuple] A list contains bounding box coordinates of ground truth</p> required <code>pd_box</code> <code>List[tuple]</code> <p>List[tuple] A list contains bounding box coordinates of prediction</p> required <code>iou_type</code> <p>int 0: intersection / union, normal IOU 1: intersection / min(areas), useful when boxes are under/over-segmented</p> <code>0</code> <code>Input</code> <code>format</code> <p>[(x1, y1), (x2, y1), (x2, y2), (x1, y2)]</p> required <code>Annotation</code> <code>for each element in bbox</code> required <p>Returns:</p> Type Description <code>int</code> <p>Intersection over union value</p> Source code in <code>kotaemon\\loaders\\utils\\box.py</code> <pre><code>def get_rect_iou(gt_box: List[tuple], pd_box: List[tuple], iou_type=0) -&gt; int:\n    \"\"\"Intersection over union on layout rectangle\n\n    Args:\n        gt_box: List[tuple]\n            A list contains bounding box coordinates of ground truth\n        pd_box: List[tuple]\n            A list contains bounding box coordinates of prediction\n        iou_type: int\n            0: intersection / union, normal IOU\n            1: intersection / min(areas), useful when boxes are under/over-segmented\n\n        Input format: [(x1, y1), (x2, y1), (x2, y2), (x1, y2)]\n        Annotation for each element in bbox:\n        (x1, y1)        (x2, y1)\n            +-------+\n            |       |\n            |       |\n            +-------+\n        (x1, y2)        (x2, y2)\n\n    Returns:\n        Intersection over union value\n    \"\"\"\n\n    assert iou_type in [0, 1], \"Only support 0: origin iou, 1: intersection / min(area)\"\n\n    # determine the (x, y)-coordinates of the intersection rectangle\n    # gt_box: [(x1, y1), (x2, y1), (x2, y2), (x1, y2)]\n    # pd_box: [(x1, y1), (x2, y1), (x2, y2), (x1, y2)]\n    x_left = max(gt_box[0][0], pd_box[0][0])\n    y_top = max(gt_box[0][1], pd_box[0][1])\n    x_right = min(gt_box[2][0], pd_box[2][0])\n    y_bottom = min(gt_box[2][1], pd_box[2][1])\n\n    # compute the area of intersection rectangle\n    interArea = max(0, x_right - x_left) * max(0, y_bottom - y_top)\n\n    # compute the area of both the prediction and ground-truth\n    # rectangles\n    gt_area = (gt_box[2][0] - gt_box[0][0]) * (gt_box[2][1] - gt_box[0][1])\n    pd_area = (pd_box[2][0] - pd_box[0][0]) * (pd_box[2][1] - pd_box[0][1])\n\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    if iou_type == 0:\n        iou = interArea / float(gt_area + pd_area - interArea)\n    elif iou_type == 1:\n        iou = interArea / max(min(gt_area, pd_area), 1)\n\n    # return the intersection over union value\n    return iou\n</code></pre>"},{"location":"reference/loaders/utils/box/#loaders.utils.box.sort_funsd_reading_order","title":"sort_funsd_reading_order","text":"<pre><code>sort_funsd_reading_order(lines, box_key_name='box')\n</code></pre> <p>Sort cell list to create the right reading order using their locations</p> <p>Parameters:</p> Name Type Description Default <code>lines</code> <code>List[dict]</code> <p>list of cells to sort</p> required <p>Returns:</p> Type Description <p>a list of cell lists in the right reading order that contain</p> <p>no key or start with a key and contain no other key</p> Source code in <code>kotaemon\\loaders\\utils\\box.py</code> <pre><code>def sort_funsd_reading_order(lines: List[dict], box_key_name: str = \"box\"):\n    \"\"\"Sort cell list to create the right reading order using their locations\n\n    Args:\n        lines: list of cells to sort\n\n    Returns:\n        a list of cell lists in the right reading order that contain\n        no key or start with a key and contain no other key\n    \"\"\"\n    sorted_list = []\n\n    if len(lines) == 0:\n        return lines\n\n    while len(lines) &gt; 1:\n        topleft_line = lines[0]\n        for line in lines[1:]:\n            topleft_line_pos = topleft_line[box_key_name]\n            topleft_line_center_y = (topleft_line_pos[1] + topleft_line_pos[3]) / 2\n            x1, y1, x2, y2 = line[box_key_name]\n            box_center_x = (x1 + x2) / 2\n            box_center_y = (y1 + y2) / 2\n            cell_h = y2 - y1\n            if box_center_y &lt;= topleft_line_center_y - cell_h / 2:\n                topleft_line = line\n                continue\n            if (\n                box_center_x &lt; topleft_line_pos[2]\n                and box_center_y &lt; topleft_line_pos[3]\n            ):\n                topleft_line = line\n                continue\n        sorted_list.append(topleft_line)\n        lines.remove(topleft_line)\n\n    sorted_list.append(lines[0])\n\n    return sorted_list\n</code></pre>"},{"location":"reference/loaders/utils/pdf_ocr/","title":"Pdf Ocr","text":""},{"location":"reference/loaders/utils/pdf_ocr/#loaders.utils.pdf_ocr.read_pdf_unstructured","title":"read_pdf_unstructured","text":"<pre><code>read_pdf_unstructured(input_path)\n</code></pre> <p>Convert PDF from specified path to list of text items with location information</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[Path, str]</code> <p>path to input file</p> required <p>Returns:</p> Type Description <p>Dict page_number: list of text boxes</p> Source code in <code>kotaemon\\loaders\\utils\\pdf_ocr.py</code> <pre><code>def read_pdf_unstructured(input_path: Union[Path, str]):\n    \"\"\"Convert PDF from specified path to list of text items with\n    location information\n\n    Args:\n        input_path: path to input file\n\n    Returns:\n        Dict page_number: list of text boxes\n    \"\"\"\n    try:\n        from unstructured.partition.auto import partition\n    except ImportError:\n        raise ImportError(\n            \"Please install unstructured PDF reader \\\n              `pip install unstructured[pdf]`\"\n        )\n\n    page_items = defaultdict(list)\n    items = partition(input_path)\n    for item in items:\n        page_number = item.metadata.page_number\n        bbox = points_to_bbox(item.metadata.coordinates.points)\n        coord_system = item.metadata.coordinates.system\n        max_w, max_h = coord_system.width, coord_system.height\n        page_items[page_number - 1].append(\n            {\n                \"text\": item.text,\n                \"box\": bbox,\n                \"location\": bbox_to_points(bbox),\n                \"page_shape\": (max_w, max_h),\n            }\n        )\n\n    return page_items\n</code></pre>"},{"location":"reference/loaders/utils/pdf_ocr/#loaders.utils.pdf_ocr.merge_ocr_and_pdf_texts","title":"merge_ocr_and_pdf_texts","text":"<pre><code>merge_ocr_and_pdf_texts(ocr_list, pdf_text_list, debug_info=None)\n</code></pre> <p>Merge PDF and OCR text using IOU overlaping location Args:     ocr_list: List of OCR items {\"text\", \"box\", \"location\"}     pdf_text_list: List of PDF items {\"text\", \"box\", \"location\"}</p> <p>Returns:</p> Type Description <p>Combined list of PDF text and non-overlap OCR text</p> Source code in <code>kotaemon\\loaders\\utils\\pdf_ocr.py</code> <pre><code>def merge_ocr_and_pdf_texts(\n    ocr_list: List[dict], pdf_text_list: List[dict], debug_info=None\n):\n    \"\"\"Merge PDF and OCR text using IOU overlaping location\n    Args:\n        ocr_list: List of OCR items {\"text\", \"box\", \"location\"}\n        pdf_text_list: List of PDF items {\"text\", \"box\", \"location\"}\n\n    Returns:\n        Combined list of PDF text and non-overlap OCR text\n    \"\"\"\n    not_matched_ocr = []\n\n    # check for debug info\n    if debug_info is not None:\n        cv2, debug_im = debug_info\n\n    for ocr_item in ocr_list:\n        matched = False\n        for pdf_item in pdf_text_list:\n            if (\n                get_rect_iou(ocr_item[\"location\"], pdf_item[\"location\"], iou_type=1)\n                &gt; IOU_THRES\n            ):\n                matched = True\n                break\n\n        color = (255, 0, 0)\n        if not matched:\n            ocr_item[\"matched\"] = False\n            not_matched_ocr.append(ocr_item)\n            color = (0, 255, 255)\n\n        if debug_info is not None:\n            cv2.rectangle(\n                debug_im,\n                ocr_item[\"location\"][0],\n                ocr_item[\"location\"][2],\n                color=color,\n                thickness=1,\n            )\n\n    if debug_info is not None:\n        for pdf_item in pdf_text_list:\n            cv2.rectangle(\n                debug_im,\n                pdf_item[\"location\"][0],\n                pdf_item[\"location\"][2],\n                color=(0, 255, 0),\n                thickness=2,\n            )\n\n    return pdf_text_list + not_matched_ocr\n</code></pre>"},{"location":"reference/loaders/utils/pdf_ocr/#loaders.utils.pdf_ocr.merge_table_cell_and_ocr","title":"merge_table_cell_and_ocr","text":"<pre><code>merge_table_cell_and_ocr(table_list, ocr_list, pdf_list, debug_info=None)\n</code></pre> <p>Merge table items with OCR text using IOU overlaping location Args:     table_list: List of table items         \"type\": (\"table\", \"cell\", \"text\"), \"text\", \"box\", \"location\"}     ocr_list: List of OCR items {\"text\", \"box\", \"location\"}     pdf_list: List of PDF items {\"text\", \"box\", \"location\"}</p> <p>Returns:</p> Name Type Description <code>all_table_cells</code> <p>List of tables, each of table is reprented by list of cells with combined text from OCR</p> <code>not_matched_items</code> <p>List of PDF text which is not overlapped by table region</p> Source code in <code>kotaemon\\loaders\\utils\\pdf_ocr.py</code> <pre><code>def merge_table_cell_and_ocr(\n    table_list: List[dict], ocr_list: List[dict], pdf_list: List[dict], debug_info=None\n):\n    \"\"\"Merge table items with OCR text using IOU overlaping location\n    Args:\n        table_list: List of table items\n            \"type\": (\"table\", \"cell\", \"text\"), \"text\", \"box\", \"location\"}\n        ocr_list: List of OCR items {\"text\", \"box\", \"location\"}\n        pdf_list: List of PDF items {\"text\", \"box\", \"location\"}\n\n    Returns:\n        all_table_cells: List of tables, each of table is reprented\n            by list of cells with combined text from OCR\n        not_matched_items: List of PDF text which is not overlapped by table region\n    \"\"\"\n    # check for debug info\n    if debug_info is not None:\n        cv2, debug_im = debug_info\n\n    cell_list = [item for item in table_list if item[\"type\"] == \"cell\"]\n    table_list = [item for item in table_list if item[\"type\"] == \"table\"]\n\n    # sort table by area\n    table_list = sorted(table_list, key=lambda item: box_area(item[\"bbox\"]))\n\n    all_tables = []\n    matched_pdf_ids = []\n    matched_cell_ids = []\n\n    for table in table_list:\n        if debug_info is not None:\n            cv2.rectangle(\n                debug_im,\n                table[\"location\"][0],\n                table[\"location\"][2],\n                color=[0, 0, 255],\n                thickness=5,\n            )\n\n        cur_table_cells = []\n        for cell_id, cell in enumerate(cell_list):\n            if cell_id in matched_cell_ids:\n                continue\n\n            if get_rect_iou(\n                table[\"location\"], cell[\"location\"], iou_type=1\n            ) &gt; IOU_THRES and box_area(table[\"bbox\"]) &gt; box_area(cell[\"bbox\"]):\n                color = [128, 0, 128]\n                # cell matched to table\n                for item_list, item_type in [(pdf_list, \"pdf\"), (ocr_list, \"ocr\")]:\n                    cell[\"ocr\"] = []\n                    for item_id, item in enumerate(item_list):\n                        if item_type == \"pdf\" and item_id in matched_pdf_ids:\n                            continue\n                        if (\n                            get_rect_iou(item[\"location\"], cell[\"location\"], iou_type=1)\n                            &gt; IOU_THRES\n                        ):\n                            cell[\"ocr\"].append(item)\n                            if item_type == \"pdf\":\n                                matched_pdf_ids.append(item_id)\n\n                    if len(cell[\"ocr\"]) &gt; 0:\n                        # check if union of matched ocr does\n                        # not extend over cell boundary,\n                        # if True, continue to use OCR_list to match\n                        all_box_points_in_cell = []\n                        for item in cell[\"ocr\"]:\n                            all_box_points_in_cell.extend(item[\"location\"])\n                        union_box = union_points(all_box_points_in_cell)\n                        cell_okay = (\n                            box_h(union_box) &lt;= box_h(cell[\"bbox\"]) * PADDING_THRES\n                            and box_w(union_box) &lt;= box_w(cell[\"bbox\"]) * PADDING_THRES\n                        )\n                    else:\n                        cell_okay = False\n\n                    if cell_okay:\n                        if item_type == \"pdf\":\n                            color = [255, 0, 255]\n                        break\n\n                if debug_info is not None:\n                    cv2.rectangle(\n                        debug_im,\n                        cell[\"location\"][0],\n                        cell[\"location\"][2],\n                        color=color,\n                        thickness=3,\n                    )\n\n                matched_cell_ids.append(cell_id)\n                cur_table_cells.append(cell)\n\n        all_tables.append(cur_table_cells)\n\n    not_matched_items = [\n        item for _id, item in enumerate(pdf_list) if _id not in matched_pdf_ids\n    ]\n    if debug_info is not None:\n        for item in not_matched_items:\n            cv2.rectangle(\n                debug_im,\n                item[\"location\"][0],\n                item[\"location\"][2],\n                color=[128, 128, 128],\n                thickness=3,\n            )\n\n    return all_tables, not_matched_items\n</code></pre>"},{"location":"reference/loaders/utils/pdf_ocr/#loaders.utils.pdf_ocr.parse_ocr_output","title":"parse_ocr_output","text":"<pre><code>parse_ocr_output(ocr_page_items, pdf_page_items, artifact_path=None, debug_path=None)\n</code></pre> <p>Main function to combine OCR output and PDF text to form list of table / non-table regions Args:     ocr_page_items: List of OCR items by page     pdf_page_items: Dict of PDF texts (page number as key)     debug_path: If specified, use OpenCV to plot debug image and save to debug_path</p> Source code in <code>kotaemon\\loaders\\utils\\pdf_ocr.py</code> <pre><code>def parse_ocr_output(\n    ocr_page_items: List[dict],\n    pdf_page_items: Dict[int, List[dict]],\n    artifact_path: Optional[str] = None,\n    debug_path: Optional[str] = None,\n):\n    \"\"\"Main function to combine OCR output and PDF text to\n    form list of table / non-table regions\n    Args:\n        ocr_page_items: List of OCR items by page\n        pdf_page_items: Dict of PDF texts (page number as key)\n        debug_path: If specified, use OpenCV to plot debug image and save to debug_path\n    \"\"\"\n    all_tables = []\n    all_texts = []\n\n    for page_id, page in enumerate(ocr_page_items):\n        ocr_list = page[\"json\"][\"ocr\"]\n        table_list = page[\"json\"][\"table\"]\n        page_shape = page[\"image_shape\"]\n        pdf_item_list = pdf_page_items[page_id]\n\n        # create bbox additional information\n        for item in ocr_list:\n            item[\"box\"] = points_to_bbox(item[\"location\"])\n\n        # re-scale pdf items according to new image size\n        for item in pdf_item_list:\n            scale_factor = page_shape[0] / item[\"page_shape\"][0]\n            item[\"box\"] = scale_box(item[\"box\"], scale_factor=scale_factor)\n            item[\"location\"] = scale_points(item[\"location\"], scale_factor=scale_factor)\n\n        # if using debug mode, openCV must be installed\n        if debug_path and artifact_path is not None:\n            try:\n                import cv2\n            except ImportError:\n                raise ImportError(\n                    \"Please install openCV first to use OCRReader debug mode\"\n                )\n            image_path = Path(artifact_path) / page[\"image\"]\n            image = cv2.imread(str(image_path))\n            debug_info = (cv2, image)\n        else:\n            debug_info = None\n\n        new_pdf_list = merge_ocr_and_pdf_texts(\n            ocr_list, pdf_item_list, debug_info=debug_info\n        )\n\n        # sort by reading order\n        ocr_list = sort_funsd_reading_order(ocr_list)\n        new_pdf_list = sort_funsd_reading_order(new_pdf_list)\n\n        all_table_cells, non_table_text_list = merge_table_cell_and_ocr(\n            table_list, ocr_list, new_pdf_list, debug_info=debug_info\n        )\n\n        table_texts = [table_cells_to_markdown(cells) for cells in all_table_cells]\n        all_tables.extend([(page_id, text) for text in table_texts])\n        all_texts.append(\n            (page_id, \" \".join(item[\"text\"] for item in non_table_text_list))\n        )\n\n        # export debug image to debug_path\n        if debug_path:\n            cv2.imwrite(str(Path(debug_path) / \"page_{}.png\".format(page_id)), image)\n\n    return all_tables, all_texts\n</code></pre>"},{"location":"reference/loaders/utils/table/","title":"Table","text":""},{"location":"reference/loaders/utils/table/#loaders.utils.table.check_col_conflicts","title":"check_col_conflicts","text":"<pre><code>check_col_conflicts(col_a, col_b, thres=0.15)\n</code></pre> <p>Check if 2 columns A and B has non-empty content in the same row (to be used with merge_cols)</p> <p>Parameters:</p> Name Type Description Default <code>col_a</code> <code>List[str]</code> <p>column A (list of str)</p> required <code>col_b</code> <code>List[str]</code> <p>column B (list of str)</p> required <code>thres</code> <code>float</code> <p>percentage of overlapping allowed</p> <code>0.15</code> <p>Returns:     if number of overlapping greater than threshold</p> Source code in <code>kotaemon\\loaders\\utils\\table.py</code> <pre><code>def check_col_conflicts(\n    col_a: List[str], col_b: List[str], thres: float = 0.15\n) -&gt; bool:\n    \"\"\"Check if 2 columns A and B has non-empty content in the same row\n    (to be used with merge_cols)\n\n    Args:\n        col_a: column A (list of str)\n        col_b: column B (list of str)\n        thres: percentage of overlapping allowed\n    Returns:\n        if number of overlapping greater than threshold\n    \"\"\"\n    num_rows = len([cell for cell in col_a if cell])\n    assert len(col_a) == len(col_b)\n    conflict_count = 0\n    for cell_a, cell_b in zip(col_a, col_b):\n        if cell_a and cell_b:\n            conflict_count += 1\n    return conflict_count &gt; num_rows * thres\n</code></pre>"},{"location":"reference/loaders/utils/table/#loaders.utils.table.merge_cols","title":"merge_cols","text":"<pre><code>merge_cols(col_a, col_b)\n</code></pre> <p>Merge column A and B if they do not have conflict rows</p> <p>Parameters:</p> Name Type Description Default <code>col_a</code> <code>List[str]</code> <p>column A (list of str)</p> required <code>col_b</code> <code>List[str]</code> <p>column B (list of str)</p> required <p>Returns:     merged column</p> Source code in <code>kotaemon\\loaders\\utils\\table.py</code> <pre><code>def merge_cols(col_a: List[str], col_b: List[str]) -&gt; List[str]:\n    \"\"\"Merge column A and B if they do not have conflict rows\n\n    Args:\n        col_a: column A (list of str)\n        col_b: column B (list of str)\n    Returns:\n        merged column\n    \"\"\"\n    for r_id in range(len(col_a)):\n        if col_b[r_id]:\n            col_a[r_id] = col_a[r_id] + \" \" + col_b[r_id]\n    return col_a\n</code></pre>"},{"location":"reference/loaders/utils/table/#loaders.utils.table.add_index_col","title":"add_index_col","text":"<pre><code>add_index_col(csv_rows)\n</code></pre> <p>Add index column as the first column of the table csv_rows</p> <p>Parameters:</p> Name Type Description Default <code>csv_rows</code> <code>List[List[str]]</code> <p>input table</p> required <p>Returns:     output table with index column</p> Source code in <code>kotaemon\\loaders\\utils\\table.py</code> <pre><code>def add_index_col(csv_rows: List[List[str]]) -&gt; List[List[str]]:\n    \"\"\"Add index column as the first column of the table csv_rows\n\n    Args:\n        csv_rows: input table\n    Returns:\n        output table with index column\n    \"\"\"\n    new_csv_rows = [[\"row id\"] + [\"\"] * len(csv_rows[0])]\n    for r_id, row in enumerate(csv_rows):\n        new_csv_rows.append([str(r_id + 1)] + row)\n    return new_csv_rows\n</code></pre>"},{"location":"reference/loaders/utils/table/#loaders.utils.table.compress_csv","title":"compress_csv","text":"<pre><code>compress_csv(csv_rows)\n</code></pre> <p>Compress table csv_rows by merging sparse columns (merge_cols)</p> <p>Parameters:</p> Name Type Description Default <code>csv_rows</code> <code>List[List[str]]</code> <p>input table</p> required <p>Returns:     output: compressed table</p> Source code in <code>kotaemon\\loaders\\utils\\table.py</code> <pre><code>def compress_csv(csv_rows: List[List[str]]) -&gt; List[List[str]]:\n    \"\"\"Compress table csv_rows by merging sparse columns (merge_cols)\n\n    Args:\n        csv_rows: input table\n    Returns:\n        output: compressed table\n    \"\"\"\n    csv_cols = [[r[c_id] for r in csv_rows] for c_id in range(len(csv_rows[0]))]\n    to_remove_col_ids = []\n    last_c_id = 0\n    for c_id in range(1, len(csv_cols)):\n        if not check_col_conflicts(csv_cols[last_c_id], csv_cols[c_id]):\n            to_remove_col_ids.append(c_id)\n            csv_cols[last_c_id] = merge_cols(csv_cols[last_c_id], csv_cols[c_id])\n        else:\n            last_c_id = c_id\n\n    csv_cols = [r for c_id, r in enumerate(csv_cols) if c_id not in to_remove_col_ids]\n    csv_rows = [[c[r_id] for c in csv_cols] for r_id in range(len(csv_cols[0]))]\n    return csv_rows\n</code></pre>"},{"location":"reference/loaders/utils/table/#loaders.utils.table.get_table_from_ocr","title":"get_table_from_ocr","text":"<pre><code>get_table_from_ocr(ocr_list, table_list)\n</code></pre> <p>Get list of text lines belong to table regions specified by table_list</p> <p>Parameters:</p> Name Type Description Default <code>ocr_list</code> <code>List[dict]</code> <p>list of OCR output in Casia format (Flax)</p> required <code>table_list</code> <code>List[dict]</code> <p>list of table output in Casia format (Flax)</p> required <p>Returns:</p> Name Type Description <code>_type_</code> <p>description</p> Source code in <code>kotaemon\\loaders\\utils\\table.py</code> <pre><code>def get_table_from_ocr(ocr_list: List[dict], table_list: List[dict]):\n    \"\"\"Get list of text lines belong to table regions specified by table_list\n\n    Args:\n        ocr_list: list of OCR output in Casia format (Flax)\n        table_list: list of table output in Casia format (Flax)\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    table_texts = []\n    for table in table_list:\n        if table[\"type\"] != \"table\":\n            continue\n        cur_table_texts = []\n        for ocr in ocr_list:\n            _iou = get_rect_iou(table[\"location\"], ocr[\"location\"], iou_type=1)\n            if _iou &gt; 0.8:\n                cur_table_texts.append(ocr[\"text\"])\n        table_texts.append(cur_table_texts)\n\n    return table_texts\n</code></pre>"},{"location":"reference/loaders/utils/table/#loaders.utils.table.make_markdown_table","title":"make_markdown_table","text":"<pre><code>make_markdown_table(array)\n</code></pre> <p>Convert table rows in list format to markdown string</p> <p>Parameters:</p> Name Type Description Default <code>Example</code> <code>Input</code> <pre><code>[[\"Name\", \"Age\", \"Height\"],\n[\"Jake\", 20, 5'10],\n[\"Mary\", 21, 5'7]]\n</code></pre> required <p>Returns:     String to put into a .md file</p> Source code in <code>kotaemon\\loaders\\utils\\table.py</code> <pre><code>def make_markdown_table(array: List[List[str]]) -&gt; str:\n    \"\"\"Convert table rows in list format to markdown string\n\n    Args:\n        Python list with rows of table as lists\n        First element as header.\n        Example Input:\n                [[\"Name\", \"Age\", \"Height\"],\n                [\"Jake\", 20, 5'10],\n                [\"Mary\", 21, 5'7]]\n    Returns:\n        String to put into a .md file\n    \"\"\"\n    array = compress_csv(array)\n    array = add_index_col(array)\n    markdown = \"\\n\" + str(\"| \")\n\n    for e in array[0]:\n        to_add = \" \" + str(e) + str(\" |\")\n        markdown += to_add\n    markdown += \"\\n\"\n\n    markdown += \"| \"\n    for i in range(len(array[0])):\n        markdown += str(\"--- | \")\n    markdown += \"\\n\"\n\n    for entry in array[1:]:\n        markdown += str(\"| \")\n        for e in entry:\n            to_add = str(e) + str(\" | \")\n            markdown += to_add\n        markdown += \"\\n\"\n\n    return markdown + \"\\n\"\n</code></pre>"},{"location":"reference/loaders/utils/table/#loaders.utils.table.parse_csv_string_to_list","title":"parse_csv_string_to_list","text":"<pre><code>parse_csv_string_to_list(csv_str)\n</code></pre> <p>Convert CSV string to list of rows</p> <p>Parameters:</p> Name Type Description Default <code>csv_str</code> <code>str</code> <p>input CSV string</p> required <p>Returns:</p> Type Description <code>List[List[str]]</code> <p>Output table in list format</p> Source code in <code>kotaemon\\loaders\\utils\\table.py</code> <pre><code>def parse_csv_string_to_list(csv_str: str) -&gt; List[List[str]]:\n    \"\"\"Convert CSV string to list of rows\n\n    Args:\n        csv_str: input CSV string\n\n    Returns:\n        Output table in list format\n    \"\"\"\n    io = StringIO(csv_str)\n    csv_reader = csv.reader(io, delimiter=\",\")\n    rows = [row for row in csv_reader]\n    return rows\n</code></pre>"},{"location":"reference/loaders/utils/table/#loaders.utils.table.format_cell","title":"format_cell","text":"<pre><code>format_cell(cell, length_limit=None)\n</code></pre> <p>Format cell content by remove redundant character and enforce length limit</p> <p>Parameters:</p> Name Type Description Default <code>cell</code> <code>str</code> <p>input cell text</p> required <code>length_limit</code> <code>Optional[int]</code> <p>limit of text length.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>new cell text</p> Source code in <code>kotaemon\\loaders\\utils\\table.py</code> <pre><code>def format_cell(cell: str, length_limit: Optional[int] = None) -&gt; str:\n    \"\"\"Format cell content by remove redundant character and enforce length limit\n\n    Args:\n        cell: input cell text\n        length_limit: limit of text length.\n\n    Returns:\n        new cell text\n    \"\"\"\n    cell = cell.replace(\"\\n\", \" \")\n    if length_limit:\n        cell = cell[:length_limit]\n    return cell\n</code></pre>"},{"location":"reference/loaders/utils/table/#loaders.utils.table.extract_tables_from_csv_string","title":"extract_tables_from_csv_string","text":"<pre><code>extract_tables_from_csv_string(csv_content, table_texts)\n</code></pre> <p>Extract list of table from FullOCR output (csv_content) with the specified table_texts</p> <p>Parameters:</p> Name Type Description Default <code>csv_content</code> <code>str</code> <p>CSV output from FullOCR pipeline</p> required <code>table_texts</code> <code>List[List[str]]</code> <p>list of table texts extracted</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], str]</code> <p>List of tables and non-text content</p> Source code in <code>kotaemon\\loaders\\utils\\table.py</code> <pre><code>def extract_tables_from_csv_string(\n    csv_content: str, table_texts: List[List[str]]\n) -&gt; Tuple[List[str], str]:\n    \"\"\"Extract list of table from FullOCR output\n    (csv_content) with the specified table_texts\n\n    Args:\n        csv_content: CSV output from FullOCR pipeline\n        table_texts: list of table texts extracted\n        from get_table_from_ocr()\n\n    Returns:\n        List of tables and non-text content\n    \"\"\"\n    rows = parse_csv_string_to_list(csv_content)\n    used_row_ids = []\n    table_csv_list = []\n    for table in table_texts:\n        cur_rows = []\n        for row_id, row in enumerate(rows):\n            scores = [\n                any(cell in cell_reference for cell in table)\n                for cell_reference in row\n                if cell_reference\n            ]\n            score = sum(scores) / len(scores)\n            if score &gt; 0.5 and row_id not in used_row_ids:\n                used_row_ids.append(row_id)\n                cur_rows.append([format_cell(cell) for cell in row])\n        if cur_rows:\n            table_csv_list.append(make_markdown_table(cur_rows))\n        else:\n            print(\"table not matched\", table)\n\n    non_table_rows = [\n        row for row_id, row in enumerate(rows) if row_id not in used_row_ids\n    ]\n    non_table_text = \"\\n\".join(\n        \" \".join(format_cell(cell) for cell in row) for row in non_table_rows\n    )\n    return table_csv_list, non_table_text\n</code></pre>"},{"location":"reference/loaders/utils/table/#loaders.utils.table.strip_special_chars_markdown","title":"strip_special_chars_markdown","text":"<pre><code>strip_special_chars_markdown(text)\n</code></pre> <p>Strip special characters from input text in markdown table format</p> Source code in <code>kotaemon\\loaders\\utils\\table.py</code> <pre><code>def strip_special_chars_markdown(text: str) -&gt; str:\n    \"\"\"Strip special characters from input text in markdown table format\"\"\"\n    return text.replace(\"|\", \"\").replace(\":---:\", \"\").replace(\"---\", \"\")\n</code></pre>"},{"location":"reference/loaders/utils/table/#loaders.utils.table.parse_markdown_text_to_tables","title":"parse_markdown_text_to_tables","text":"<pre><code>parse_markdown_text_to_tables(text)\n</code></pre> <p>Convert markdown text to list of non-table spans and table spans</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>input markdown text</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[str]]</code> <p>list of table spans and non-table spans</p> Source code in <code>kotaemon\\loaders\\utils\\table.py</code> <pre><code>def parse_markdown_text_to_tables(text: str) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"Convert markdown text to list of non-table spans and table spans\n\n    Args:\n        text: input markdown text\n\n    Returns:\n        list of table spans and non-table spans\n    \"\"\"\n    # init empty tables and texts list\n    tables = []\n    texts = []\n\n    # split input by line break\n    lines = text.split(\"\\n\")\n    cur_table = []\n    cur_text: List[str] = []\n    for line in lines:\n        line = line.strip()\n        if line.startswith(\"|\"):\n            if len(cur_text) &gt; 0:\n                texts.append(cur_text)\n                cur_text = []\n            cur_table.append(line)\n        else:\n            # add new table to the list\n            if len(cur_table) &gt; 0:\n                tables.append(cur_table)\n                cur_table = []\n            cur_text.append(line)\n\n    table_texts = [\"\\n\".join(table) for table in tables]\n    non_table_texts = [\"\\n\".join(text) for text in texts]\n    return table_texts, non_table_texts\n</code></pre>"},{"location":"reference/loaders/utils/table/#loaders.utils.table.table_cells_to_markdown","title":"table_cells_to_markdown","text":"<pre><code>table_cells_to_markdown(cells)\n</code></pre> <p>Convert list of cells with attached text to Markdown table</p> Source code in <code>kotaemon\\loaders\\utils\\table.py</code> <pre><code>def table_cells_to_markdown(cells: List[dict]):\n    \"\"\"Convert list of cells with attached text to Markdown table\"\"\"\n\n    if len(cells) == 0:\n        return \"\"\n\n    all_row_ids = []\n    all_col_ids = []\n    for cell in cells:\n        all_row_ids.extend(cell[\"rows\"])\n        all_col_ids.extend(cell[\"columns\"])\n\n    num_rows, num_cols = max(all_row_ids) + 1, max(all_col_ids) + 1\n    table_rows = [[\"\" for c in range(num_cols)] for r in range(num_rows)]\n\n    # start filling in the grid\n    for cell in cells:\n        cell_text = \" \".join(item[\"text\"] for item in cell[\"ocr\"])\n        start_row_id, end_row_id = cell[\"rows\"]\n        start_col_id, end_col_id = cell[\"columns\"]\n        span_cell = end_row_id != start_row_id or end_col_id != start_col_id\n\n        # do not repeat long text in span cell to prevent context length issue\n        if span_cell and len(cell_text.replace(\" \", \"\")) &lt; 20 and start_row_id &gt; 0:\n            for row in range(start_row_id, end_row_id + 1):\n                for col in range(start_col_id, end_col_id + 1):\n                    table_rows[row][col] += cell_text + \" \"\n        else:\n            table_rows[start_row_id][start_col_id] += cell_text + \" \"\n\n    return make_markdown_table(table_rows)\n</code></pre>"},{"location":"reference/parsers/","title":"Parsers","text":""},{"location":"reference/parsers/#parsers.RegexExtractor","title":"RegexExtractor","text":"<p>             Bases: <code>BaseComponent</code></p> <p>Simple class for extracting text from a document using a regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>List[str]</code> <p>The regex pattern(s) to use.</p> required <code>output_map</code> <code>dict</code> <p>A mapping from extracted text to the desired output. Defaults to None.</p> required Source code in <code>kotaemon\\parsers\\regex_extractor.py</code> <pre><code>class RegexExtractor(BaseComponent):\n    \"\"\"\n    Simple class for extracting text from a document using a regex pattern.\n\n    Args:\n        pattern (List[str]): The regex pattern(s) to use.\n        output_map (dict, optional): A mapping from extracted text to the\n            desired output. Defaults to None.\n    \"\"\"\n\n    class Config:\n        middleware_switches = {\"theflow.middleware.CachingMiddleware\": False}\n\n    pattern: list[str]\n    output_map: dict[str, str] | Callable[[str], str] = Param(\n        default_callback=lambda *_: {}\n    )\n\n    def __init__(self, pattern: str | list[str], **kwargs):\n        if isinstance(pattern, str):\n            pattern = [pattern]\n        super().__init__(pattern=pattern, **kwargs)\n\n    @staticmethod\n    def run_raw_static(pattern: str, text: str) -&gt; list[str]:\n        \"\"\"\n        Finds all non-overlapping occurrences of a pattern in a string.\n\n        Parameters:\n            pattern (str): The regular expression pattern to search for.\n            text (str): The input string to search in.\n\n        Returns:\n            List[str]: A list of all non-overlapping occurrences of the pattern in the\n                string.\n        \"\"\"\n        return re.findall(pattern, text)\n\n    @staticmethod\n    def map_output(text, output_map) -&gt; str:\n        \"\"\"\n        Maps the given `text` to its corresponding value in the `output_map` dictionary.\n\n        Parameters:\n            text (str): The input text to be mapped.\n            output_map (dict): A dictionary containing mapping of input text to output\n                values.\n\n        Returns:\n            str: The corresponding value from the `output_map` if `text` is found in the\n                dictionary, otherwise returns the original `text`.\n        \"\"\"\n        if not output_map:\n            return text\n\n        if isinstance(output_map, dict):\n            return output_map.get(text, text)\n\n        return output_map(text)\n\n    def run_raw(self, text: str) -&gt; ExtractorOutput:\n        \"\"\"\n        Matches the raw text against the pattern and rans the output mapping, returning\n            an instance of ExtractorOutput.\n\n        Args:\n            text (str): The raw text to be processed.\n\n        Returns:\n            ExtractorOutput: The processed output as a list of ExtractorOutput.\n        \"\"\"\n        output: list[str] = sum(\n            [self.run_raw_static(p, text) for p in self.pattern], []\n        )\n        output = [self.map_output(text, self.output_map) for text in output]\n\n        return ExtractorOutput(\n            text=output[0] if output else \"\",\n            matches=output,\n            metadata={\"origin\": \"RegexExtractor\"},\n        )\n\n    def run(\n        self, text: str | list[str] | Document | list[Document]\n    ) -&gt; list[ExtractorOutput]:\n        \"\"\"Match the input against a pattern and return the output for each input\n\n        Parameters:\n            text: contains the input string to be processed\n\n        Returns:\n            A list contains the output ExtractorOutput for each input\n\n        Example:\n            ```pycon\n            &gt;&gt;&gt; document1 = Document(...)\n            &gt;&gt;&gt; document2 = Document(...)\n            &gt;&gt;&gt; document_batch = [document1, document2]\n            &gt;&gt;&gt; batch_output = self(document_batch)\n            &gt;&gt;&gt; print(batch_output)\n            [output1_document1, output1_document2]\n            ```\n        \"\"\"\n        # TODO: this conversion seems common\n        input_: list[str] = []\n        if not isinstance(text, list):\n            text = [text]\n\n        for item in text:\n            if isinstance(item, str):\n                input_.append(item)\n            elif isinstance(item, Document):\n                input_.append(item.text)\n            else:\n                raise ValueError(\n                    f\"Invalid input type {type(item)}, should be str or Document\"\n                )\n\n        output = []\n        for each_input in input_:\n            output.append(self.run_raw(each_input))\n\n        return output\n</code></pre>"},{"location":"reference/parsers/#parsers.RegexExtractor.run_raw_static","title":"run_raw_static  <code>staticmethod</code>","text":"<pre><code>run_raw_static(pattern, text)\n</code></pre> <p>Finds all non-overlapping occurrences of a pattern in a string.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>The regular expression pattern to search for.</p> required <code>text</code> <code>str</code> <p>The input string to search in.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: A list of all non-overlapping occurrences of the pattern in the string.</p> Source code in <code>kotaemon\\parsers\\regex_extractor.py</code> <pre><code>@staticmethod\ndef run_raw_static(pattern: str, text: str) -&gt; list[str]:\n    \"\"\"\n    Finds all non-overlapping occurrences of a pattern in a string.\n\n    Parameters:\n        pattern (str): The regular expression pattern to search for.\n        text (str): The input string to search in.\n\n    Returns:\n        List[str]: A list of all non-overlapping occurrences of the pattern in the\n            string.\n    \"\"\"\n    return re.findall(pattern, text)\n</code></pre>"},{"location":"reference/parsers/#parsers.RegexExtractor.map_output","title":"map_output  <code>staticmethod</code>","text":"<pre><code>map_output(text, output_map)\n</code></pre> <p>Maps the given <code>text</code> to its corresponding value in the <code>output_map</code> dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to be mapped.</p> required <code>output_map</code> <code>dict</code> <p>A dictionary containing mapping of input text to output values.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The corresponding value from the <code>output_map</code> if <code>text</code> is found in the dictionary, otherwise returns the original <code>text</code>.</p> Source code in <code>kotaemon\\parsers\\regex_extractor.py</code> <pre><code>@staticmethod\ndef map_output(text, output_map) -&gt; str:\n    \"\"\"\n    Maps the given `text` to its corresponding value in the `output_map` dictionary.\n\n    Parameters:\n        text (str): The input text to be mapped.\n        output_map (dict): A dictionary containing mapping of input text to output\n            values.\n\n    Returns:\n        str: The corresponding value from the `output_map` if `text` is found in the\n            dictionary, otherwise returns the original `text`.\n    \"\"\"\n    if not output_map:\n        return text\n\n    if isinstance(output_map, dict):\n        return output_map.get(text, text)\n\n    return output_map(text)\n</code></pre>"},{"location":"reference/parsers/#parsers.RegexExtractor.run_raw","title":"run_raw","text":"<pre><code>run_raw(text)\n</code></pre> <p>Matches the raw text against the pattern and rans the output mapping, returning     an instance of ExtractorOutput.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The raw text to be processed.</p> required <p>Returns:</p> Name Type Description <code>ExtractorOutput</code> <code>ExtractorOutput</code> <p>The processed output as a list of ExtractorOutput.</p> Source code in <code>kotaemon\\parsers\\regex_extractor.py</code> <pre><code>def run_raw(self, text: str) -&gt; ExtractorOutput:\n    \"\"\"\n    Matches the raw text against the pattern and rans the output mapping, returning\n        an instance of ExtractorOutput.\n\n    Args:\n        text (str): The raw text to be processed.\n\n    Returns:\n        ExtractorOutput: The processed output as a list of ExtractorOutput.\n    \"\"\"\n    output: list[str] = sum(\n        [self.run_raw_static(p, text) for p in self.pattern], []\n    )\n    output = [self.map_output(text, self.output_map) for text in output]\n\n    return ExtractorOutput(\n        text=output[0] if output else \"\",\n        matches=output,\n        metadata={\"origin\": \"RegexExtractor\"},\n    )\n</code></pre>"},{"location":"reference/parsers/#parsers.RegexExtractor.run","title":"run","text":"<pre><code>run(text)\n</code></pre> <p>Match the input against a pattern and return the output for each input</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | list[str] | Document | list[Document]</code> <p>contains the input string to be processed</p> required <p>Returns:</p> Type Description <code>list[ExtractorOutput]</code> <p>A list contains the output ExtractorOutput for each input</p> Example <pre><code>&gt;&gt;&gt; document1 = Document(...)\n&gt;&gt;&gt; document2 = Document(...)\n&gt;&gt;&gt; document_batch = [document1, document2]\n&gt;&gt;&gt; batch_output = self(document_batch)\n&gt;&gt;&gt; print(batch_output)\n[output1_document1, output1_document2]\n</code></pre> Source code in <code>kotaemon\\parsers\\regex_extractor.py</code> <pre><code>def run(\n    self, text: str | list[str] | Document | list[Document]\n) -&gt; list[ExtractorOutput]:\n    \"\"\"Match the input against a pattern and return the output for each input\n\n    Parameters:\n        text: contains the input string to be processed\n\n    Returns:\n        A list contains the output ExtractorOutput for each input\n\n    Example:\n        ```pycon\n        &gt;&gt;&gt; document1 = Document(...)\n        &gt;&gt;&gt; document2 = Document(...)\n        &gt;&gt;&gt; document_batch = [document1, document2]\n        &gt;&gt;&gt; batch_output = self(document_batch)\n        &gt;&gt;&gt; print(batch_output)\n        [output1_document1, output1_document2]\n        ```\n    \"\"\"\n    # TODO: this conversion seems common\n    input_: list[str] = []\n    if not isinstance(text, list):\n        text = [text]\n\n    for item in text:\n        if isinstance(item, str):\n            input_.append(item)\n        elif isinstance(item, Document):\n            input_.append(item.text)\n        else:\n            raise ValueError(\n                f\"Invalid input type {type(item)}, should be str or Document\"\n            )\n\n    output = []\n    for each_input in input_:\n        output.append(self.run_raw(each_input))\n\n    return output\n</code></pre>"},{"location":"reference/parsers/regex_extractor/","title":"Regex Extractor","text":""},{"location":"reference/parsers/regex_extractor/#parsers.regex_extractor.RegexExtractor","title":"RegexExtractor","text":"<p>             Bases: <code>BaseComponent</code></p> <p>Simple class for extracting text from a document using a regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>List[str]</code> <p>The regex pattern(s) to use.</p> required <code>output_map</code> <code>dict</code> <p>A mapping from extracted text to the desired output. Defaults to None.</p> required Source code in <code>kotaemon\\parsers\\regex_extractor.py</code> <pre><code>class RegexExtractor(BaseComponent):\n    \"\"\"\n    Simple class for extracting text from a document using a regex pattern.\n\n    Args:\n        pattern (List[str]): The regex pattern(s) to use.\n        output_map (dict, optional): A mapping from extracted text to the\n            desired output. Defaults to None.\n    \"\"\"\n\n    class Config:\n        middleware_switches = {\"theflow.middleware.CachingMiddleware\": False}\n\n    pattern: list[str]\n    output_map: dict[str, str] | Callable[[str], str] = Param(\n        default_callback=lambda *_: {}\n    )\n\n    def __init__(self, pattern: str | list[str], **kwargs):\n        if isinstance(pattern, str):\n            pattern = [pattern]\n        super().__init__(pattern=pattern, **kwargs)\n\n    @staticmethod\n    def run_raw_static(pattern: str, text: str) -&gt; list[str]:\n        \"\"\"\n        Finds all non-overlapping occurrences of a pattern in a string.\n\n        Parameters:\n            pattern (str): The regular expression pattern to search for.\n            text (str): The input string to search in.\n\n        Returns:\n            List[str]: A list of all non-overlapping occurrences of the pattern in the\n                string.\n        \"\"\"\n        return re.findall(pattern, text)\n\n    @staticmethod\n    def map_output(text, output_map) -&gt; str:\n        \"\"\"\n        Maps the given `text` to its corresponding value in the `output_map` dictionary.\n\n        Parameters:\n            text (str): The input text to be mapped.\n            output_map (dict): A dictionary containing mapping of input text to output\n                values.\n\n        Returns:\n            str: The corresponding value from the `output_map` if `text` is found in the\n                dictionary, otherwise returns the original `text`.\n        \"\"\"\n        if not output_map:\n            return text\n\n        if isinstance(output_map, dict):\n            return output_map.get(text, text)\n\n        return output_map(text)\n\n    def run_raw(self, text: str) -&gt; ExtractorOutput:\n        \"\"\"\n        Matches the raw text against the pattern and rans the output mapping, returning\n            an instance of ExtractorOutput.\n\n        Args:\n            text (str): The raw text to be processed.\n\n        Returns:\n            ExtractorOutput: The processed output as a list of ExtractorOutput.\n        \"\"\"\n        output: list[str] = sum(\n            [self.run_raw_static(p, text) for p in self.pattern], []\n        )\n        output = [self.map_output(text, self.output_map) for text in output]\n\n        return ExtractorOutput(\n            text=output[0] if output else \"\",\n            matches=output,\n            metadata={\"origin\": \"RegexExtractor\"},\n        )\n\n    def run(\n        self, text: str | list[str] | Document | list[Document]\n    ) -&gt; list[ExtractorOutput]:\n        \"\"\"Match the input against a pattern and return the output for each input\n\n        Parameters:\n            text: contains the input string to be processed\n\n        Returns:\n            A list contains the output ExtractorOutput for each input\n\n        Example:\n            ```pycon\n            &gt;&gt;&gt; document1 = Document(...)\n            &gt;&gt;&gt; document2 = Document(...)\n            &gt;&gt;&gt; document_batch = [document1, document2]\n            &gt;&gt;&gt; batch_output = self(document_batch)\n            &gt;&gt;&gt; print(batch_output)\n            [output1_document1, output1_document2]\n            ```\n        \"\"\"\n        # TODO: this conversion seems common\n        input_: list[str] = []\n        if not isinstance(text, list):\n            text = [text]\n\n        for item in text:\n            if isinstance(item, str):\n                input_.append(item)\n            elif isinstance(item, Document):\n                input_.append(item.text)\n            else:\n                raise ValueError(\n                    f\"Invalid input type {type(item)}, should be str or Document\"\n                )\n\n        output = []\n        for each_input in input_:\n            output.append(self.run_raw(each_input))\n\n        return output\n</code></pre>"},{"location":"reference/parsers/regex_extractor/#parsers.regex_extractor.RegexExtractor.run_raw_static","title":"run_raw_static  <code>staticmethod</code>","text":"<pre><code>run_raw_static(pattern, text)\n</code></pre> <p>Finds all non-overlapping occurrences of a pattern in a string.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>The regular expression pattern to search for.</p> required <code>text</code> <code>str</code> <p>The input string to search in.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: A list of all non-overlapping occurrences of the pattern in the string.</p> Source code in <code>kotaemon\\parsers\\regex_extractor.py</code> <pre><code>@staticmethod\ndef run_raw_static(pattern: str, text: str) -&gt; list[str]:\n    \"\"\"\n    Finds all non-overlapping occurrences of a pattern in a string.\n\n    Parameters:\n        pattern (str): The regular expression pattern to search for.\n        text (str): The input string to search in.\n\n    Returns:\n        List[str]: A list of all non-overlapping occurrences of the pattern in the\n            string.\n    \"\"\"\n    return re.findall(pattern, text)\n</code></pre>"},{"location":"reference/parsers/regex_extractor/#parsers.regex_extractor.RegexExtractor.map_output","title":"map_output  <code>staticmethod</code>","text":"<pre><code>map_output(text, output_map)\n</code></pre> <p>Maps the given <code>text</code> to its corresponding value in the <code>output_map</code> dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to be mapped.</p> required <code>output_map</code> <code>dict</code> <p>A dictionary containing mapping of input text to output values.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The corresponding value from the <code>output_map</code> if <code>text</code> is found in the dictionary, otherwise returns the original <code>text</code>.</p> Source code in <code>kotaemon\\parsers\\regex_extractor.py</code> <pre><code>@staticmethod\ndef map_output(text, output_map) -&gt; str:\n    \"\"\"\n    Maps the given `text` to its corresponding value in the `output_map` dictionary.\n\n    Parameters:\n        text (str): The input text to be mapped.\n        output_map (dict): A dictionary containing mapping of input text to output\n            values.\n\n    Returns:\n        str: The corresponding value from the `output_map` if `text` is found in the\n            dictionary, otherwise returns the original `text`.\n    \"\"\"\n    if not output_map:\n        return text\n\n    if isinstance(output_map, dict):\n        return output_map.get(text, text)\n\n    return output_map(text)\n</code></pre>"},{"location":"reference/parsers/regex_extractor/#parsers.regex_extractor.RegexExtractor.run_raw","title":"run_raw","text":"<pre><code>run_raw(text)\n</code></pre> <p>Matches the raw text against the pattern and rans the output mapping, returning     an instance of ExtractorOutput.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The raw text to be processed.</p> required <p>Returns:</p> Name Type Description <code>ExtractorOutput</code> <code>ExtractorOutput</code> <p>The processed output as a list of ExtractorOutput.</p> Source code in <code>kotaemon\\parsers\\regex_extractor.py</code> <pre><code>def run_raw(self, text: str) -&gt; ExtractorOutput:\n    \"\"\"\n    Matches the raw text against the pattern and rans the output mapping, returning\n        an instance of ExtractorOutput.\n\n    Args:\n        text (str): The raw text to be processed.\n\n    Returns:\n        ExtractorOutput: The processed output as a list of ExtractorOutput.\n    \"\"\"\n    output: list[str] = sum(\n        [self.run_raw_static(p, text) for p in self.pattern], []\n    )\n    output = [self.map_output(text, self.output_map) for text in output]\n\n    return ExtractorOutput(\n        text=output[0] if output else \"\",\n        matches=output,\n        metadata={\"origin\": \"RegexExtractor\"},\n    )\n</code></pre>"},{"location":"reference/parsers/regex_extractor/#parsers.regex_extractor.RegexExtractor.run","title":"run","text":"<pre><code>run(text)\n</code></pre> <p>Match the input against a pattern and return the output for each input</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str | list[str] | Document | list[Document]</code> <p>contains the input string to be processed</p> required <p>Returns:</p> Type Description <code>list[ExtractorOutput]</code> <p>A list contains the output ExtractorOutput for each input</p> Example <pre><code>&gt;&gt;&gt; document1 = Document(...)\n&gt;&gt;&gt; document2 = Document(...)\n&gt;&gt;&gt; document_batch = [document1, document2]\n&gt;&gt;&gt; batch_output = self(document_batch)\n&gt;&gt;&gt; print(batch_output)\n[output1_document1, output1_document2]\n</code></pre> Source code in <code>kotaemon\\parsers\\regex_extractor.py</code> <pre><code>def run(\n    self, text: str | list[str] | Document | list[Document]\n) -&gt; list[ExtractorOutput]:\n    \"\"\"Match the input against a pattern and return the output for each input\n\n    Parameters:\n        text: contains the input string to be processed\n\n    Returns:\n        A list contains the output ExtractorOutput for each input\n\n    Example:\n        ```pycon\n        &gt;&gt;&gt; document1 = Document(...)\n        &gt;&gt;&gt; document2 = Document(...)\n        &gt;&gt;&gt; document_batch = [document1, document2]\n        &gt;&gt;&gt; batch_output = self(document_batch)\n        &gt;&gt;&gt; print(batch_output)\n        [output1_document1, output1_document2]\n        ```\n    \"\"\"\n    # TODO: this conversion seems common\n    input_: list[str] = []\n    if not isinstance(text, list):\n        text = [text]\n\n    for item in text:\n        if isinstance(item, str):\n            input_.append(item)\n        elif isinstance(item, Document):\n            input_.append(item.text)\n        else:\n            raise ValueError(\n                f\"Invalid input type {type(item)}, should be str or Document\"\n            )\n\n    output = []\n    for each_input in input_:\n        output.append(self.run_raw(each_input))\n\n    return output\n</code></pre>"},{"location":"reference/storages/","title":"Storages","text":""},{"location":"reference/storages/#storages.BaseDocumentStore","title":"BaseDocumentStore","text":"<p>             Bases: <code>ABC</code></p> <p>A document store is in charged of storing and managing documents</p> Source code in <code>kotaemon\\storages\\docstores\\base.py</code> <pre><code>class BaseDocumentStore(ABC):\n    \"\"\"A document store is in charged of storing and managing documents\"\"\"\n\n    @abstractmethod\n    def __init__(self, *args, **kwargs):\n        ...\n\n    @abstractmethod\n    def add(\n        self,\n        docs: Union[Document, List[Document]],\n        ids: Optional[Union[List[str], str]] = None,\n        **kwargs,\n    ):\n        \"\"\"Add document into document store\n\n        Args:\n            docs: Document or list of documents\n            ids: List of ids of the documents. Optional, if not set will use doc.doc_id\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get(self, ids: Union[List[str], str]) -&gt; List[Document]:\n        \"\"\"Get document by id\"\"\"\n        ...\n\n    @abstractmethod\n    def get_all(self) -&gt; List[Document]:\n        \"\"\"Get all documents\"\"\"\n        ...\n\n    @abstractmethod\n    def count(self) -&gt; int:\n        \"\"\"Count number of documents\"\"\"\n        ...\n\n    @abstractmethod\n    def delete(self, ids: Union[List[str], str]):\n        \"\"\"Delete document by id\"\"\"\n        ...\n</code></pre>"},{"location":"reference/storages/#storages.BaseDocumentStore.add","title":"add  <code>abstractmethod</code>","text":"<pre><code>add(docs, ids=None, **kwargs)\n</code></pre> <p>Add document into document store</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[Document, List[Document]]</code> <p>Document or list of documents</p> required <code>ids</code> <code>Optional[Union[List[str], str]]</code> <p>List of ids of the documents. Optional, if not set will use doc.doc_id</p> <code>None</code> Source code in <code>kotaemon\\storages\\docstores\\base.py</code> <pre><code>@abstractmethod\ndef add(\n    self,\n    docs: Union[Document, List[Document]],\n    ids: Optional[Union[List[str], str]] = None,\n    **kwargs,\n):\n    \"\"\"Add document into document store\n\n    Args:\n        docs: Document or list of documents\n        ids: List of ids of the documents. Optional, if not set will use doc.doc_id\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/#storages.BaseDocumentStore.get","title":"get  <code>abstractmethod</code>","text":"<pre><code>get(ids)\n</code></pre> <p>Get document by id</p> Source code in <code>kotaemon\\storages\\docstores\\base.py</code> <pre><code>@abstractmethod\ndef get(self, ids: Union[List[str], str]) -&gt; List[Document]:\n    \"\"\"Get document by id\"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/#storages.BaseDocumentStore.get_all","title":"get_all  <code>abstractmethod</code>","text":"<pre><code>get_all()\n</code></pre> <p>Get all documents</p> Source code in <code>kotaemon\\storages\\docstores\\base.py</code> <pre><code>@abstractmethod\ndef get_all(self) -&gt; List[Document]:\n    \"\"\"Get all documents\"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/#storages.BaseDocumentStore.count","title":"count  <code>abstractmethod</code>","text":"<pre><code>count()\n</code></pre> <p>Count number of documents</p> Source code in <code>kotaemon\\storages\\docstores\\base.py</code> <pre><code>@abstractmethod\ndef count(self) -&gt; int:\n    \"\"\"Count number of documents\"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/#storages.BaseDocumentStore.delete","title":"delete  <code>abstractmethod</code>","text":"<pre><code>delete(ids)\n</code></pre> <p>Delete document by id</p> Source code in <code>kotaemon\\storages\\docstores\\base.py</code> <pre><code>@abstractmethod\ndef delete(self, ids: Union[List[str], str]):\n    \"\"\"Delete document by id\"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/#storages.ElasticsearchDocumentStore","title":"ElasticsearchDocumentStore","text":"<p>             Bases: <code>BaseDocumentStore</code></p> <p>Simple memory document store that store document in a dictionary</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>class ElasticsearchDocumentStore(BaseDocumentStore):\n    \"\"\"Simple memory document store that store document in a dictionary\"\"\"\n\n    def __init__(\n        self,\n        index_name: str = \"docstore\",\n        elasticsearch_url: str = \"http://localhost:9200\",\n        k1: float = 2.0,\n        b: float = 0.75,\n    ):\n        try:\n            from elasticsearch import Elasticsearch\n            from elasticsearch.helpers import bulk\n        except ImportError:\n            raise ImportError(\n                \"To use ElaticsearchDocstore please install `pip install elasticsearch`\"\n            )\n\n        self.elasticsearch_url = elasticsearch_url\n        self.index_name = index_name\n        self.k1 = k1\n        self.b = b\n\n        # Create an Elasticsearch client instance\n        self.client = Elasticsearch(elasticsearch_url)\n        self.es_bulk = bulk\n        # Define the index settings and mappings\n        settings = {\n            \"analysis\": {\"analyzer\": {\"default\": {\"type\": \"standard\"}}},\n            \"similarity\": {\n                \"custom_bm25\": {\n                    \"type\": \"BM25\",\n                    \"k1\": k1,\n                    \"b\": b,\n                }\n            },\n        }\n        mappings = {\n            \"properties\": {\n                \"content\": {\n                    \"type\": \"text\",\n                    \"similarity\": \"custom_bm25\",  # Use the custom BM25 similarity\n                }\n            }\n        }\n\n        # Create the index with the specified settings and mappings\n        if not self.client.indices.exists(index=index_name):\n            self.client.indices.create(\n                index=index_name, mappings=mappings, settings=settings\n            )\n\n    def add(\n        self,\n        docs: Union[Document, List[Document]],\n        ids: Optional[Union[List[str], str]] = None,\n        **kwargs\n    ):\n        \"\"\"Add document into document store\n\n        Args:\n            docs: list of documents to add\n            ids: specify the ids of documents to add or\n                use existing doc.doc_id\n            refresh_indices: request Elasticsearch to update\n                its index (default to True)\n        \"\"\"\n        refresh_indices = kwargs.pop(\"refresh_indices\", True)\n\n        if ids and not isinstance(ids, list):\n            ids = [ids]\n        if not isinstance(docs, list):\n            docs = [docs]\n        doc_ids = ids if ids else [doc.doc_id for doc in docs]\n\n        requests = []\n        for doc_id, doc in zip(doc_ids, docs):\n            text = doc.text\n            metadata = doc.metadata\n            request = {\n                \"_op_type\": \"index\",\n                \"_index\": self.index_name,\n                \"content\": text,\n                \"metadata\": metadata,\n                \"_id\": doc_id,\n            }\n            requests.append(request)\n        self.es_bulk(self.client, requests)\n\n        if refresh_indices:\n            self.client.indices.refresh(index=self.index_name)\n\n    def query_raw(self, query: dict) -&gt; List[Document]:\n        \"\"\"Query Elasticsearch store using query format of ES client\n\n        Args:\n            query (dict): Elasticsearch query format\n\n        Returns:\n            List[Document]: List of result documents\n        \"\"\"\n        res = self.client.search(index=self.index_name, body=query)\n        docs = []\n        for r in res[\"hits\"][\"hits\"]:\n            docs.append(\n                Document(\n                    id_=r[\"_id\"],\n                    text=r[\"_source\"][\"content\"],\n                    metadata=r[\"_source\"][\"metadata\"],\n                )\n            )\n        return docs\n\n    def query(self, query: str, top_k: int = 10) -&gt; List[Document]:\n        \"\"\"Search Elasticsearch docstore using search query (BM25)\n\n        Args:\n            query (str): query text\n            top_k (int, optional): number of\n                top documents to return. Defaults to 10.\n\n        Returns:\n            List[Document]: List of result documents\n        \"\"\"\n        query_dict = {\"query\": {\"match\": {\"content\": query}}, \"size\": top_k}\n        return self.query_raw(query_dict)\n\n    def get(self, ids: Union[List[str], str]) -&gt; List[Document]:\n        \"\"\"Get document by id\"\"\"\n        if not isinstance(ids, list):\n            ids = [ids]\n        query_dict = {\"query\": {\"terms\": {\"_id\": ids}}}\n        return self.query_raw(query_dict)\n\n    def count(self) -&gt; int:\n        \"\"\"Count number of documents\"\"\"\n        count = int(\n            self.client.cat.count(index=self.index_name, format=\"json\")[0][\"count\"]\n        )\n        return count\n\n    def get_all(self) -&gt; List[Document]:\n        \"\"\"Get all documents\"\"\"\n        query_dict = {\"query\": {\"match_all\": {}}, \"size\": MAX_DOCS_TO_GET}\n        return self.query_raw(query_dict)\n\n    def delete(self, ids: Union[List[str], str]):\n        \"\"\"Delete document by id\"\"\"\n        if not isinstance(ids, list):\n            ids = [ids]\n\n        query = {\"query\": {\"terms\": {\"_id\": ids}}}\n        self.client.delete_by_query(index=self.index_name, body=query)\n        self.client.indices.refresh(index=self.index_name)\n\n    def __persist_flow__(self):\n        return {\n            \"index_name\": self.index_name,\n            \"elasticsearch_url\": self.elasticsearch_url,\n            \"k1\": self.k1,\n            \"b\": self.b,\n        }\n</code></pre>"},{"location":"reference/storages/#storages.ElasticsearchDocumentStore.add","title":"add","text":"<pre><code>add(docs, ids=None, **kwargs)\n</code></pre> <p>Add document into document store</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[Document, List[Document]]</code> <p>list of documents to add</p> required <code>ids</code> <code>Optional[Union[List[str], str]]</code> <p>specify the ids of documents to add or use existing doc.doc_id</p> <code>None</code> <code>refresh_indices</code> <p>request Elasticsearch to update its index (default to True)</p> required Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def add(\n    self,\n    docs: Union[Document, List[Document]],\n    ids: Optional[Union[List[str], str]] = None,\n    **kwargs\n):\n    \"\"\"Add document into document store\n\n    Args:\n        docs: list of documents to add\n        ids: specify the ids of documents to add or\n            use existing doc.doc_id\n        refresh_indices: request Elasticsearch to update\n            its index (default to True)\n    \"\"\"\n    refresh_indices = kwargs.pop(\"refresh_indices\", True)\n\n    if ids and not isinstance(ids, list):\n        ids = [ids]\n    if not isinstance(docs, list):\n        docs = [docs]\n    doc_ids = ids if ids else [doc.doc_id for doc in docs]\n\n    requests = []\n    for doc_id, doc in zip(doc_ids, docs):\n        text = doc.text\n        metadata = doc.metadata\n        request = {\n            \"_op_type\": \"index\",\n            \"_index\": self.index_name,\n            \"content\": text,\n            \"metadata\": metadata,\n            \"_id\": doc_id,\n        }\n        requests.append(request)\n    self.es_bulk(self.client, requests)\n\n    if refresh_indices:\n        self.client.indices.refresh(index=self.index_name)\n</code></pre>"},{"location":"reference/storages/#storages.ElasticsearchDocumentStore.query_raw","title":"query_raw","text":"<pre><code>query_raw(query)\n</code></pre> <p>Query Elasticsearch store using query format of ES client</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>dict</code> <p>Elasticsearch query format</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of result documents</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def query_raw(self, query: dict) -&gt; List[Document]:\n    \"\"\"Query Elasticsearch store using query format of ES client\n\n    Args:\n        query (dict): Elasticsearch query format\n\n    Returns:\n        List[Document]: List of result documents\n    \"\"\"\n    res = self.client.search(index=self.index_name, body=query)\n    docs = []\n    for r in res[\"hits\"][\"hits\"]:\n        docs.append(\n            Document(\n                id_=r[\"_id\"],\n                text=r[\"_source\"][\"content\"],\n                metadata=r[\"_source\"][\"metadata\"],\n            )\n        )\n    return docs\n</code></pre>"},{"location":"reference/storages/#storages.ElasticsearchDocumentStore.query","title":"query","text":"<pre><code>query(query, top_k=10)\n</code></pre> <p>Search Elasticsearch docstore using search query (BM25)</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query text</p> required <code>top_k</code> <code>int</code> <p>number of top documents to return. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of result documents</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def query(self, query: str, top_k: int = 10) -&gt; List[Document]:\n    \"\"\"Search Elasticsearch docstore using search query (BM25)\n\n    Args:\n        query (str): query text\n        top_k (int, optional): number of\n            top documents to return. Defaults to 10.\n\n    Returns:\n        List[Document]: List of result documents\n    \"\"\"\n    query_dict = {\"query\": {\"match\": {\"content\": query}}, \"size\": top_k}\n    return self.query_raw(query_dict)\n</code></pre>"},{"location":"reference/storages/#storages.ElasticsearchDocumentStore.get","title":"get","text":"<pre><code>get(ids)\n</code></pre> <p>Get document by id</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def get(self, ids: Union[List[str], str]) -&gt; List[Document]:\n    \"\"\"Get document by id\"\"\"\n    if not isinstance(ids, list):\n        ids = [ids]\n    query_dict = {\"query\": {\"terms\": {\"_id\": ids}}}\n    return self.query_raw(query_dict)\n</code></pre>"},{"location":"reference/storages/#storages.ElasticsearchDocumentStore.count","title":"count","text":"<pre><code>count()\n</code></pre> <p>Count number of documents</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def count(self) -&gt; int:\n    \"\"\"Count number of documents\"\"\"\n    count = int(\n        self.client.cat.count(index=self.index_name, format=\"json\")[0][\"count\"]\n    )\n    return count\n</code></pre>"},{"location":"reference/storages/#storages.ElasticsearchDocumentStore.get_all","title":"get_all","text":"<pre><code>get_all()\n</code></pre> <p>Get all documents</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def get_all(self) -&gt; List[Document]:\n    \"\"\"Get all documents\"\"\"\n    query_dict = {\"query\": {\"match_all\": {}}, \"size\": MAX_DOCS_TO_GET}\n    return self.query_raw(query_dict)\n</code></pre>"},{"location":"reference/storages/#storages.ElasticsearchDocumentStore.delete","title":"delete","text":"<pre><code>delete(ids)\n</code></pre> <p>Delete document by id</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def delete(self, ids: Union[List[str], str]):\n    \"\"\"Delete document by id\"\"\"\n    if not isinstance(ids, list):\n        ids = [ids]\n\n    query = {\"query\": {\"terms\": {\"_id\": ids}}}\n    self.client.delete_by_query(index=self.index_name, body=query)\n    self.client.indices.refresh(index=self.index_name)\n</code></pre>"},{"location":"reference/storages/#storages.InMemoryDocumentStore","title":"InMemoryDocumentStore","text":"<p>             Bases: <code>BaseDocumentStore</code></p> <p>Simple memory document store that store document in a dictionary</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>class InMemoryDocumentStore(BaseDocumentStore):\n    \"\"\"Simple memory document store that store document in a dictionary\"\"\"\n\n    def __init__(self):\n        self._store = {}\n\n    def add(\n        self,\n        docs: Union[Document, List[Document]],\n        ids: Optional[Union[List[str], str]] = None,\n        **kwargs,\n    ):\n        \"\"\"Add document into document store\n\n        Args:\n            docs: list of documents to add\n            ids: specify the ids of documents to add or\n                use existing doc.doc_id\n            exist_ok: raise error when duplicate doc-id\n                found in the docstore (default to False)\n        \"\"\"\n        exist_ok: bool = kwargs.pop(\"exist_ok\", False)\n\n        if ids and not isinstance(ids, list):\n            ids = [ids]\n        if not isinstance(docs, list):\n            docs = [docs]\n        doc_ids = ids if ids else [doc.doc_id for doc in docs]\n\n        for doc_id, doc in zip(doc_ids, docs):\n            if doc_id in self._store and not exist_ok:\n                raise ValueError(f\"Document with id {doc_id} already exist\")\n            self._store[doc_id] = doc\n\n    def get(self, ids: Union[List[str], str]) -&gt; List[Document]:\n        \"\"\"Get document by id\"\"\"\n        if not isinstance(ids, list):\n            ids = [ids]\n\n        return [self._store[doc_id] for doc_id in ids]\n\n    def get_all(self) -&gt; List[Document]:\n        \"\"\"Get all documents\"\"\"\n        return list(self._store.values())\n\n    def count(self) -&gt; int:\n        \"\"\"Count number of documents\"\"\"\n        return len(self._store)\n\n    def delete(self, ids: Union[List[str], str]):\n        \"\"\"Delete document by id\"\"\"\n        if not isinstance(ids, list):\n            ids = [ids]\n\n        for doc_id in ids:\n            del self._store[doc_id]\n\n    def save(self, path: Union[str, Path]):\n        \"\"\"Save document to path\"\"\"\n        store = {key: value.to_dict() for key, value in self._store.items()}\n        with open(path, \"w\") as f:\n            json.dump(store, f)\n\n    def load(self, path: Union[str, Path]):\n        \"\"\"Load document store from path\"\"\"\n        with open(path) as f:\n            store = json.load(f)\n        self._store = {key: Document.from_dict(value) for key, value in store.items()}\n\n    def __persist_flow__(self):\n        return {}\n</code></pre>"},{"location":"reference/storages/#storages.InMemoryDocumentStore.add","title":"add","text":"<pre><code>add(docs, ids=None, **kwargs)\n</code></pre> <p>Add document into document store</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[Document, List[Document]]</code> <p>list of documents to add</p> required <code>ids</code> <code>Optional[Union[List[str], str]]</code> <p>specify the ids of documents to add or use existing doc.doc_id</p> <code>None</code> <code>exist_ok</code> <p>raise error when duplicate doc-id found in the docstore (default to False)</p> required Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def add(\n    self,\n    docs: Union[Document, List[Document]],\n    ids: Optional[Union[List[str], str]] = None,\n    **kwargs,\n):\n    \"\"\"Add document into document store\n\n    Args:\n        docs: list of documents to add\n        ids: specify the ids of documents to add or\n            use existing doc.doc_id\n        exist_ok: raise error when duplicate doc-id\n            found in the docstore (default to False)\n    \"\"\"\n    exist_ok: bool = kwargs.pop(\"exist_ok\", False)\n\n    if ids and not isinstance(ids, list):\n        ids = [ids]\n    if not isinstance(docs, list):\n        docs = [docs]\n    doc_ids = ids if ids else [doc.doc_id for doc in docs]\n\n    for doc_id, doc in zip(doc_ids, docs):\n        if doc_id in self._store and not exist_ok:\n            raise ValueError(f\"Document with id {doc_id} already exist\")\n        self._store[doc_id] = doc\n</code></pre>"},{"location":"reference/storages/#storages.InMemoryDocumentStore.get","title":"get","text":"<pre><code>get(ids)\n</code></pre> <p>Get document by id</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def get(self, ids: Union[List[str], str]) -&gt; List[Document]:\n    \"\"\"Get document by id\"\"\"\n    if not isinstance(ids, list):\n        ids = [ids]\n\n    return [self._store[doc_id] for doc_id in ids]\n</code></pre>"},{"location":"reference/storages/#storages.InMemoryDocumentStore.get_all","title":"get_all","text":"<pre><code>get_all()\n</code></pre> <p>Get all documents</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def get_all(self) -&gt; List[Document]:\n    \"\"\"Get all documents\"\"\"\n    return list(self._store.values())\n</code></pre>"},{"location":"reference/storages/#storages.InMemoryDocumentStore.count","title":"count","text":"<pre><code>count()\n</code></pre> <p>Count number of documents</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def count(self) -&gt; int:\n    \"\"\"Count number of documents\"\"\"\n    return len(self._store)\n</code></pre>"},{"location":"reference/storages/#storages.InMemoryDocumentStore.delete","title":"delete","text":"<pre><code>delete(ids)\n</code></pre> <p>Delete document by id</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def delete(self, ids: Union[List[str], str]):\n    \"\"\"Delete document by id\"\"\"\n    if not isinstance(ids, list):\n        ids = [ids]\n\n    for doc_id in ids:\n        del self._store[doc_id]\n</code></pre>"},{"location":"reference/storages/#storages.InMemoryDocumentStore.save","title":"save","text":"<pre><code>save(path)\n</code></pre> <p>Save document to path</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def save(self, path: Union[str, Path]):\n    \"\"\"Save document to path\"\"\"\n    store = {key: value.to_dict() for key, value in self._store.items()}\n    with open(path, \"w\") as f:\n        json.dump(store, f)\n</code></pre>"},{"location":"reference/storages/#storages.InMemoryDocumentStore.load","title":"load","text":"<pre><code>load(path)\n</code></pre> <p>Load document store from path</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def load(self, path: Union[str, Path]):\n    \"\"\"Load document store from path\"\"\"\n    with open(path) as f:\n        store = json.load(f)\n    self._store = {key: Document.from_dict(value) for key, value in store.items()}\n</code></pre>"},{"location":"reference/storages/#storages.SimpleFileDocumentStore","title":"SimpleFileDocumentStore","text":"<p>             Bases: <code>InMemoryDocumentStore</code></p> <p>Improve InMemoryDocumentStore by auto saving whenever the corpus is changed</p> Source code in <code>kotaemon\\storages\\docstores\\simple_file.py</code> <pre><code>class SimpleFileDocumentStore(InMemoryDocumentStore):\n    \"\"\"Improve InMemoryDocumentStore by auto saving whenever the corpus is changed\"\"\"\n\n    def __init__(self, path: str | Path):\n        super().__init__()\n        self._path = path\n        if path is not None and Path(path).is_file():\n            self.load(path)\n\n    def add(\n        self,\n        docs: Union[Document, List[Document]],\n        ids: Optional[Union[List[str], str]] = None,\n        **kwargs,\n    ):\n        \"\"\"Add document into document store\n\n        Args:\n            docs: list of documents to add\n            ids: specify the ids of documents to add or\n                use existing doc.doc_id\n            exist_ok: raise error when duplicate doc-id\n                found in the docstore (default to False)\n        \"\"\"\n        super().add(docs=docs, ids=ids, **kwargs)\n        self.save(self._path)\n\n    def delete(self, ids: Union[List[str], str]):\n        \"\"\"Delete document by id\"\"\"\n        super().delete(ids=ids)\n        self.save(self._path)\n\n    def __persist_flow__(self):\n        from theflow.utils.modules import serialize\n\n        return {\"path\": serialize(self._path)}\n</code></pre>"},{"location":"reference/storages/#storages.SimpleFileDocumentStore.add","title":"add","text":"<pre><code>add(docs, ids=None, **kwargs)\n</code></pre> <p>Add document into document store</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[Document, List[Document]]</code> <p>list of documents to add</p> required <code>ids</code> <code>Optional[Union[List[str], str]]</code> <p>specify the ids of documents to add or use existing doc.doc_id</p> <code>None</code> <code>exist_ok</code> <p>raise error when duplicate doc-id found in the docstore (default to False)</p> required Source code in <code>kotaemon\\storages\\docstores\\simple_file.py</code> <pre><code>def add(\n    self,\n    docs: Union[Document, List[Document]],\n    ids: Optional[Union[List[str], str]] = None,\n    **kwargs,\n):\n    \"\"\"Add document into document store\n\n    Args:\n        docs: list of documents to add\n        ids: specify the ids of documents to add or\n            use existing doc.doc_id\n        exist_ok: raise error when duplicate doc-id\n            found in the docstore (default to False)\n    \"\"\"\n    super().add(docs=docs, ids=ids, **kwargs)\n    self.save(self._path)\n</code></pre>"},{"location":"reference/storages/#storages.SimpleFileDocumentStore.delete","title":"delete","text":"<pre><code>delete(ids)\n</code></pre> <p>Delete document by id</p> Source code in <code>kotaemon\\storages\\docstores\\simple_file.py</code> <pre><code>def delete(self, ids: Union[List[str], str]):\n    \"\"\"Delete document by id\"\"\"\n    super().delete(ids=ids)\n    self.save(self._path)\n</code></pre>"},{"location":"reference/storages/#storages.BaseVectorStore","title":"BaseVectorStore","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>kotaemon\\storages\\vectorstores\\base.py</code> <pre><code>class BaseVectorStore(ABC):\n    @abstractmethod\n    def __init__(self, *args, **kwargs):\n        ...\n\n    @abstractmethod\n    def add(\n        self,\n        embeddings: list[list[float]] | list[DocumentWithEmbedding],\n        metadatas: Optional[list[dict]] = None,\n        ids: Optional[list[str]] = None,\n    ) -&gt; list[str]:\n        \"\"\"Add vector embeddings to vector stores\n\n        Args:\n            embeddings: List of embeddings\n            metadatas: List of metadata of the embeddings\n            ids: List of ids of the embeddings\n            kwargs: meant for vectorstore-specific parameters\n\n        Returns:\n            List of ids of the embeddings\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def delete(self, ids: list[str], **kwargs):\n        \"\"\"Delete vector embeddings from vector stores\n\n        Args:\n            ids: List of ids of the embeddings to be deleted\n            kwargs: meant for vectorstore-specific parameters\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def query(\n        self,\n        embedding: list[float],\n        top_k: int = 1,\n        ids: Optional[list[str]] = None,\n        **kwargs,\n    ) -&gt; tuple[list[list[float]], list[float], list[str]]:\n        \"\"\"Return the top k most similar vector embeddings\n\n        Args:\n            embedding: List of embeddings\n            top_k: Number of most similar embeddings to return\n            ids: List of ids of the embeddings to be queried\n\n        Returns:\n            the matched embeddings, the similarity scores, and the ids\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/storages/#storages.BaseVectorStore.add","title":"add  <code>abstractmethod</code>","text":"<pre><code>add(embeddings, metadatas=None, ids=None)\n</code></pre> <p>Add vector embeddings to vector stores</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>list[list[float]] | list[DocumentWithEmbedding]</code> <p>List of embeddings</p> required <code>metadatas</code> <code>Optional[list[dict]]</code> <p>List of metadata of the embeddings</p> <code>None</code> <code>ids</code> <code>Optional[list[str]]</code> <p>List of ids of the embeddings</p> <code>None</code> <code>kwargs</code> <p>meant for vectorstore-specific parameters</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of ids of the embeddings</p> Source code in <code>kotaemon\\storages\\vectorstores\\base.py</code> <pre><code>@abstractmethod\ndef add(\n    self,\n    embeddings: list[list[float]] | list[DocumentWithEmbedding],\n    metadatas: Optional[list[dict]] = None,\n    ids: Optional[list[str]] = None,\n) -&gt; list[str]:\n    \"\"\"Add vector embeddings to vector stores\n\n    Args:\n        embeddings: List of embeddings\n        metadatas: List of metadata of the embeddings\n        ids: List of ids of the embeddings\n        kwargs: meant for vectorstore-specific parameters\n\n    Returns:\n        List of ids of the embeddings\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/#storages.BaseVectorStore.delete","title":"delete  <code>abstractmethod</code>","text":"<pre><code>delete(ids, **kwargs)\n</code></pre> <p>Delete vector embeddings from vector stores</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>list[str]</code> <p>List of ids of the embeddings to be deleted</p> required <code>kwargs</code> <p>meant for vectorstore-specific parameters</p> <code>{}</code> Source code in <code>kotaemon\\storages\\vectorstores\\base.py</code> <pre><code>@abstractmethod\ndef delete(self, ids: list[str], **kwargs):\n    \"\"\"Delete vector embeddings from vector stores\n\n    Args:\n        ids: List of ids of the embeddings to be deleted\n        kwargs: meant for vectorstore-specific parameters\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/#storages.BaseVectorStore.query","title":"query  <code>abstractmethod</code>","text":"<pre><code>query(embedding, top_k=1, ids=None, **kwargs)\n</code></pre> <p>Return the top k most similar vector embeddings</p> <p>Parameters:</p> Name Type Description Default <code>embedding</code> <code>list[float]</code> <p>List of embeddings</p> required <code>top_k</code> <code>int</code> <p>Number of most similar embeddings to return</p> <code>1</code> <code>ids</code> <code>Optional[list[str]]</code> <p>List of ids of the embeddings to be queried</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[list[list[float]], list[float], list[str]]</code> <p>the matched embeddings, the similarity scores, and the ids</p> Source code in <code>kotaemon\\storages\\vectorstores\\base.py</code> <pre><code>@abstractmethod\ndef query(\n    self,\n    embedding: list[float],\n    top_k: int = 1,\n    ids: Optional[list[str]] = None,\n    **kwargs,\n) -&gt; tuple[list[list[float]], list[float], list[str]]:\n    \"\"\"Return the top k most similar vector embeddings\n\n    Args:\n        embedding: List of embeddings\n        top_k: Number of most similar embeddings to return\n        ids: List of ids of the embeddings to be queried\n\n    Returns:\n        the matched embeddings, the similarity scores, and the ids\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/#storages.ChromaVectorStore","title":"ChromaVectorStore","text":"<p>             Bases: <code>LlamaIndexVectorStore</code></p> Source code in <code>kotaemon\\storages\\vectorstores\\chroma.py</code> <pre><code>class ChromaVectorStore(LlamaIndexVectorStore):\n    _li_class: Type[LIChromaVectorStore] = LIChromaVectorStore\n\n    def __init__(\n        self,\n        path: str = \"./chroma\",\n        collection_name: str = \"default\",\n        host: str = \"localhost\",\n        port: str = \"8000\",\n        ssl: bool = False,\n        headers: Optional[Dict[str, str]] = None,\n        collection_kwargs: Optional[dict] = None,\n        stores_text: bool = True,\n        flat_metadata: bool = True,\n        **kwargs: Any,\n    ):\n        self._path = path\n        self._collection_name = collection_name\n        self._host = host\n        self._port = port\n        self._ssl = ssl\n        self._headers = headers\n        self._collection_kwargs = collection_kwargs\n        self._stores_text = stores_text\n        self._flat_metadata = flat_metadata\n        self._kwargs = kwargs\n\n        try:\n            import chromadb\n        except ImportError:\n            raise ImportError(\n                \"ChromaVectorStore requires chromadb. \"\n                \"Please install chromadb first `pip install chromadb`\"\n            )\n\n        client = chromadb.PersistentClient(path=path)\n        collection = client.get_or_create_collection(collection_name)\n\n        # pass through for nice IDE support\n        super().__init__(\n            chroma_collection=collection,\n            host=host,\n            port=port,\n            ssl=ssl,\n            headers=headers or {},\n            collection_kwargs=collection_kwargs or {},\n            stores_text=stores_text,\n            flat_metadata=flat_metadata,\n            **kwargs,\n        )\n        self._client = cast(LIChromaVectorStore, self._client)\n\n    def delete(self, ids: List[str], **kwargs):\n        \"\"\"Delete vector embeddings from vector stores\n\n        Args:\n            ids: List of ids of the embeddings to be deleted\n            kwargs: meant for vectorstore-specific parameters\n        \"\"\"\n        self._client._collection.delete(ids=ids)\n\n    def delete_collection(self, collection_name: Optional[str] = None):\n        \"\"\"Delete entire collection under specified name from vector stores\n\n        Args:\n            collection_name: Name of the collection to delete\n        \"\"\"\n        # a rather ugly chain call but it do the job of finding\n        # original chromadb client and call delete_collection() method\n        if collection_name is None:\n            collection_name = self._client.client.name\n        self._client.client._client.delete_collection(collection_name)\n\n    def count(self) -&gt; int:\n        return self._collection.count()\n\n    def __persist_flow__(self):\n        return {\n            \"path\": self._path,\n            \"collection_name\": self._collection_name,\n            \"host\": self._host,\n            \"port\": self._port,\n            \"ssl\": self._ssl,\n            \"headers\": self._headers,\n            \"collection_kwargs\": self._collection_kwargs,\n            \"stores_text\": self._stores_text,\n            \"flat_metadata\": self._flat_metadata,\n            **self._kwargs,\n        }\n</code></pre>"},{"location":"reference/storages/#storages.ChromaVectorStore.delete","title":"delete","text":"<pre><code>delete(ids, **kwargs)\n</code></pre> <p>Delete vector embeddings from vector stores</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>List[str]</code> <p>List of ids of the embeddings to be deleted</p> required <code>kwargs</code> <p>meant for vectorstore-specific parameters</p> <code>{}</code> Source code in <code>kotaemon\\storages\\vectorstores\\chroma.py</code> <pre><code>def delete(self, ids: List[str], **kwargs):\n    \"\"\"Delete vector embeddings from vector stores\n\n    Args:\n        ids: List of ids of the embeddings to be deleted\n        kwargs: meant for vectorstore-specific parameters\n    \"\"\"\n    self._client._collection.delete(ids=ids)\n</code></pre>"},{"location":"reference/storages/#storages.ChromaVectorStore.delete_collection","title":"delete_collection","text":"<pre><code>delete_collection(collection_name=None)\n</code></pre> <p>Delete entire collection under specified name from vector stores</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>Optional[str]</code> <p>Name of the collection to delete</p> <code>None</code> Source code in <code>kotaemon\\storages\\vectorstores\\chroma.py</code> <pre><code>def delete_collection(self, collection_name: Optional[str] = None):\n    \"\"\"Delete entire collection under specified name from vector stores\n\n    Args:\n        collection_name: Name of the collection to delete\n    \"\"\"\n    # a rather ugly chain call but it do the job of finding\n    # original chromadb client and call delete_collection() method\n    if collection_name is None:\n        collection_name = self._client.client.name\n    self._client.client._client.delete_collection(collection_name)\n</code></pre>"},{"location":"reference/storages/#storages.InMemoryVectorStore","title":"InMemoryVectorStore","text":"<p>             Bases: <code>LlamaIndexVectorStore</code></p> Source code in <code>kotaemon\\storages\\vectorstores\\in_memory.py</code> <pre><code>class InMemoryVectorStore(LlamaIndexVectorStore):\n    _li_class: Type[LISimpleVectorStore] = LISimpleVectorStore\n    store_text: bool = False\n\n    def __init__(\n        self,\n        data: Optional[SimpleVectorStoreData] = None,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize params.\"\"\"\n        self._data = data or SimpleVectorStoreData()\n        self._fs = fs or fsspec.filesystem(\"file\")\n\n        super().__init__(\n            data=data,\n            fs=fs,\n            **kwargs,\n        )\n\n    def save(\n        self,\n        save_path: str,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n        **kwargs,\n    ):\n\n        \"\"\"save a simpleVectorStore to a dictionary.\n\n        Args:\n            save_path: Path of saving vector to disk.\n            fs: An abstract super-class for pythonic file-systems\n        \"\"\"\n        self._client.persist(persist_path=save_path, fs=fs)\n\n    def load(self, load_path: str, fs: Optional[fsspec.AbstractFileSystem] = None):\n\n        \"\"\"Create a SimpleKVStore from a load directory.\n\n        Args:\n            load_path: Path of loading vector.\n            fs: An abstract super-class for pythonic file-systems\n        \"\"\"\n        self._client = self._client.from_persist_path(persist_path=load_path, fs=fs)\n\n    def __persist_flow__(self):\n        d = self._data.to_dict()\n        d[\"__type__\"] = f\"{self._data.__module__}.{self._data.__class__.__qualname__}\"\n        return {\n            \"data\": d,\n            # \"fs\": self._fs,\n        }\n</code></pre>"},{"location":"reference/storages/#storages.InMemoryVectorStore.save","title":"save","text":"<pre><code>save(save_path, fs=None, **kwargs)\n</code></pre> <p>save a simpleVectorStore to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Path of saving vector to disk.</p> required <code>fs</code> <code>Optional[AbstractFileSystem]</code> <p>An abstract super-class for pythonic file-systems</p> <code>None</code> Source code in <code>kotaemon\\storages\\vectorstores\\in_memory.py</code> <pre><code>def save(\n    self,\n    save_path: str,\n    fs: Optional[fsspec.AbstractFileSystem] = None,\n    **kwargs,\n):\n\n    \"\"\"save a simpleVectorStore to a dictionary.\n\n    Args:\n        save_path: Path of saving vector to disk.\n        fs: An abstract super-class for pythonic file-systems\n    \"\"\"\n    self._client.persist(persist_path=save_path, fs=fs)\n</code></pre>"},{"location":"reference/storages/#storages.InMemoryVectorStore.load","title":"load","text":"<pre><code>load(load_path, fs=None)\n</code></pre> <p>Create a SimpleKVStore from a load directory.</p> <p>Parameters:</p> Name Type Description Default <code>load_path</code> <code>str</code> <p>Path of loading vector.</p> required <code>fs</code> <code>Optional[AbstractFileSystem]</code> <p>An abstract super-class for pythonic file-systems</p> <code>None</code> Source code in <code>kotaemon\\storages\\vectorstores\\in_memory.py</code> <pre><code>def load(self, load_path: str, fs: Optional[fsspec.AbstractFileSystem] = None):\n\n    \"\"\"Create a SimpleKVStore from a load directory.\n\n    Args:\n        load_path: Path of loading vector.\n        fs: An abstract super-class for pythonic file-systems\n    \"\"\"\n    self._client = self._client.from_persist_path(persist_path=load_path, fs=fs)\n</code></pre>"},{"location":"reference/storages/#storages.SimpleFileVectorStore","title":"SimpleFileVectorStore","text":"<p>             Bases: <code>LlamaIndexVectorStore</code></p> <p>Similar to InMemoryVectorStore but is backed by file by default</p> Source code in <code>kotaemon\\storages\\vectorstores\\simple_file.py</code> <pre><code>class SimpleFileVectorStore(LlamaIndexVectorStore):\n    \"\"\"Similar to InMemoryVectorStore but is backed by file by default\"\"\"\n\n    _li_class: Type[LISimpleVectorStore] = LISimpleVectorStore\n    store_text: bool = False\n\n    def __init__(\n        self,\n        path: str | Path,\n        data: Optional[SimpleVectorStoreData] = None,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize params.\"\"\"\n        self._data = data or SimpleVectorStoreData()\n        self._fs = fs or fsspec.filesystem(\"file\")\n        self._path = path\n        self._save_path = Path(path)\n\n        super().__init__(\n            data=data,\n            fs=fs,\n            **kwargs,\n        )\n\n        if self._save_path.is_file():\n            self._client = self._li_class.from_persist_path(\n                persist_path=str(self._save_path), fs=self._fs\n            )\n\n    def add(\n        self,\n        embeddings: list[list[float]] | list[DocumentWithEmbedding],\n        metadatas: Optional[list[dict]] = None,\n        ids: Optional[list[str]] = None,\n    ):\n        r = super().add(embeddings, metadatas, ids)\n        self._client.persist(str(self._save_path), self._fs)\n        return r\n\n    def delete(self, ids: list[str], **kwargs):\n        r = super().delete(ids, **kwargs)\n        self._client.persist(str(self._save_path), self._fs)\n        return r\n\n    def __persist_flow__(self):\n        d = self._data.to_dict()\n        d[\"__type__\"] = f\"{self._data.__module__}.{self._data.__class__.__qualname__}\"\n        return {\n            \"data\": d,\n            \"path\": str(self._path),\n            # \"fs\": self._fs,\n        }\n</code></pre>"},{"location":"reference/storages/docstores/","title":"Docstores","text":""},{"location":"reference/storages/docstores/#storages.docstores.BaseDocumentStore","title":"BaseDocumentStore","text":"<p>             Bases: <code>ABC</code></p> <p>A document store is in charged of storing and managing documents</p> Source code in <code>kotaemon\\storages\\docstores\\base.py</code> <pre><code>class BaseDocumentStore(ABC):\n    \"\"\"A document store is in charged of storing and managing documents\"\"\"\n\n    @abstractmethod\n    def __init__(self, *args, **kwargs):\n        ...\n\n    @abstractmethod\n    def add(\n        self,\n        docs: Union[Document, List[Document]],\n        ids: Optional[Union[List[str], str]] = None,\n        **kwargs,\n    ):\n        \"\"\"Add document into document store\n\n        Args:\n            docs: Document or list of documents\n            ids: List of ids of the documents. Optional, if not set will use doc.doc_id\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get(self, ids: Union[List[str], str]) -&gt; List[Document]:\n        \"\"\"Get document by id\"\"\"\n        ...\n\n    @abstractmethod\n    def get_all(self) -&gt; List[Document]:\n        \"\"\"Get all documents\"\"\"\n        ...\n\n    @abstractmethod\n    def count(self) -&gt; int:\n        \"\"\"Count number of documents\"\"\"\n        ...\n\n    @abstractmethod\n    def delete(self, ids: Union[List[str], str]):\n        \"\"\"Delete document by id\"\"\"\n        ...\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.BaseDocumentStore.add","title":"add  <code>abstractmethod</code>","text":"<pre><code>add(docs, ids=None, **kwargs)\n</code></pre> <p>Add document into document store</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[Document, List[Document]]</code> <p>Document or list of documents</p> required <code>ids</code> <code>Optional[Union[List[str], str]]</code> <p>List of ids of the documents. Optional, if not set will use doc.doc_id</p> <code>None</code> Source code in <code>kotaemon\\storages\\docstores\\base.py</code> <pre><code>@abstractmethod\ndef add(\n    self,\n    docs: Union[Document, List[Document]],\n    ids: Optional[Union[List[str], str]] = None,\n    **kwargs,\n):\n    \"\"\"Add document into document store\n\n    Args:\n        docs: Document or list of documents\n        ids: List of ids of the documents. Optional, if not set will use doc.doc_id\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.BaseDocumentStore.get","title":"get  <code>abstractmethod</code>","text":"<pre><code>get(ids)\n</code></pre> <p>Get document by id</p> Source code in <code>kotaemon\\storages\\docstores\\base.py</code> <pre><code>@abstractmethod\ndef get(self, ids: Union[List[str], str]) -&gt; List[Document]:\n    \"\"\"Get document by id\"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.BaseDocumentStore.get_all","title":"get_all  <code>abstractmethod</code>","text":"<pre><code>get_all()\n</code></pre> <p>Get all documents</p> Source code in <code>kotaemon\\storages\\docstores\\base.py</code> <pre><code>@abstractmethod\ndef get_all(self) -&gt; List[Document]:\n    \"\"\"Get all documents\"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.BaseDocumentStore.count","title":"count  <code>abstractmethod</code>","text":"<pre><code>count()\n</code></pre> <p>Count number of documents</p> Source code in <code>kotaemon\\storages\\docstores\\base.py</code> <pre><code>@abstractmethod\ndef count(self) -&gt; int:\n    \"\"\"Count number of documents\"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.BaseDocumentStore.delete","title":"delete  <code>abstractmethod</code>","text":"<pre><code>delete(ids)\n</code></pre> <p>Delete document by id</p> Source code in <code>kotaemon\\storages\\docstores\\base.py</code> <pre><code>@abstractmethod\ndef delete(self, ids: Union[List[str], str]):\n    \"\"\"Delete document by id\"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.ElasticsearchDocumentStore","title":"ElasticsearchDocumentStore","text":"<p>             Bases: <code>BaseDocumentStore</code></p> <p>Simple memory document store that store document in a dictionary</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>class ElasticsearchDocumentStore(BaseDocumentStore):\n    \"\"\"Simple memory document store that store document in a dictionary\"\"\"\n\n    def __init__(\n        self,\n        index_name: str = \"docstore\",\n        elasticsearch_url: str = \"http://localhost:9200\",\n        k1: float = 2.0,\n        b: float = 0.75,\n    ):\n        try:\n            from elasticsearch import Elasticsearch\n            from elasticsearch.helpers import bulk\n        except ImportError:\n            raise ImportError(\n                \"To use ElaticsearchDocstore please install `pip install elasticsearch`\"\n            )\n\n        self.elasticsearch_url = elasticsearch_url\n        self.index_name = index_name\n        self.k1 = k1\n        self.b = b\n\n        # Create an Elasticsearch client instance\n        self.client = Elasticsearch(elasticsearch_url)\n        self.es_bulk = bulk\n        # Define the index settings and mappings\n        settings = {\n            \"analysis\": {\"analyzer\": {\"default\": {\"type\": \"standard\"}}},\n            \"similarity\": {\n                \"custom_bm25\": {\n                    \"type\": \"BM25\",\n                    \"k1\": k1,\n                    \"b\": b,\n                }\n            },\n        }\n        mappings = {\n            \"properties\": {\n                \"content\": {\n                    \"type\": \"text\",\n                    \"similarity\": \"custom_bm25\",  # Use the custom BM25 similarity\n                }\n            }\n        }\n\n        # Create the index with the specified settings and mappings\n        if not self.client.indices.exists(index=index_name):\n            self.client.indices.create(\n                index=index_name, mappings=mappings, settings=settings\n            )\n\n    def add(\n        self,\n        docs: Union[Document, List[Document]],\n        ids: Optional[Union[List[str], str]] = None,\n        **kwargs\n    ):\n        \"\"\"Add document into document store\n\n        Args:\n            docs: list of documents to add\n            ids: specify the ids of documents to add or\n                use existing doc.doc_id\n            refresh_indices: request Elasticsearch to update\n                its index (default to True)\n        \"\"\"\n        refresh_indices = kwargs.pop(\"refresh_indices\", True)\n\n        if ids and not isinstance(ids, list):\n            ids = [ids]\n        if not isinstance(docs, list):\n            docs = [docs]\n        doc_ids = ids if ids else [doc.doc_id for doc in docs]\n\n        requests = []\n        for doc_id, doc in zip(doc_ids, docs):\n            text = doc.text\n            metadata = doc.metadata\n            request = {\n                \"_op_type\": \"index\",\n                \"_index\": self.index_name,\n                \"content\": text,\n                \"metadata\": metadata,\n                \"_id\": doc_id,\n            }\n            requests.append(request)\n        self.es_bulk(self.client, requests)\n\n        if refresh_indices:\n            self.client.indices.refresh(index=self.index_name)\n\n    def query_raw(self, query: dict) -&gt; List[Document]:\n        \"\"\"Query Elasticsearch store using query format of ES client\n\n        Args:\n            query (dict): Elasticsearch query format\n\n        Returns:\n            List[Document]: List of result documents\n        \"\"\"\n        res = self.client.search(index=self.index_name, body=query)\n        docs = []\n        for r in res[\"hits\"][\"hits\"]:\n            docs.append(\n                Document(\n                    id_=r[\"_id\"],\n                    text=r[\"_source\"][\"content\"],\n                    metadata=r[\"_source\"][\"metadata\"],\n                )\n            )\n        return docs\n\n    def query(self, query: str, top_k: int = 10) -&gt; List[Document]:\n        \"\"\"Search Elasticsearch docstore using search query (BM25)\n\n        Args:\n            query (str): query text\n            top_k (int, optional): number of\n                top documents to return. Defaults to 10.\n\n        Returns:\n            List[Document]: List of result documents\n        \"\"\"\n        query_dict = {\"query\": {\"match\": {\"content\": query}}, \"size\": top_k}\n        return self.query_raw(query_dict)\n\n    def get(self, ids: Union[List[str], str]) -&gt; List[Document]:\n        \"\"\"Get document by id\"\"\"\n        if not isinstance(ids, list):\n            ids = [ids]\n        query_dict = {\"query\": {\"terms\": {\"_id\": ids}}}\n        return self.query_raw(query_dict)\n\n    def count(self) -&gt; int:\n        \"\"\"Count number of documents\"\"\"\n        count = int(\n            self.client.cat.count(index=self.index_name, format=\"json\")[0][\"count\"]\n        )\n        return count\n\n    def get_all(self) -&gt; List[Document]:\n        \"\"\"Get all documents\"\"\"\n        query_dict = {\"query\": {\"match_all\": {}}, \"size\": MAX_DOCS_TO_GET}\n        return self.query_raw(query_dict)\n\n    def delete(self, ids: Union[List[str], str]):\n        \"\"\"Delete document by id\"\"\"\n        if not isinstance(ids, list):\n            ids = [ids]\n\n        query = {\"query\": {\"terms\": {\"_id\": ids}}}\n        self.client.delete_by_query(index=self.index_name, body=query)\n        self.client.indices.refresh(index=self.index_name)\n\n    def __persist_flow__(self):\n        return {\n            \"index_name\": self.index_name,\n            \"elasticsearch_url\": self.elasticsearch_url,\n            \"k1\": self.k1,\n            \"b\": self.b,\n        }\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.ElasticsearchDocumentStore.add","title":"add","text":"<pre><code>add(docs, ids=None, **kwargs)\n</code></pre> <p>Add document into document store</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[Document, List[Document]]</code> <p>list of documents to add</p> required <code>ids</code> <code>Optional[Union[List[str], str]]</code> <p>specify the ids of documents to add or use existing doc.doc_id</p> <code>None</code> <code>refresh_indices</code> <p>request Elasticsearch to update its index (default to True)</p> required Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def add(\n    self,\n    docs: Union[Document, List[Document]],\n    ids: Optional[Union[List[str], str]] = None,\n    **kwargs\n):\n    \"\"\"Add document into document store\n\n    Args:\n        docs: list of documents to add\n        ids: specify the ids of documents to add or\n            use existing doc.doc_id\n        refresh_indices: request Elasticsearch to update\n            its index (default to True)\n    \"\"\"\n    refresh_indices = kwargs.pop(\"refresh_indices\", True)\n\n    if ids and not isinstance(ids, list):\n        ids = [ids]\n    if not isinstance(docs, list):\n        docs = [docs]\n    doc_ids = ids if ids else [doc.doc_id for doc in docs]\n\n    requests = []\n    for doc_id, doc in zip(doc_ids, docs):\n        text = doc.text\n        metadata = doc.metadata\n        request = {\n            \"_op_type\": \"index\",\n            \"_index\": self.index_name,\n            \"content\": text,\n            \"metadata\": metadata,\n            \"_id\": doc_id,\n        }\n        requests.append(request)\n    self.es_bulk(self.client, requests)\n\n    if refresh_indices:\n        self.client.indices.refresh(index=self.index_name)\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.ElasticsearchDocumentStore.query_raw","title":"query_raw","text":"<pre><code>query_raw(query)\n</code></pre> <p>Query Elasticsearch store using query format of ES client</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>dict</code> <p>Elasticsearch query format</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of result documents</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def query_raw(self, query: dict) -&gt; List[Document]:\n    \"\"\"Query Elasticsearch store using query format of ES client\n\n    Args:\n        query (dict): Elasticsearch query format\n\n    Returns:\n        List[Document]: List of result documents\n    \"\"\"\n    res = self.client.search(index=self.index_name, body=query)\n    docs = []\n    for r in res[\"hits\"][\"hits\"]:\n        docs.append(\n            Document(\n                id_=r[\"_id\"],\n                text=r[\"_source\"][\"content\"],\n                metadata=r[\"_source\"][\"metadata\"],\n            )\n        )\n    return docs\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.ElasticsearchDocumentStore.query","title":"query","text":"<pre><code>query(query, top_k=10)\n</code></pre> <p>Search Elasticsearch docstore using search query (BM25)</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query text</p> required <code>top_k</code> <code>int</code> <p>number of top documents to return. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of result documents</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def query(self, query: str, top_k: int = 10) -&gt; List[Document]:\n    \"\"\"Search Elasticsearch docstore using search query (BM25)\n\n    Args:\n        query (str): query text\n        top_k (int, optional): number of\n            top documents to return. Defaults to 10.\n\n    Returns:\n        List[Document]: List of result documents\n    \"\"\"\n    query_dict = {\"query\": {\"match\": {\"content\": query}}, \"size\": top_k}\n    return self.query_raw(query_dict)\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.ElasticsearchDocumentStore.get","title":"get","text":"<pre><code>get(ids)\n</code></pre> <p>Get document by id</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def get(self, ids: Union[List[str], str]) -&gt; List[Document]:\n    \"\"\"Get document by id\"\"\"\n    if not isinstance(ids, list):\n        ids = [ids]\n    query_dict = {\"query\": {\"terms\": {\"_id\": ids}}}\n    return self.query_raw(query_dict)\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.ElasticsearchDocumentStore.count","title":"count","text":"<pre><code>count()\n</code></pre> <p>Count number of documents</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def count(self) -&gt; int:\n    \"\"\"Count number of documents\"\"\"\n    count = int(\n        self.client.cat.count(index=self.index_name, format=\"json\")[0][\"count\"]\n    )\n    return count\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.ElasticsearchDocumentStore.get_all","title":"get_all","text":"<pre><code>get_all()\n</code></pre> <p>Get all documents</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def get_all(self) -&gt; List[Document]:\n    \"\"\"Get all documents\"\"\"\n    query_dict = {\"query\": {\"match_all\": {}}, \"size\": MAX_DOCS_TO_GET}\n    return self.query_raw(query_dict)\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.ElasticsearchDocumentStore.delete","title":"delete","text":"<pre><code>delete(ids)\n</code></pre> <p>Delete document by id</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def delete(self, ids: Union[List[str], str]):\n    \"\"\"Delete document by id\"\"\"\n    if not isinstance(ids, list):\n        ids = [ids]\n\n    query = {\"query\": {\"terms\": {\"_id\": ids}}}\n    self.client.delete_by_query(index=self.index_name, body=query)\n    self.client.indices.refresh(index=self.index_name)\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.InMemoryDocumentStore","title":"InMemoryDocumentStore","text":"<p>             Bases: <code>BaseDocumentStore</code></p> <p>Simple memory document store that store document in a dictionary</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>class InMemoryDocumentStore(BaseDocumentStore):\n    \"\"\"Simple memory document store that store document in a dictionary\"\"\"\n\n    def __init__(self):\n        self._store = {}\n\n    def add(\n        self,\n        docs: Union[Document, List[Document]],\n        ids: Optional[Union[List[str], str]] = None,\n        **kwargs,\n    ):\n        \"\"\"Add document into document store\n\n        Args:\n            docs: list of documents to add\n            ids: specify the ids of documents to add or\n                use existing doc.doc_id\n            exist_ok: raise error when duplicate doc-id\n                found in the docstore (default to False)\n        \"\"\"\n        exist_ok: bool = kwargs.pop(\"exist_ok\", False)\n\n        if ids and not isinstance(ids, list):\n            ids = [ids]\n        if not isinstance(docs, list):\n            docs = [docs]\n        doc_ids = ids if ids else [doc.doc_id for doc in docs]\n\n        for doc_id, doc in zip(doc_ids, docs):\n            if doc_id in self._store and not exist_ok:\n                raise ValueError(f\"Document with id {doc_id} already exist\")\n            self._store[doc_id] = doc\n\n    def get(self, ids: Union[List[str], str]) -&gt; List[Document]:\n        \"\"\"Get document by id\"\"\"\n        if not isinstance(ids, list):\n            ids = [ids]\n\n        return [self._store[doc_id] for doc_id in ids]\n\n    def get_all(self) -&gt; List[Document]:\n        \"\"\"Get all documents\"\"\"\n        return list(self._store.values())\n\n    def count(self) -&gt; int:\n        \"\"\"Count number of documents\"\"\"\n        return len(self._store)\n\n    def delete(self, ids: Union[List[str], str]):\n        \"\"\"Delete document by id\"\"\"\n        if not isinstance(ids, list):\n            ids = [ids]\n\n        for doc_id in ids:\n            del self._store[doc_id]\n\n    def save(self, path: Union[str, Path]):\n        \"\"\"Save document to path\"\"\"\n        store = {key: value.to_dict() for key, value in self._store.items()}\n        with open(path, \"w\") as f:\n            json.dump(store, f)\n\n    def load(self, path: Union[str, Path]):\n        \"\"\"Load document store from path\"\"\"\n        with open(path) as f:\n            store = json.load(f)\n        self._store = {key: Document.from_dict(value) for key, value in store.items()}\n\n    def __persist_flow__(self):\n        return {}\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.InMemoryDocumentStore.add","title":"add","text":"<pre><code>add(docs, ids=None, **kwargs)\n</code></pre> <p>Add document into document store</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[Document, List[Document]]</code> <p>list of documents to add</p> required <code>ids</code> <code>Optional[Union[List[str], str]]</code> <p>specify the ids of documents to add or use existing doc.doc_id</p> <code>None</code> <code>exist_ok</code> <p>raise error when duplicate doc-id found in the docstore (default to False)</p> required Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def add(\n    self,\n    docs: Union[Document, List[Document]],\n    ids: Optional[Union[List[str], str]] = None,\n    **kwargs,\n):\n    \"\"\"Add document into document store\n\n    Args:\n        docs: list of documents to add\n        ids: specify the ids of documents to add or\n            use existing doc.doc_id\n        exist_ok: raise error when duplicate doc-id\n            found in the docstore (default to False)\n    \"\"\"\n    exist_ok: bool = kwargs.pop(\"exist_ok\", False)\n\n    if ids and not isinstance(ids, list):\n        ids = [ids]\n    if not isinstance(docs, list):\n        docs = [docs]\n    doc_ids = ids if ids else [doc.doc_id for doc in docs]\n\n    for doc_id, doc in zip(doc_ids, docs):\n        if doc_id in self._store and not exist_ok:\n            raise ValueError(f\"Document with id {doc_id} already exist\")\n        self._store[doc_id] = doc\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.InMemoryDocumentStore.get","title":"get","text":"<pre><code>get(ids)\n</code></pre> <p>Get document by id</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def get(self, ids: Union[List[str], str]) -&gt; List[Document]:\n    \"\"\"Get document by id\"\"\"\n    if not isinstance(ids, list):\n        ids = [ids]\n\n    return [self._store[doc_id] for doc_id in ids]\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.InMemoryDocumentStore.get_all","title":"get_all","text":"<pre><code>get_all()\n</code></pre> <p>Get all documents</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def get_all(self) -&gt; List[Document]:\n    \"\"\"Get all documents\"\"\"\n    return list(self._store.values())\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.InMemoryDocumentStore.count","title":"count","text":"<pre><code>count()\n</code></pre> <p>Count number of documents</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def count(self) -&gt; int:\n    \"\"\"Count number of documents\"\"\"\n    return len(self._store)\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.InMemoryDocumentStore.delete","title":"delete","text":"<pre><code>delete(ids)\n</code></pre> <p>Delete document by id</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def delete(self, ids: Union[List[str], str]):\n    \"\"\"Delete document by id\"\"\"\n    if not isinstance(ids, list):\n        ids = [ids]\n\n    for doc_id in ids:\n        del self._store[doc_id]\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.InMemoryDocumentStore.save","title":"save","text":"<pre><code>save(path)\n</code></pre> <p>Save document to path</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def save(self, path: Union[str, Path]):\n    \"\"\"Save document to path\"\"\"\n    store = {key: value.to_dict() for key, value in self._store.items()}\n    with open(path, \"w\") as f:\n        json.dump(store, f)\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.InMemoryDocumentStore.load","title":"load","text":"<pre><code>load(path)\n</code></pre> <p>Load document store from path</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def load(self, path: Union[str, Path]):\n    \"\"\"Load document store from path\"\"\"\n    with open(path) as f:\n        store = json.load(f)\n    self._store = {key: Document.from_dict(value) for key, value in store.items()}\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.SimpleFileDocumentStore","title":"SimpleFileDocumentStore","text":"<p>             Bases: <code>InMemoryDocumentStore</code></p> <p>Improve InMemoryDocumentStore by auto saving whenever the corpus is changed</p> Source code in <code>kotaemon\\storages\\docstores\\simple_file.py</code> <pre><code>class SimpleFileDocumentStore(InMemoryDocumentStore):\n    \"\"\"Improve InMemoryDocumentStore by auto saving whenever the corpus is changed\"\"\"\n\n    def __init__(self, path: str | Path):\n        super().__init__()\n        self._path = path\n        if path is not None and Path(path).is_file():\n            self.load(path)\n\n    def add(\n        self,\n        docs: Union[Document, List[Document]],\n        ids: Optional[Union[List[str], str]] = None,\n        **kwargs,\n    ):\n        \"\"\"Add document into document store\n\n        Args:\n            docs: list of documents to add\n            ids: specify the ids of documents to add or\n                use existing doc.doc_id\n            exist_ok: raise error when duplicate doc-id\n                found in the docstore (default to False)\n        \"\"\"\n        super().add(docs=docs, ids=ids, **kwargs)\n        self.save(self._path)\n\n    def delete(self, ids: Union[List[str], str]):\n        \"\"\"Delete document by id\"\"\"\n        super().delete(ids=ids)\n        self.save(self._path)\n\n    def __persist_flow__(self):\n        from theflow.utils.modules import serialize\n\n        return {\"path\": serialize(self._path)}\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.SimpleFileDocumentStore.add","title":"add","text":"<pre><code>add(docs, ids=None, **kwargs)\n</code></pre> <p>Add document into document store</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[Document, List[Document]]</code> <p>list of documents to add</p> required <code>ids</code> <code>Optional[Union[List[str], str]]</code> <p>specify the ids of documents to add or use existing doc.doc_id</p> <code>None</code> <code>exist_ok</code> <p>raise error when duplicate doc-id found in the docstore (default to False)</p> required Source code in <code>kotaemon\\storages\\docstores\\simple_file.py</code> <pre><code>def add(\n    self,\n    docs: Union[Document, List[Document]],\n    ids: Optional[Union[List[str], str]] = None,\n    **kwargs,\n):\n    \"\"\"Add document into document store\n\n    Args:\n        docs: list of documents to add\n        ids: specify the ids of documents to add or\n            use existing doc.doc_id\n        exist_ok: raise error when duplicate doc-id\n            found in the docstore (default to False)\n    \"\"\"\n    super().add(docs=docs, ids=ids, **kwargs)\n    self.save(self._path)\n</code></pre>"},{"location":"reference/storages/docstores/#storages.docstores.SimpleFileDocumentStore.delete","title":"delete","text":"<pre><code>delete(ids)\n</code></pre> <p>Delete document by id</p> Source code in <code>kotaemon\\storages\\docstores\\simple_file.py</code> <pre><code>def delete(self, ids: Union[List[str], str]):\n    \"\"\"Delete document by id\"\"\"\n    super().delete(ids=ids)\n    self.save(self._path)\n</code></pre>"},{"location":"reference/storages/docstores/base/","title":"Base","text":""},{"location":"reference/storages/docstores/base/#storages.docstores.base.BaseDocumentStore","title":"BaseDocumentStore","text":"<p>             Bases: <code>ABC</code></p> <p>A document store is in charged of storing and managing documents</p> Source code in <code>kotaemon\\storages\\docstores\\base.py</code> <pre><code>class BaseDocumentStore(ABC):\n    \"\"\"A document store is in charged of storing and managing documents\"\"\"\n\n    @abstractmethod\n    def __init__(self, *args, **kwargs):\n        ...\n\n    @abstractmethod\n    def add(\n        self,\n        docs: Union[Document, List[Document]],\n        ids: Optional[Union[List[str], str]] = None,\n        **kwargs,\n    ):\n        \"\"\"Add document into document store\n\n        Args:\n            docs: Document or list of documents\n            ids: List of ids of the documents. Optional, if not set will use doc.doc_id\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get(self, ids: Union[List[str], str]) -&gt; List[Document]:\n        \"\"\"Get document by id\"\"\"\n        ...\n\n    @abstractmethod\n    def get_all(self) -&gt; List[Document]:\n        \"\"\"Get all documents\"\"\"\n        ...\n\n    @abstractmethod\n    def count(self) -&gt; int:\n        \"\"\"Count number of documents\"\"\"\n        ...\n\n    @abstractmethod\n    def delete(self, ids: Union[List[str], str]):\n        \"\"\"Delete document by id\"\"\"\n        ...\n</code></pre>"},{"location":"reference/storages/docstores/base/#storages.docstores.base.BaseDocumentStore.add","title":"add  <code>abstractmethod</code>","text":"<pre><code>add(docs, ids=None, **kwargs)\n</code></pre> <p>Add document into document store</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[Document, List[Document]]</code> <p>Document or list of documents</p> required <code>ids</code> <code>Optional[Union[List[str], str]]</code> <p>List of ids of the documents. Optional, if not set will use doc.doc_id</p> <code>None</code> Source code in <code>kotaemon\\storages\\docstores\\base.py</code> <pre><code>@abstractmethod\ndef add(\n    self,\n    docs: Union[Document, List[Document]],\n    ids: Optional[Union[List[str], str]] = None,\n    **kwargs,\n):\n    \"\"\"Add document into document store\n\n    Args:\n        docs: Document or list of documents\n        ids: List of ids of the documents. Optional, if not set will use doc.doc_id\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/docstores/base/#storages.docstores.base.BaseDocumentStore.get","title":"get  <code>abstractmethod</code>","text":"<pre><code>get(ids)\n</code></pre> <p>Get document by id</p> Source code in <code>kotaemon\\storages\\docstores\\base.py</code> <pre><code>@abstractmethod\ndef get(self, ids: Union[List[str], str]) -&gt; List[Document]:\n    \"\"\"Get document by id\"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/docstores/base/#storages.docstores.base.BaseDocumentStore.get_all","title":"get_all  <code>abstractmethod</code>","text":"<pre><code>get_all()\n</code></pre> <p>Get all documents</p> Source code in <code>kotaemon\\storages\\docstores\\base.py</code> <pre><code>@abstractmethod\ndef get_all(self) -&gt; List[Document]:\n    \"\"\"Get all documents\"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/docstores/base/#storages.docstores.base.BaseDocumentStore.count","title":"count  <code>abstractmethod</code>","text":"<pre><code>count()\n</code></pre> <p>Count number of documents</p> Source code in <code>kotaemon\\storages\\docstores\\base.py</code> <pre><code>@abstractmethod\ndef count(self) -&gt; int:\n    \"\"\"Count number of documents\"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/docstores/base/#storages.docstores.base.BaseDocumentStore.delete","title":"delete  <code>abstractmethod</code>","text":"<pre><code>delete(ids)\n</code></pre> <p>Delete document by id</p> Source code in <code>kotaemon\\storages\\docstores\\base.py</code> <pre><code>@abstractmethod\ndef delete(self, ids: Union[List[str], str]):\n    \"\"\"Delete document by id\"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/docstores/elasticsearch/","title":"Elasticsearch","text":""},{"location":"reference/storages/docstores/elasticsearch/#storages.docstores.elasticsearch.ElasticsearchDocumentStore","title":"ElasticsearchDocumentStore","text":"<p>             Bases: <code>BaseDocumentStore</code></p> <p>Simple memory document store that store document in a dictionary</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>class ElasticsearchDocumentStore(BaseDocumentStore):\n    \"\"\"Simple memory document store that store document in a dictionary\"\"\"\n\n    def __init__(\n        self,\n        index_name: str = \"docstore\",\n        elasticsearch_url: str = \"http://localhost:9200\",\n        k1: float = 2.0,\n        b: float = 0.75,\n    ):\n        try:\n            from elasticsearch import Elasticsearch\n            from elasticsearch.helpers import bulk\n        except ImportError:\n            raise ImportError(\n                \"To use ElaticsearchDocstore please install `pip install elasticsearch`\"\n            )\n\n        self.elasticsearch_url = elasticsearch_url\n        self.index_name = index_name\n        self.k1 = k1\n        self.b = b\n\n        # Create an Elasticsearch client instance\n        self.client = Elasticsearch(elasticsearch_url)\n        self.es_bulk = bulk\n        # Define the index settings and mappings\n        settings = {\n            \"analysis\": {\"analyzer\": {\"default\": {\"type\": \"standard\"}}},\n            \"similarity\": {\n                \"custom_bm25\": {\n                    \"type\": \"BM25\",\n                    \"k1\": k1,\n                    \"b\": b,\n                }\n            },\n        }\n        mappings = {\n            \"properties\": {\n                \"content\": {\n                    \"type\": \"text\",\n                    \"similarity\": \"custom_bm25\",  # Use the custom BM25 similarity\n                }\n            }\n        }\n\n        # Create the index with the specified settings and mappings\n        if not self.client.indices.exists(index=index_name):\n            self.client.indices.create(\n                index=index_name, mappings=mappings, settings=settings\n            )\n\n    def add(\n        self,\n        docs: Union[Document, List[Document]],\n        ids: Optional[Union[List[str], str]] = None,\n        **kwargs\n    ):\n        \"\"\"Add document into document store\n\n        Args:\n            docs: list of documents to add\n            ids: specify the ids of documents to add or\n                use existing doc.doc_id\n            refresh_indices: request Elasticsearch to update\n                its index (default to True)\n        \"\"\"\n        refresh_indices = kwargs.pop(\"refresh_indices\", True)\n\n        if ids and not isinstance(ids, list):\n            ids = [ids]\n        if not isinstance(docs, list):\n            docs = [docs]\n        doc_ids = ids if ids else [doc.doc_id for doc in docs]\n\n        requests = []\n        for doc_id, doc in zip(doc_ids, docs):\n            text = doc.text\n            metadata = doc.metadata\n            request = {\n                \"_op_type\": \"index\",\n                \"_index\": self.index_name,\n                \"content\": text,\n                \"metadata\": metadata,\n                \"_id\": doc_id,\n            }\n            requests.append(request)\n        self.es_bulk(self.client, requests)\n\n        if refresh_indices:\n            self.client.indices.refresh(index=self.index_name)\n\n    def query_raw(self, query: dict) -&gt; List[Document]:\n        \"\"\"Query Elasticsearch store using query format of ES client\n\n        Args:\n            query (dict): Elasticsearch query format\n\n        Returns:\n            List[Document]: List of result documents\n        \"\"\"\n        res = self.client.search(index=self.index_name, body=query)\n        docs = []\n        for r in res[\"hits\"][\"hits\"]:\n            docs.append(\n                Document(\n                    id_=r[\"_id\"],\n                    text=r[\"_source\"][\"content\"],\n                    metadata=r[\"_source\"][\"metadata\"],\n                )\n            )\n        return docs\n\n    def query(self, query: str, top_k: int = 10) -&gt; List[Document]:\n        \"\"\"Search Elasticsearch docstore using search query (BM25)\n\n        Args:\n            query (str): query text\n            top_k (int, optional): number of\n                top documents to return. Defaults to 10.\n\n        Returns:\n            List[Document]: List of result documents\n        \"\"\"\n        query_dict = {\"query\": {\"match\": {\"content\": query}}, \"size\": top_k}\n        return self.query_raw(query_dict)\n\n    def get(self, ids: Union[List[str], str]) -&gt; List[Document]:\n        \"\"\"Get document by id\"\"\"\n        if not isinstance(ids, list):\n            ids = [ids]\n        query_dict = {\"query\": {\"terms\": {\"_id\": ids}}}\n        return self.query_raw(query_dict)\n\n    def count(self) -&gt; int:\n        \"\"\"Count number of documents\"\"\"\n        count = int(\n            self.client.cat.count(index=self.index_name, format=\"json\")[0][\"count\"]\n        )\n        return count\n\n    def get_all(self) -&gt; List[Document]:\n        \"\"\"Get all documents\"\"\"\n        query_dict = {\"query\": {\"match_all\": {}}, \"size\": MAX_DOCS_TO_GET}\n        return self.query_raw(query_dict)\n\n    def delete(self, ids: Union[List[str], str]):\n        \"\"\"Delete document by id\"\"\"\n        if not isinstance(ids, list):\n            ids = [ids]\n\n        query = {\"query\": {\"terms\": {\"_id\": ids}}}\n        self.client.delete_by_query(index=self.index_name, body=query)\n        self.client.indices.refresh(index=self.index_name)\n\n    def __persist_flow__(self):\n        return {\n            \"index_name\": self.index_name,\n            \"elasticsearch_url\": self.elasticsearch_url,\n            \"k1\": self.k1,\n            \"b\": self.b,\n        }\n</code></pre>"},{"location":"reference/storages/docstores/elasticsearch/#storages.docstores.elasticsearch.ElasticsearchDocumentStore.add","title":"add","text":"<pre><code>add(docs, ids=None, **kwargs)\n</code></pre> <p>Add document into document store</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[Document, List[Document]]</code> <p>list of documents to add</p> required <code>ids</code> <code>Optional[Union[List[str], str]]</code> <p>specify the ids of documents to add or use existing doc.doc_id</p> <code>None</code> <code>refresh_indices</code> <p>request Elasticsearch to update its index (default to True)</p> required Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def add(\n    self,\n    docs: Union[Document, List[Document]],\n    ids: Optional[Union[List[str], str]] = None,\n    **kwargs\n):\n    \"\"\"Add document into document store\n\n    Args:\n        docs: list of documents to add\n        ids: specify the ids of documents to add or\n            use existing doc.doc_id\n        refresh_indices: request Elasticsearch to update\n            its index (default to True)\n    \"\"\"\n    refresh_indices = kwargs.pop(\"refresh_indices\", True)\n\n    if ids and not isinstance(ids, list):\n        ids = [ids]\n    if not isinstance(docs, list):\n        docs = [docs]\n    doc_ids = ids if ids else [doc.doc_id for doc in docs]\n\n    requests = []\n    for doc_id, doc in zip(doc_ids, docs):\n        text = doc.text\n        metadata = doc.metadata\n        request = {\n            \"_op_type\": \"index\",\n            \"_index\": self.index_name,\n            \"content\": text,\n            \"metadata\": metadata,\n            \"_id\": doc_id,\n        }\n        requests.append(request)\n    self.es_bulk(self.client, requests)\n\n    if refresh_indices:\n        self.client.indices.refresh(index=self.index_name)\n</code></pre>"},{"location":"reference/storages/docstores/elasticsearch/#storages.docstores.elasticsearch.ElasticsearchDocumentStore.query_raw","title":"query_raw","text":"<pre><code>query_raw(query)\n</code></pre> <p>Query Elasticsearch store using query format of ES client</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>dict</code> <p>Elasticsearch query format</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of result documents</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def query_raw(self, query: dict) -&gt; List[Document]:\n    \"\"\"Query Elasticsearch store using query format of ES client\n\n    Args:\n        query (dict): Elasticsearch query format\n\n    Returns:\n        List[Document]: List of result documents\n    \"\"\"\n    res = self.client.search(index=self.index_name, body=query)\n    docs = []\n    for r in res[\"hits\"][\"hits\"]:\n        docs.append(\n            Document(\n                id_=r[\"_id\"],\n                text=r[\"_source\"][\"content\"],\n                metadata=r[\"_source\"][\"metadata\"],\n            )\n        )\n    return docs\n</code></pre>"},{"location":"reference/storages/docstores/elasticsearch/#storages.docstores.elasticsearch.ElasticsearchDocumentStore.query","title":"query","text":"<pre><code>query(query, top_k=10)\n</code></pre> <p>Search Elasticsearch docstore using search query (BM25)</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query text</p> required <code>top_k</code> <code>int</code> <p>number of top documents to return. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of result documents</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def query(self, query: str, top_k: int = 10) -&gt; List[Document]:\n    \"\"\"Search Elasticsearch docstore using search query (BM25)\n\n    Args:\n        query (str): query text\n        top_k (int, optional): number of\n            top documents to return. Defaults to 10.\n\n    Returns:\n        List[Document]: List of result documents\n    \"\"\"\n    query_dict = {\"query\": {\"match\": {\"content\": query}}, \"size\": top_k}\n    return self.query_raw(query_dict)\n</code></pre>"},{"location":"reference/storages/docstores/elasticsearch/#storages.docstores.elasticsearch.ElasticsearchDocumentStore.get","title":"get","text":"<pre><code>get(ids)\n</code></pre> <p>Get document by id</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def get(self, ids: Union[List[str], str]) -&gt; List[Document]:\n    \"\"\"Get document by id\"\"\"\n    if not isinstance(ids, list):\n        ids = [ids]\n    query_dict = {\"query\": {\"terms\": {\"_id\": ids}}}\n    return self.query_raw(query_dict)\n</code></pre>"},{"location":"reference/storages/docstores/elasticsearch/#storages.docstores.elasticsearch.ElasticsearchDocumentStore.count","title":"count","text":"<pre><code>count()\n</code></pre> <p>Count number of documents</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def count(self) -&gt; int:\n    \"\"\"Count number of documents\"\"\"\n    count = int(\n        self.client.cat.count(index=self.index_name, format=\"json\")[0][\"count\"]\n    )\n    return count\n</code></pre>"},{"location":"reference/storages/docstores/elasticsearch/#storages.docstores.elasticsearch.ElasticsearchDocumentStore.get_all","title":"get_all","text":"<pre><code>get_all()\n</code></pre> <p>Get all documents</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def get_all(self) -&gt; List[Document]:\n    \"\"\"Get all documents\"\"\"\n    query_dict = {\"query\": {\"match_all\": {}}, \"size\": MAX_DOCS_TO_GET}\n    return self.query_raw(query_dict)\n</code></pre>"},{"location":"reference/storages/docstores/elasticsearch/#storages.docstores.elasticsearch.ElasticsearchDocumentStore.delete","title":"delete","text":"<pre><code>delete(ids)\n</code></pre> <p>Delete document by id</p> Source code in <code>kotaemon\\storages\\docstores\\elasticsearch.py</code> <pre><code>def delete(self, ids: Union[List[str], str]):\n    \"\"\"Delete document by id\"\"\"\n    if not isinstance(ids, list):\n        ids = [ids]\n\n    query = {\"query\": {\"terms\": {\"_id\": ids}}}\n    self.client.delete_by_query(index=self.index_name, body=query)\n    self.client.indices.refresh(index=self.index_name)\n</code></pre>"},{"location":"reference/storages/docstores/in_memory/","title":"In Memory","text":""},{"location":"reference/storages/docstores/in_memory/#storages.docstores.in_memory.InMemoryDocumentStore","title":"InMemoryDocumentStore","text":"<p>             Bases: <code>BaseDocumentStore</code></p> <p>Simple memory document store that store document in a dictionary</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>class InMemoryDocumentStore(BaseDocumentStore):\n    \"\"\"Simple memory document store that store document in a dictionary\"\"\"\n\n    def __init__(self):\n        self._store = {}\n\n    def add(\n        self,\n        docs: Union[Document, List[Document]],\n        ids: Optional[Union[List[str], str]] = None,\n        **kwargs,\n    ):\n        \"\"\"Add document into document store\n\n        Args:\n            docs: list of documents to add\n            ids: specify the ids of documents to add or\n                use existing doc.doc_id\n            exist_ok: raise error when duplicate doc-id\n                found in the docstore (default to False)\n        \"\"\"\n        exist_ok: bool = kwargs.pop(\"exist_ok\", False)\n\n        if ids and not isinstance(ids, list):\n            ids = [ids]\n        if not isinstance(docs, list):\n            docs = [docs]\n        doc_ids = ids if ids else [doc.doc_id for doc in docs]\n\n        for doc_id, doc in zip(doc_ids, docs):\n            if doc_id in self._store and not exist_ok:\n                raise ValueError(f\"Document with id {doc_id} already exist\")\n            self._store[doc_id] = doc\n\n    def get(self, ids: Union[List[str], str]) -&gt; List[Document]:\n        \"\"\"Get document by id\"\"\"\n        if not isinstance(ids, list):\n            ids = [ids]\n\n        return [self._store[doc_id] for doc_id in ids]\n\n    def get_all(self) -&gt; List[Document]:\n        \"\"\"Get all documents\"\"\"\n        return list(self._store.values())\n\n    def count(self) -&gt; int:\n        \"\"\"Count number of documents\"\"\"\n        return len(self._store)\n\n    def delete(self, ids: Union[List[str], str]):\n        \"\"\"Delete document by id\"\"\"\n        if not isinstance(ids, list):\n            ids = [ids]\n\n        for doc_id in ids:\n            del self._store[doc_id]\n\n    def save(self, path: Union[str, Path]):\n        \"\"\"Save document to path\"\"\"\n        store = {key: value.to_dict() for key, value in self._store.items()}\n        with open(path, \"w\") as f:\n            json.dump(store, f)\n\n    def load(self, path: Union[str, Path]):\n        \"\"\"Load document store from path\"\"\"\n        with open(path) as f:\n            store = json.load(f)\n        self._store = {key: Document.from_dict(value) for key, value in store.items()}\n\n    def __persist_flow__(self):\n        return {}\n</code></pre>"},{"location":"reference/storages/docstores/in_memory/#storages.docstores.in_memory.InMemoryDocumentStore.add","title":"add","text":"<pre><code>add(docs, ids=None, **kwargs)\n</code></pre> <p>Add document into document store</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[Document, List[Document]]</code> <p>list of documents to add</p> required <code>ids</code> <code>Optional[Union[List[str], str]]</code> <p>specify the ids of documents to add or use existing doc.doc_id</p> <code>None</code> <code>exist_ok</code> <p>raise error when duplicate doc-id found in the docstore (default to False)</p> required Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def add(\n    self,\n    docs: Union[Document, List[Document]],\n    ids: Optional[Union[List[str], str]] = None,\n    **kwargs,\n):\n    \"\"\"Add document into document store\n\n    Args:\n        docs: list of documents to add\n        ids: specify the ids of documents to add or\n            use existing doc.doc_id\n        exist_ok: raise error when duplicate doc-id\n            found in the docstore (default to False)\n    \"\"\"\n    exist_ok: bool = kwargs.pop(\"exist_ok\", False)\n\n    if ids and not isinstance(ids, list):\n        ids = [ids]\n    if not isinstance(docs, list):\n        docs = [docs]\n    doc_ids = ids if ids else [doc.doc_id for doc in docs]\n\n    for doc_id, doc in zip(doc_ids, docs):\n        if doc_id in self._store and not exist_ok:\n            raise ValueError(f\"Document with id {doc_id} already exist\")\n        self._store[doc_id] = doc\n</code></pre>"},{"location":"reference/storages/docstores/in_memory/#storages.docstores.in_memory.InMemoryDocumentStore.get","title":"get","text":"<pre><code>get(ids)\n</code></pre> <p>Get document by id</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def get(self, ids: Union[List[str], str]) -&gt; List[Document]:\n    \"\"\"Get document by id\"\"\"\n    if not isinstance(ids, list):\n        ids = [ids]\n\n    return [self._store[doc_id] for doc_id in ids]\n</code></pre>"},{"location":"reference/storages/docstores/in_memory/#storages.docstores.in_memory.InMemoryDocumentStore.get_all","title":"get_all","text":"<pre><code>get_all()\n</code></pre> <p>Get all documents</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def get_all(self) -&gt; List[Document]:\n    \"\"\"Get all documents\"\"\"\n    return list(self._store.values())\n</code></pre>"},{"location":"reference/storages/docstores/in_memory/#storages.docstores.in_memory.InMemoryDocumentStore.count","title":"count","text":"<pre><code>count()\n</code></pre> <p>Count number of documents</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def count(self) -&gt; int:\n    \"\"\"Count number of documents\"\"\"\n    return len(self._store)\n</code></pre>"},{"location":"reference/storages/docstores/in_memory/#storages.docstores.in_memory.InMemoryDocumentStore.delete","title":"delete","text":"<pre><code>delete(ids)\n</code></pre> <p>Delete document by id</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def delete(self, ids: Union[List[str], str]):\n    \"\"\"Delete document by id\"\"\"\n    if not isinstance(ids, list):\n        ids = [ids]\n\n    for doc_id in ids:\n        del self._store[doc_id]\n</code></pre>"},{"location":"reference/storages/docstores/in_memory/#storages.docstores.in_memory.InMemoryDocumentStore.save","title":"save","text":"<pre><code>save(path)\n</code></pre> <p>Save document to path</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def save(self, path: Union[str, Path]):\n    \"\"\"Save document to path\"\"\"\n    store = {key: value.to_dict() for key, value in self._store.items()}\n    with open(path, \"w\") as f:\n        json.dump(store, f)\n</code></pre>"},{"location":"reference/storages/docstores/in_memory/#storages.docstores.in_memory.InMemoryDocumentStore.load","title":"load","text":"<pre><code>load(path)\n</code></pre> <p>Load document store from path</p> Source code in <code>kotaemon\\storages\\docstores\\in_memory.py</code> <pre><code>def load(self, path: Union[str, Path]):\n    \"\"\"Load document store from path\"\"\"\n    with open(path) as f:\n        store = json.load(f)\n    self._store = {key: Document.from_dict(value) for key, value in store.items()}\n</code></pre>"},{"location":"reference/storages/docstores/simple_file/","title":"Simple File","text":""},{"location":"reference/storages/docstores/simple_file/#storages.docstores.simple_file.SimpleFileDocumentStore","title":"SimpleFileDocumentStore","text":"<p>             Bases: <code>InMemoryDocumentStore</code></p> <p>Improve InMemoryDocumentStore by auto saving whenever the corpus is changed</p> Source code in <code>kotaemon\\storages\\docstores\\simple_file.py</code> <pre><code>class SimpleFileDocumentStore(InMemoryDocumentStore):\n    \"\"\"Improve InMemoryDocumentStore by auto saving whenever the corpus is changed\"\"\"\n\n    def __init__(self, path: str | Path):\n        super().__init__()\n        self._path = path\n        if path is not None and Path(path).is_file():\n            self.load(path)\n\n    def add(\n        self,\n        docs: Union[Document, List[Document]],\n        ids: Optional[Union[List[str], str]] = None,\n        **kwargs,\n    ):\n        \"\"\"Add document into document store\n\n        Args:\n            docs: list of documents to add\n            ids: specify the ids of documents to add or\n                use existing doc.doc_id\n            exist_ok: raise error when duplicate doc-id\n                found in the docstore (default to False)\n        \"\"\"\n        super().add(docs=docs, ids=ids, **kwargs)\n        self.save(self._path)\n\n    def delete(self, ids: Union[List[str], str]):\n        \"\"\"Delete document by id\"\"\"\n        super().delete(ids=ids)\n        self.save(self._path)\n\n    def __persist_flow__(self):\n        from theflow.utils.modules import serialize\n\n        return {\"path\": serialize(self._path)}\n</code></pre>"},{"location":"reference/storages/docstores/simple_file/#storages.docstores.simple_file.SimpleFileDocumentStore.add","title":"add","text":"<pre><code>add(docs, ids=None, **kwargs)\n</code></pre> <p>Add document into document store</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[Document, List[Document]]</code> <p>list of documents to add</p> required <code>ids</code> <code>Optional[Union[List[str], str]]</code> <p>specify the ids of documents to add or use existing doc.doc_id</p> <code>None</code> <code>exist_ok</code> <p>raise error when duplicate doc-id found in the docstore (default to False)</p> required Source code in <code>kotaemon\\storages\\docstores\\simple_file.py</code> <pre><code>def add(\n    self,\n    docs: Union[Document, List[Document]],\n    ids: Optional[Union[List[str], str]] = None,\n    **kwargs,\n):\n    \"\"\"Add document into document store\n\n    Args:\n        docs: list of documents to add\n        ids: specify the ids of documents to add or\n            use existing doc.doc_id\n        exist_ok: raise error when duplicate doc-id\n            found in the docstore (default to False)\n    \"\"\"\n    super().add(docs=docs, ids=ids, **kwargs)\n    self.save(self._path)\n</code></pre>"},{"location":"reference/storages/docstores/simple_file/#storages.docstores.simple_file.SimpleFileDocumentStore.delete","title":"delete","text":"<pre><code>delete(ids)\n</code></pre> <p>Delete document by id</p> Source code in <code>kotaemon\\storages\\docstores\\simple_file.py</code> <pre><code>def delete(self, ids: Union[List[str], str]):\n    \"\"\"Delete document by id\"\"\"\n    super().delete(ids=ids)\n    self.save(self._path)\n</code></pre>"},{"location":"reference/storages/vectorstores/","title":"Vectorstores","text":""},{"location":"reference/storages/vectorstores/#storages.vectorstores.BaseVectorStore","title":"BaseVectorStore","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>kotaemon\\storages\\vectorstores\\base.py</code> <pre><code>class BaseVectorStore(ABC):\n    @abstractmethod\n    def __init__(self, *args, **kwargs):\n        ...\n\n    @abstractmethod\n    def add(\n        self,\n        embeddings: list[list[float]] | list[DocumentWithEmbedding],\n        metadatas: Optional[list[dict]] = None,\n        ids: Optional[list[str]] = None,\n    ) -&gt; list[str]:\n        \"\"\"Add vector embeddings to vector stores\n\n        Args:\n            embeddings: List of embeddings\n            metadatas: List of metadata of the embeddings\n            ids: List of ids of the embeddings\n            kwargs: meant for vectorstore-specific parameters\n\n        Returns:\n            List of ids of the embeddings\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def delete(self, ids: list[str], **kwargs):\n        \"\"\"Delete vector embeddings from vector stores\n\n        Args:\n            ids: List of ids of the embeddings to be deleted\n            kwargs: meant for vectorstore-specific parameters\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def query(\n        self,\n        embedding: list[float],\n        top_k: int = 1,\n        ids: Optional[list[str]] = None,\n        **kwargs,\n    ) -&gt; tuple[list[list[float]], list[float], list[str]]:\n        \"\"\"Return the top k most similar vector embeddings\n\n        Args:\n            embedding: List of embeddings\n            top_k: Number of most similar embeddings to return\n            ids: List of ids of the embeddings to be queried\n\n        Returns:\n            the matched embeddings, the similarity scores, and the ids\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/storages/vectorstores/#storages.vectorstores.BaseVectorStore.add","title":"add  <code>abstractmethod</code>","text":"<pre><code>add(embeddings, metadatas=None, ids=None)\n</code></pre> <p>Add vector embeddings to vector stores</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>list[list[float]] | list[DocumentWithEmbedding]</code> <p>List of embeddings</p> required <code>metadatas</code> <code>Optional[list[dict]]</code> <p>List of metadata of the embeddings</p> <code>None</code> <code>ids</code> <code>Optional[list[str]]</code> <p>List of ids of the embeddings</p> <code>None</code> <code>kwargs</code> <p>meant for vectorstore-specific parameters</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of ids of the embeddings</p> Source code in <code>kotaemon\\storages\\vectorstores\\base.py</code> <pre><code>@abstractmethod\ndef add(\n    self,\n    embeddings: list[list[float]] | list[DocumentWithEmbedding],\n    metadatas: Optional[list[dict]] = None,\n    ids: Optional[list[str]] = None,\n) -&gt; list[str]:\n    \"\"\"Add vector embeddings to vector stores\n\n    Args:\n        embeddings: List of embeddings\n        metadatas: List of metadata of the embeddings\n        ids: List of ids of the embeddings\n        kwargs: meant for vectorstore-specific parameters\n\n    Returns:\n        List of ids of the embeddings\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/vectorstores/#storages.vectorstores.BaseVectorStore.delete","title":"delete  <code>abstractmethod</code>","text":"<pre><code>delete(ids, **kwargs)\n</code></pre> <p>Delete vector embeddings from vector stores</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>list[str]</code> <p>List of ids of the embeddings to be deleted</p> required <code>kwargs</code> <p>meant for vectorstore-specific parameters</p> <code>{}</code> Source code in <code>kotaemon\\storages\\vectorstores\\base.py</code> <pre><code>@abstractmethod\ndef delete(self, ids: list[str], **kwargs):\n    \"\"\"Delete vector embeddings from vector stores\n\n    Args:\n        ids: List of ids of the embeddings to be deleted\n        kwargs: meant for vectorstore-specific parameters\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/vectorstores/#storages.vectorstores.BaseVectorStore.query","title":"query  <code>abstractmethod</code>","text":"<pre><code>query(embedding, top_k=1, ids=None, **kwargs)\n</code></pre> <p>Return the top k most similar vector embeddings</p> <p>Parameters:</p> Name Type Description Default <code>embedding</code> <code>list[float]</code> <p>List of embeddings</p> required <code>top_k</code> <code>int</code> <p>Number of most similar embeddings to return</p> <code>1</code> <code>ids</code> <code>Optional[list[str]]</code> <p>List of ids of the embeddings to be queried</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[list[list[float]], list[float], list[str]]</code> <p>the matched embeddings, the similarity scores, and the ids</p> Source code in <code>kotaemon\\storages\\vectorstores\\base.py</code> <pre><code>@abstractmethod\ndef query(\n    self,\n    embedding: list[float],\n    top_k: int = 1,\n    ids: Optional[list[str]] = None,\n    **kwargs,\n) -&gt; tuple[list[list[float]], list[float], list[str]]:\n    \"\"\"Return the top k most similar vector embeddings\n\n    Args:\n        embedding: List of embeddings\n        top_k: Number of most similar embeddings to return\n        ids: List of ids of the embeddings to be queried\n\n    Returns:\n        the matched embeddings, the similarity scores, and the ids\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/vectorstores/#storages.vectorstores.ChromaVectorStore","title":"ChromaVectorStore","text":"<p>             Bases: <code>LlamaIndexVectorStore</code></p> Source code in <code>kotaemon\\storages\\vectorstores\\chroma.py</code> <pre><code>class ChromaVectorStore(LlamaIndexVectorStore):\n    _li_class: Type[LIChromaVectorStore] = LIChromaVectorStore\n\n    def __init__(\n        self,\n        path: str = \"./chroma\",\n        collection_name: str = \"default\",\n        host: str = \"localhost\",\n        port: str = \"8000\",\n        ssl: bool = False,\n        headers: Optional[Dict[str, str]] = None,\n        collection_kwargs: Optional[dict] = None,\n        stores_text: bool = True,\n        flat_metadata: bool = True,\n        **kwargs: Any,\n    ):\n        self._path = path\n        self._collection_name = collection_name\n        self._host = host\n        self._port = port\n        self._ssl = ssl\n        self._headers = headers\n        self._collection_kwargs = collection_kwargs\n        self._stores_text = stores_text\n        self._flat_metadata = flat_metadata\n        self._kwargs = kwargs\n\n        try:\n            import chromadb\n        except ImportError:\n            raise ImportError(\n                \"ChromaVectorStore requires chromadb. \"\n                \"Please install chromadb first `pip install chromadb`\"\n            )\n\n        client = chromadb.PersistentClient(path=path)\n        collection = client.get_or_create_collection(collection_name)\n\n        # pass through for nice IDE support\n        super().__init__(\n            chroma_collection=collection,\n            host=host,\n            port=port,\n            ssl=ssl,\n            headers=headers or {},\n            collection_kwargs=collection_kwargs or {},\n            stores_text=stores_text,\n            flat_metadata=flat_metadata,\n            **kwargs,\n        )\n        self._client = cast(LIChromaVectorStore, self._client)\n\n    def delete(self, ids: List[str], **kwargs):\n        \"\"\"Delete vector embeddings from vector stores\n\n        Args:\n            ids: List of ids of the embeddings to be deleted\n            kwargs: meant for vectorstore-specific parameters\n        \"\"\"\n        self._client._collection.delete(ids=ids)\n\n    def delete_collection(self, collection_name: Optional[str] = None):\n        \"\"\"Delete entire collection under specified name from vector stores\n\n        Args:\n            collection_name: Name of the collection to delete\n        \"\"\"\n        # a rather ugly chain call but it do the job of finding\n        # original chromadb client and call delete_collection() method\n        if collection_name is None:\n            collection_name = self._client.client.name\n        self._client.client._client.delete_collection(collection_name)\n\n    def count(self) -&gt; int:\n        return self._collection.count()\n\n    def __persist_flow__(self):\n        return {\n            \"path\": self._path,\n            \"collection_name\": self._collection_name,\n            \"host\": self._host,\n            \"port\": self._port,\n            \"ssl\": self._ssl,\n            \"headers\": self._headers,\n            \"collection_kwargs\": self._collection_kwargs,\n            \"stores_text\": self._stores_text,\n            \"flat_metadata\": self._flat_metadata,\n            **self._kwargs,\n        }\n</code></pre>"},{"location":"reference/storages/vectorstores/#storages.vectorstores.ChromaVectorStore.delete","title":"delete","text":"<pre><code>delete(ids, **kwargs)\n</code></pre> <p>Delete vector embeddings from vector stores</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>List[str]</code> <p>List of ids of the embeddings to be deleted</p> required <code>kwargs</code> <p>meant for vectorstore-specific parameters</p> <code>{}</code> Source code in <code>kotaemon\\storages\\vectorstores\\chroma.py</code> <pre><code>def delete(self, ids: List[str], **kwargs):\n    \"\"\"Delete vector embeddings from vector stores\n\n    Args:\n        ids: List of ids of the embeddings to be deleted\n        kwargs: meant for vectorstore-specific parameters\n    \"\"\"\n    self._client._collection.delete(ids=ids)\n</code></pre>"},{"location":"reference/storages/vectorstores/#storages.vectorstores.ChromaVectorStore.delete_collection","title":"delete_collection","text":"<pre><code>delete_collection(collection_name=None)\n</code></pre> <p>Delete entire collection under specified name from vector stores</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>Optional[str]</code> <p>Name of the collection to delete</p> <code>None</code> Source code in <code>kotaemon\\storages\\vectorstores\\chroma.py</code> <pre><code>def delete_collection(self, collection_name: Optional[str] = None):\n    \"\"\"Delete entire collection under specified name from vector stores\n\n    Args:\n        collection_name: Name of the collection to delete\n    \"\"\"\n    # a rather ugly chain call but it do the job of finding\n    # original chromadb client and call delete_collection() method\n    if collection_name is None:\n        collection_name = self._client.client.name\n    self._client.client._client.delete_collection(collection_name)\n</code></pre>"},{"location":"reference/storages/vectorstores/#storages.vectorstores.InMemoryVectorStore","title":"InMemoryVectorStore","text":"<p>             Bases: <code>LlamaIndexVectorStore</code></p> Source code in <code>kotaemon\\storages\\vectorstores\\in_memory.py</code> <pre><code>class InMemoryVectorStore(LlamaIndexVectorStore):\n    _li_class: Type[LISimpleVectorStore] = LISimpleVectorStore\n    store_text: bool = False\n\n    def __init__(\n        self,\n        data: Optional[SimpleVectorStoreData] = None,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize params.\"\"\"\n        self._data = data or SimpleVectorStoreData()\n        self._fs = fs or fsspec.filesystem(\"file\")\n\n        super().__init__(\n            data=data,\n            fs=fs,\n            **kwargs,\n        )\n\n    def save(\n        self,\n        save_path: str,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n        **kwargs,\n    ):\n\n        \"\"\"save a simpleVectorStore to a dictionary.\n\n        Args:\n            save_path: Path of saving vector to disk.\n            fs: An abstract super-class for pythonic file-systems\n        \"\"\"\n        self._client.persist(persist_path=save_path, fs=fs)\n\n    def load(self, load_path: str, fs: Optional[fsspec.AbstractFileSystem] = None):\n\n        \"\"\"Create a SimpleKVStore from a load directory.\n\n        Args:\n            load_path: Path of loading vector.\n            fs: An abstract super-class for pythonic file-systems\n        \"\"\"\n        self._client = self._client.from_persist_path(persist_path=load_path, fs=fs)\n\n    def __persist_flow__(self):\n        d = self._data.to_dict()\n        d[\"__type__\"] = f\"{self._data.__module__}.{self._data.__class__.__qualname__}\"\n        return {\n            \"data\": d,\n            # \"fs\": self._fs,\n        }\n</code></pre>"},{"location":"reference/storages/vectorstores/#storages.vectorstores.InMemoryVectorStore.save","title":"save","text":"<pre><code>save(save_path, fs=None, **kwargs)\n</code></pre> <p>save a simpleVectorStore to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Path of saving vector to disk.</p> required <code>fs</code> <code>Optional[AbstractFileSystem]</code> <p>An abstract super-class for pythonic file-systems</p> <code>None</code> Source code in <code>kotaemon\\storages\\vectorstores\\in_memory.py</code> <pre><code>def save(\n    self,\n    save_path: str,\n    fs: Optional[fsspec.AbstractFileSystem] = None,\n    **kwargs,\n):\n\n    \"\"\"save a simpleVectorStore to a dictionary.\n\n    Args:\n        save_path: Path of saving vector to disk.\n        fs: An abstract super-class for pythonic file-systems\n    \"\"\"\n    self._client.persist(persist_path=save_path, fs=fs)\n</code></pre>"},{"location":"reference/storages/vectorstores/#storages.vectorstores.InMemoryVectorStore.load","title":"load","text":"<pre><code>load(load_path, fs=None)\n</code></pre> <p>Create a SimpleKVStore from a load directory.</p> <p>Parameters:</p> Name Type Description Default <code>load_path</code> <code>str</code> <p>Path of loading vector.</p> required <code>fs</code> <code>Optional[AbstractFileSystem]</code> <p>An abstract super-class for pythonic file-systems</p> <code>None</code> Source code in <code>kotaemon\\storages\\vectorstores\\in_memory.py</code> <pre><code>def load(self, load_path: str, fs: Optional[fsspec.AbstractFileSystem] = None):\n\n    \"\"\"Create a SimpleKVStore from a load directory.\n\n    Args:\n        load_path: Path of loading vector.\n        fs: An abstract super-class for pythonic file-systems\n    \"\"\"\n    self._client = self._client.from_persist_path(persist_path=load_path, fs=fs)\n</code></pre>"},{"location":"reference/storages/vectorstores/#storages.vectorstores.SimpleFileVectorStore","title":"SimpleFileVectorStore","text":"<p>             Bases: <code>LlamaIndexVectorStore</code></p> <p>Similar to InMemoryVectorStore but is backed by file by default</p> Source code in <code>kotaemon\\storages\\vectorstores\\simple_file.py</code> <pre><code>class SimpleFileVectorStore(LlamaIndexVectorStore):\n    \"\"\"Similar to InMemoryVectorStore but is backed by file by default\"\"\"\n\n    _li_class: Type[LISimpleVectorStore] = LISimpleVectorStore\n    store_text: bool = False\n\n    def __init__(\n        self,\n        path: str | Path,\n        data: Optional[SimpleVectorStoreData] = None,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize params.\"\"\"\n        self._data = data or SimpleVectorStoreData()\n        self._fs = fs or fsspec.filesystem(\"file\")\n        self._path = path\n        self._save_path = Path(path)\n\n        super().__init__(\n            data=data,\n            fs=fs,\n            **kwargs,\n        )\n\n        if self._save_path.is_file():\n            self._client = self._li_class.from_persist_path(\n                persist_path=str(self._save_path), fs=self._fs\n            )\n\n    def add(\n        self,\n        embeddings: list[list[float]] | list[DocumentWithEmbedding],\n        metadatas: Optional[list[dict]] = None,\n        ids: Optional[list[str]] = None,\n    ):\n        r = super().add(embeddings, metadatas, ids)\n        self._client.persist(str(self._save_path), self._fs)\n        return r\n\n    def delete(self, ids: list[str], **kwargs):\n        r = super().delete(ids, **kwargs)\n        self._client.persist(str(self._save_path), self._fs)\n        return r\n\n    def __persist_flow__(self):\n        d = self._data.to_dict()\n        d[\"__type__\"] = f\"{self._data.__module__}.{self._data.__class__.__qualname__}\"\n        return {\n            \"data\": d,\n            \"path\": str(self._path),\n            # \"fs\": self._fs,\n        }\n</code></pre>"},{"location":"reference/storages/vectorstores/base/","title":"Base","text":""},{"location":"reference/storages/vectorstores/base/#storages.vectorstores.base.BaseVectorStore","title":"BaseVectorStore","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>kotaemon\\storages\\vectorstores\\base.py</code> <pre><code>class BaseVectorStore(ABC):\n    @abstractmethod\n    def __init__(self, *args, **kwargs):\n        ...\n\n    @abstractmethod\n    def add(\n        self,\n        embeddings: list[list[float]] | list[DocumentWithEmbedding],\n        metadatas: Optional[list[dict]] = None,\n        ids: Optional[list[str]] = None,\n    ) -&gt; list[str]:\n        \"\"\"Add vector embeddings to vector stores\n\n        Args:\n            embeddings: List of embeddings\n            metadatas: List of metadata of the embeddings\n            ids: List of ids of the embeddings\n            kwargs: meant for vectorstore-specific parameters\n\n        Returns:\n            List of ids of the embeddings\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def delete(self, ids: list[str], **kwargs):\n        \"\"\"Delete vector embeddings from vector stores\n\n        Args:\n            ids: List of ids of the embeddings to be deleted\n            kwargs: meant for vectorstore-specific parameters\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def query(\n        self,\n        embedding: list[float],\n        top_k: int = 1,\n        ids: Optional[list[str]] = None,\n        **kwargs,\n    ) -&gt; tuple[list[list[float]], list[float], list[str]]:\n        \"\"\"Return the top k most similar vector embeddings\n\n        Args:\n            embedding: List of embeddings\n            top_k: Number of most similar embeddings to return\n            ids: List of ids of the embeddings to be queried\n\n        Returns:\n            the matched embeddings, the similarity scores, and the ids\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/storages/vectorstores/base/#storages.vectorstores.base.BaseVectorStore.add","title":"add  <code>abstractmethod</code>","text":"<pre><code>add(embeddings, metadatas=None, ids=None)\n</code></pre> <p>Add vector embeddings to vector stores</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>list[list[float]] | list[DocumentWithEmbedding]</code> <p>List of embeddings</p> required <code>metadatas</code> <code>Optional[list[dict]]</code> <p>List of metadata of the embeddings</p> <code>None</code> <code>ids</code> <code>Optional[list[str]]</code> <p>List of ids of the embeddings</p> <code>None</code> <code>kwargs</code> <p>meant for vectorstore-specific parameters</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of ids of the embeddings</p> Source code in <code>kotaemon\\storages\\vectorstores\\base.py</code> <pre><code>@abstractmethod\ndef add(\n    self,\n    embeddings: list[list[float]] | list[DocumentWithEmbedding],\n    metadatas: Optional[list[dict]] = None,\n    ids: Optional[list[str]] = None,\n) -&gt; list[str]:\n    \"\"\"Add vector embeddings to vector stores\n\n    Args:\n        embeddings: List of embeddings\n        metadatas: List of metadata of the embeddings\n        ids: List of ids of the embeddings\n        kwargs: meant for vectorstore-specific parameters\n\n    Returns:\n        List of ids of the embeddings\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/vectorstores/base/#storages.vectorstores.base.BaseVectorStore.delete","title":"delete  <code>abstractmethod</code>","text":"<pre><code>delete(ids, **kwargs)\n</code></pre> <p>Delete vector embeddings from vector stores</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>list[str]</code> <p>List of ids of the embeddings to be deleted</p> required <code>kwargs</code> <p>meant for vectorstore-specific parameters</p> <code>{}</code> Source code in <code>kotaemon\\storages\\vectorstores\\base.py</code> <pre><code>@abstractmethod\ndef delete(self, ids: list[str], **kwargs):\n    \"\"\"Delete vector embeddings from vector stores\n\n    Args:\n        ids: List of ids of the embeddings to be deleted\n        kwargs: meant for vectorstore-specific parameters\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/vectorstores/base/#storages.vectorstores.base.BaseVectorStore.query","title":"query  <code>abstractmethod</code>","text":"<pre><code>query(embedding, top_k=1, ids=None, **kwargs)\n</code></pre> <p>Return the top k most similar vector embeddings</p> <p>Parameters:</p> Name Type Description Default <code>embedding</code> <code>list[float]</code> <p>List of embeddings</p> required <code>top_k</code> <code>int</code> <p>Number of most similar embeddings to return</p> <code>1</code> <code>ids</code> <code>Optional[list[str]]</code> <p>List of ids of the embeddings to be queried</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[list[list[float]], list[float], list[str]]</code> <p>the matched embeddings, the similarity scores, and the ids</p> Source code in <code>kotaemon\\storages\\vectorstores\\base.py</code> <pre><code>@abstractmethod\ndef query(\n    self,\n    embedding: list[float],\n    top_k: int = 1,\n    ids: Optional[list[str]] = None,\n    **kwargs,\n) -&gt; tuple[list[list[float]], list[float], list[str]]:\n    \"\"\"Return the top k most similar vector embeddings\n\n    Args:\n        embedding: List of embeddings\n        top_k: Number of most similar embeddings to return\n        ids: List of ids of the embeddings to be queried\n\n    Returns:\n        the matched embeddings, the similarity scores, and the ids\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/storages/vectorstores/chroma/","title":"Chroma","text":""},{"location":"reference/storages/vectorstores/chroma/#storages.vectorstores.chroma.ChromaVectorStore","title":"ChromaVectorStore","text":"<p>             Bases: <code>LlamaIndexVectorStore</code></p> Source code in <code>kotaemon\\storages\\vectorstores\\chroma.py</code> <pre><code>class ChromaVectorStore(LlamaIndexVectorStore):\n    _li_class: Type[LIChromaVectorStore] = LIChromaVectorStore\n\n    def __init__(\n        self,\n        path: str = \"./chroma\",\n        collection_name: str = \"default\",\n        host: str = \"localhost\",\n        port: str = \"8000\",\n        ssl: bool = False,\n        headers: Optional[Dict[str, str]] = None,\n        collection_kwargs: Optional[dict] = None,\n        stores_text: bool = True,\n        flat_metadata: bool = True,\n        **kwargs: Any,\n    ):\n        self._path = path\n        self._collection_name = collection_name\n        self._host = host\n        self._port = port\n        self._ssl = ssl\n        self._headers = headers\n        self._collection_kwargs = collection_kwargs\n        self._stores_text = stores_text\n        self._flat_metadata = flat_metadata\n        self._kwargs = kwargs\n\n        try:\n            import chromadb\n        except ImportError:\n            raise ImportError(\n                \"ChromaVectorStore requires chromadb. \"\n                \"Please install chromadb first `pip install chromadb`\"\n            )\n\n        client = chromadb.PersistentClient(path=path)\n        collection = client.get_or_create_collection(collection_name)\n\n        # pass through for nice IDE support\n        super().__init__(\n            chroma_collection=collection,\n            host=host,\n            port=port,\n            ssl=ssl,\n            headers=headers or {},\n            collection_kwargs=collection_kwargs or {},\n            stores_text=stores_text,\n            flat_metadata=flat_metadata,\n            **kwargs,\n        )\n        self._client = cast(LIChromaVectorStore, self._client)\n\n    def delete(self, ids: List[str], **kwargs):\n        \"\"\"Delete vector embeddings from vector stores\n\n        Args:\n            ids: List of ids of the embeddings to be deleted\n            kwargs: meant for vectorstore-specific parameters\n        \"\"\"\n        self._client._collection.delete(ids=ids)\n\n    def delete_collection(self, collection_name: Optional[str] = None):\n        \"\"\"Delete entire collection under specified name from vector stores\n\n        Args:\n            collection_name: Name of the collection to delete\n        \"\"\"\n        # a rather ugly chain call but it do the job of finding\n        # original chromadb client and call delete_collection() method\n        if collection_name is None:\n            collection_name = self._client.client.name\n        self._client.client._client.delete_collection(collection_name)\n\n    def count(self) -&gt; int:\n        return self._collection.count()\n\n    def __persist_flow__(self):\n        return {\n            \"path\": self._path,\n            \"collection_name\": self._collection_name,\n            \"host\": self._host,\n            \"port\": self._port,\n            \"ssl\": self._ssl,\n            \"headers\": self._headers,\n            \"collection_kwargs\": self._collection_kwargs,\n            \"stores_text\": self._stores_text,\n            \"flat_metadata\": self._flat_metadata,\n            **self._kwargs,\n        }\n</code></pre>"},{"location":"reference/storages/vectorstores/chroma/#storages.vectorstores.chroma.ChromaVectorStore.delete","title":"delete","text":"<pre><code>delete(ids, **kwargs)\n</code></pre> <p>Delete vector embeddings from vector stores</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>List[str]</code> <p>List of ids of the embeddings to be deleted</p> required <code>kwargs</code> <p>meant for vectorstore-specific parameters</p> <code>{}</code> Source code in <code>kotaemon\\storages\\vectorstores\\chroma.py</code> <pre><code>def delete(self, ids: List[str], **kwargs):\n    \"\"\"Delete vector embeddings from vector stores\n\n    Args:\n        ids: List of ids of the embeddings to be deleted\n        kwargs: meant for vectorstore-specific parameters\n    \"\"\"\n    self._client._collection.delete(ids=ids)\n</code></pre>"},{"location":"reference/storages/vectorstores/chroma/#storages.vectorstores.chroma.ChromaVectorStore.delete_collection","title":"delete_collection","text":"<pre><code>delete_collection(collection_name=None)\n</code></pre> <p>Delete entire collection under specified name from vector stores</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>Optional[str]</code> <p>Name of the collection to delete</p> <code>None</code> Source code in <code>kotaemon\\storages\\vectorstores\\chroma.py</code> <pre><code>def delete_collection(self, collection_name: Optional[str] = None):\n    \"\"\"Delete entire collection under specified name from vector stores\n\n    Args:\n        collection_name: Name of the collection to delete\n    \"\"\"\n    # a rather ugly chain call but it do the job of finding\n    # original chromadb client and call delete_collection() method\n    if collection_name is None:\n        collection_name = self._client.client.name\n    self._client.client._client.delete_collection(collection_name)\n</code></pre>"},{"location":"reference/storages/vectorstores/in_memory/","title":"In Memory","text":"<p>Simple vector store index.</p>"},{"location":"reference/storages/vectorstores/in_memory/#storages.vectorstores.in_memory.InMemoryVectorStore","title":"InMemoryVectorStore","text":"<p>             Bases: <code>LlamaIndexVectorStore</code></p> Source code in <code>kotaemon\\storages\\vectorstores\\in_memory.py</code> <pre><code>class InMemoryVectorStore(LlamaIndexVectorStore):\n    _li_class: Type[LISimpleVectorStore] = LISimpleVectorStore\n    store_text: bool = False\n\n    def __init__(\n        self,\n        data: Optional[SimpleVectorStoreData] = None,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize params.\"\"\"\n        self._data = data or SimpleVectorStoreData()\n        self._fs = fs or fsspec.filesystem(\"file\")\n\n        super().__init__(\n            data=data,\n            fs=fs,\n            **kwargs,\n        )\n\n    def save(\n        self,\n        save_path: str,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n        **kwargs,\n    ):\n\n        \"\"\"save a simpleVectorStore to a dictionary.\n\n        Args:\n            save_path: Path of saving vector to disk.\n            fs: An abstract super-class for pythonic file-systems\n        \"\"\"\n        self._client.persist(persist_path=save_path, fs=fs)\n\n    def load(self, load_path: str, fs: Optional[fsspec.AbstractFileSystem] = None):\n\n        \"\"\"Create a SimpleKVStore from a load directory.\n\n        Args:\n            load_path: Path of loading vector.\n            fs: An abstract super-class for pythonic file-systems\n        \"\"\"\n        self._client = self._client.from_persist_path(persist_path=load_path, fs=fs)\n\n    def __persist_flow__(self):\n        d = self._data.to_dict()\n        d[\"__type__\"] = f\"{self._data.__module__}.{self._data.__class__.__qualname__}\"\n        return {\n            \"data\": d,\n            # \"fs\": self._fs,\n        }\n</code></pre>"},{"location":"reference/storages/vectorstores/in_memory/#storages.vectorstores.in_memory.InMemoryVectorStore.save","title":"save","text":"<pre><code>save(save_path, fs=None, **kwargs)\n</code></pre> <p>save a simpleVectorStore to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Path of saving vector to disk.</p> required <code>fs</code> <code>Optional[AbstractFileSystem]</code> <p>An abstract super-class for pythonic file-systems</p> <code>None</code> Source code in <code>kotaemon\\storages\\vectorstores\\in_memory.py</code> <pre><code>def save(\n    self,\n    save_path: str,\n    fs: Optional[fsspec.AbstractFileSystem] = None,\n    **kwargs,\n):\n\n    \"\"\"save a simpleVectorStore to a dictionary.\n\n    Args:\n        save_path: Path of saving vector to disk.\n        fs: An abstract super-class for pythonic file-systems\n    \"\"\"\n    self._client.persist(persist_path=save_path, fs=fs)\n</code></pre>"},{"location":"reference/storages/vectorstores/in_memory/#storages.vectorstores.in_memory.InMemoryVectorStore.load","title":"load","text":"<pre><code>load(load_path, fs=None)\n</code></pre> <p>Create a SimpleKVStore from a load directory.</p> <p>Parameters:</p> Name Type Description Default <code>load_path</code> <code>str</code> <p>Path of loading vector.</p> required <code>fs</code> <code>Optional[AbstractFileSystem]</code> <p>An abstract super-class for pythonic file-systems</p> <code>None</code> Source code in <code>kotaemon\\storages\\vectorstores\\in_memory.py</code> <pre><code>def load(self, load_path: str, fs: Optional[fsspec.AbstractFileSystem] = None):\n\n    \"\"\"Create a SimpleKVStore from a load directory.\n\n    Args:\n        load_path: Path of loading vector.\n        fs: An abstract super-class for pythonic file-systems\n    \"\"\"\n    self._client = self._client.from_persist_path(persist_path=load_path, fs=fs)\n</code></pre>"},{"location":"reference/storages/vectorstores/simple_file/","title":"Simple File","text":"<p>Simple file vector store index.</p>"},{"location":"reference/storages/vectorstores/simple_file/#storages.vectorstores.simple_file.SimpleFileVectorStore","title":"SimpleFileVectorStore","text":"<p>             Bases: <code>LlamaIndexVectorStore</code></p> <p>Similar to InMemoryVectorStore but is backed by file by default</p> Source code in <code>kotaemon\\storages\\vectorstores\\simple_file.py</code> <pre><code>class SimpleFileVectorStore(LlamaIndexVectorStore):\n    \"\"\"Similar to InMemoryVectorStore but is backed by file by default\"\"\"\n\n    _li_class: Type[LISimpleVectorStore] = LISimpleVectorStore\n    store_text: bool = False\n\n    def __init__(\n        self,\n        path: str | Path,\n        data: Optional[SimpleVectorStoreData] = None,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize params.\"\"\"\n        self._data = data or SimpleVectorStoreData()\n        self._fs = fs or fsspec.filesystem(\"file\")\n        self._path = path\n        self._save_path = Path(path)\n\n        super().__init__(\n            data=data,\n            fs=fs,\n            **kwargs,\n        )\n\n        if self._save_path.is_file():\n            self._client = self._li_class.from_persist_path(\n                persist_path=str(self._save_path), fs=self._fs\n            )\n\n    def add(\n        self,\n        embeddings: list[list[float]] | list[DocumentWithEmbedding],\n        metadatas: Optional[list[dict]] = None,\n        ids: Optional[list[str]] = None,\n    ):\n        r = super().add(embeddings, metadatas, ids)\n        self._client.persist(str(self._save_path), self._fs)\n        return r\n\n    def delete(self, ids: list[str], **kwargs):\n        r = super().delete(ids, **kwargs)\n        self._client.persist(str(self._save_path), self._fs)\n        return r\n\n    def __persist_flow__(self):\n        d = self._data.to_dict()\n        d[\"__type__\"] = f\"{self._data.__module__}.{self._data.__class__.__qualname__}\"\n        return {\n            \"data\": d,\n            \"path\": str(self._path),\n            # \"fs\": self._fs,\n        }\n</code></pre>"}]}